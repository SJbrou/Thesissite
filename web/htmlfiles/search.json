[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis",
    "section": "",
    "text": "Here I’ll provide all the data for my thesis. Hosting it makes it easy to show the analysises that I do. Code is provided in the code, and also hosted complementary on github.\nFirst drafts of my thesis, presentation, collected data and scripts will be available here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html",
    "href": "LoadXsense.html",
    "title": "LoadXsense",
    "section": "",
    "text": "Data visualisation and management\nThis is the code used to visualise and manage the output of the Xsense dot IMU data. The source code can be found on my github",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#correct-time",
    "href": "LoadXsense.html#correct-time",
    "title": "LoadXsense",
    "section": "",
    "text": "Code for loading the data\n\n\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Function to load Xsense data (and convert time to seconds)\nLoadXsenseData &lt;- function(directory) {\n  \n  \n  files &lt;- list.files(path = directory, full.names = TRUE)  # Searches each marker file\n  data_list &lt;- lapply(files, function(file) { # creates a list storing the data\n    data &lt;- read.csv(file, header = TRUE, skip = 10) # reads each csv file\n    return(data)\n  })\n  names(data_list) &lt;- paste0(\"data\", seq_along(data_list))  # Names elements to my preference\n  \n  ## Re-align time\n  min_time &lt;- min(sapply(data_list, function(df) min(df$SampleTimeFine))) #  Identify the minimum SampleTimeFine value across all data\n  adjusted_data_list &lt;- lapply(data_list, function(df) { # Subtract min_time from SampleTimeFine for each dataframe in data_list\n    df$SampleTimeFine &lt;- df$SampleTimeFine - min_time\n    df$A_abs &lt;- sqrt(df$FreeAcc_X^2 + df$FreeAcc_Y^2 + df$FreeAcc_Z^2)\n    df$TimeS &lt;- (df$SampleTimeFine / 1e6) - (df$SampleTimeFine[1] / 1e6) # Converts time to seconds\n    return(df)\n  })\n\n  # Removing first elements of longer list\n  # Find the minimum number of rows among all dataframes\n  min_rows &lt;- min(sapply(adjusted_data_list, nrow))\n\n  # Remove elements from the beginning of each dataframe in adjusted_data_list\n  adjusted_data_list &lt;- lapply(adjusted_data_list, function(df) {\n    if (nrow(df) &gt; min_rows) {\n      df &lt;- df[(nrow(df) - min_rows + 1):nrow(df), , drop = FALSE]  # Remove elements from the beginning\n    }\n    return(df)\n  })\n\n  # Define Markers dataframe\n  markers &lt;- data.frame(PacketCounter = adjusted_data_list[[1]]$PacketCounter,\n                        SampleTimeFine = adjusted_data_list[[1]]$SampleTimeFine,\n                        TimeS = adjusted_data_list[[1]]$TimeS)\n\n  # Add A_abs columns dynamically for each dataframe\n  for (i in 1:5) {\n    markers[paste0(\"A_abs\", i)] &lt;- adjusted_data_list[[i]]$A_abs\n  }\n\n  # Add FreeAcc_X, FreeAcc_Y, FreeAcc_Z columns for each dataframe\n  # for (i in 1:5) {\n  #   markers[paste0(\"A_X\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_X\n  #   markers[paste0(\"A_Y\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Y\n  #   markers[paste0(\"A_Z\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Z\n  # }\n\n  return(markers)  # Return the Markers dataframe\n}",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "thesis.html",
    "href": "thesis.html",
    "title": "2  From push press to push jerk: does sex influence the movement pattern?",
    "section": "",
    "text": "2.1 Introduction\nThe ability to generate high power -the product of force and velocity- is essential for both daily activities and occupational tasks. It is widely acknowledged to be correlated with overall good health, and retaining power is considered a significant indicator of healthy ageing. Moreover, in the real of sports, power output is regarded as a strong determinant of athletic success. Consequently, the evaluation of muscular power and the development of training strategies aimed at enhancing power are of considerable interest to sport coaches, healthcare workers and researchers alike, and is reflected in the considerable attention focused on the development of power in athletes, and the research examining specific techniques used to maximize power development.\nExploratory investigations have delved into the power performances across various athletic disciplines, reporting Olympic weightlifting (OLY) as one of the disciplines where athletes demonstrated some of the highest absolute and relative peak power outputs. Consequently, OLY has been proposed as an effective means of enhancing peak power capacities. OLY involves lifting a loaded barbell off the ground to an overhead locked position, in either the Clean and Jerk (C&J) or Snatch, requiring high force in limited time to overcome gravity (5)\nThese assertions are supported by the biomechanical mechanics inherent to OLY exercises. Firstly, OLY exercises mimic sport-specific movements by engaging in forceful triple extension patterns involving the hips, knees and ankles. Secondly, they facilitate the generation of high rates of force development and power output.\nFurther justification for the inclusion of OLY exercises in training programs is provided by research highlighting correlations between OLY performance and various athletic attributes. Studies have shown significant associations between the hang power clean and sprinting (r = -0.58, p&lt;0.01), jumping (r=0.41, p &lt; 0.05) and change of direction tasks (r = -0.41, p &lt; 0.05). Additionally, a recent meta-analysis emphasis the efficacy of incorporating OLY exercises and their derivatives into training regimens, particularly in improving jumping performance compared to traditional resistance training, with a notable ~5% difference (effect size [ES] = 0.64, p &lt; 0.001) (Hackett et al., 2016).\nStudies have delved into the mechanics underlying these correlations. OLY exercises trigger hormonal responses similar to those observed following conventional strength and hypertrophy protocols. Additionally, cross-sectional data suggests that OLY training induces a transformation from type IIX to type-IIA muscle fibers, accompanied by hypertrophy specifically in type II fibers. This transfer confers advantages for maximal force production. As a result, weightlifters exhibit approximately 15-20% higher isometric peak force and a 13-16% faster contractile rate of force development compared to other strength and power athletes.\nFurthermore, OLY training has been found to reduce the typical sex-related gap in neuromuscular strength and power expression. However, this apparent sex-related difference seems to amplify with advancing adult age, revealing that women undergo a more pronounced age-related decline in muscle shortening velocity and peak power output when compared with men.\nNumerous studies have investigated the impact of sex on strength and other indices of musclular performance, consistently reporting lower levels of strength in females, even after accounting for confounding factors such as body weight. Some research has examined the relationship between shoulder strength and the Push Jerk (PJ), a movement where a barbell is propelled overhead, reporting differences in the ratio between the Strict Press and PJ across sexes. Generally there are three distinct manners of moving a barbell overhead: the Strict Press (SP), where exclusively the upper-limb strength is utilized to move the barbell; The Push Press (PP), involving a slight downward dip to engage the power of the triple extension; or the Push Jerk (PJ), allowing for catching the barbell at a lower position. Studies have indicated differences in the ratio between the SP and PJ across sexes. This has raised the question at what relative intensity movement patters transition from SP to PP, and form PP to PJ, and wether these thresholds differ between males and females. Given that males exhibit a higher SP-to-PJ ratio, it is hypothesized that female will transition to altered movement patterns at lower relative intensities.\nThe primary objective of this study is to investigate the relative intensities at which the movement pattern shifts from performing an overhead barbell press to push press, and from a push press to push jerk. Additionally, this study aims to assess the magnitude of these differences between male and female participants.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "thesis.html#methods",
    "href": "thesis.html#methods",
    "title": "2  From push press to push jerk: does sex influence the movement pattern?",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nA within-subject repeated measures research design was used to determine the kinetics of the S2O under different loads, and the corresponding shifts in preferred technique. IMU’s were placed on the right side of the barbell, lower arm, trunk, upper thighs and lower leg. Kinematic data was collected at [x Hz] and filtered with an [filter]\nParticipants were asked to press progressively loaded barbells overhead and instructed to SP as long as possible before switching to PP, and PP as long as possible before switching to PJ.\nDefining movements Technical aspects of the exercises have been well documented and defined elsewhere (19, 12). Differentiating visually can be error prone. Therefore, the movement performed is determined by the kinetics of the barbell, trunk, thighs and legs.\nStrict press Strict presses were defined as those movements where the barbell is pressed overhead in one upward motion, without any (prior) downward motion of the trunk or thighs. The lifter starts with the barbell in front-rack with their preferred grip width. The barbell should be presses overhead by extending the elbows and flexing the shoulders. The legs must not be involved. The lift is not disqualified if the barbell itself has downward motion at some point, as long as it eventually finishes overhead.\nPush press Generally the same set-up as in the strict press is used, but the movement starts with counter-movement: by dipping down and coming up the lifter can utilize the power of knee and hip extension, which generates additional upwards momentum. Push presses were defined as those movements where the barbell, trunk and possibly thighs have downward momentum proceeding the upward momentum.\nPush jerk Again, the same general set-up applies, but after accelerating the barbell upward, the athlete dips downward to catch the barbell in a lower position, thus requiring less upward momentum, and then stands up with the bar. PJ were defined as those movements where the momentum of the barbell, trunk or thighs starts downwards, and then reverses three times.\nTesting procedures Testing started with a warming-up protocol of two sets of 10 repetition of exercise specific drills: airsquats, front squats, SP, PP, PJ). After the warming up the participants were asked to rotate arround their vertical axis once to demarcate the starting of the testing in the IMU’s. Testing started with an empty 20kg barbell for the males, of 15kg barbell for the females, as is standard for olympic weightlifting. Participants were asked to press the barbell overhead, while retaining the SP and PP technique as long as possible. After each successful lift the barbell was loaded with an additional 5kg, unless the participants self-repoted an 1RM PJ of less then 80kg, in which case incremental steps of 2.5kg were used. Each participant was allowed reattempts, but testing stopped after more than two consecutive misses. Rest time between attempts was not allowed to exceed 2 minutes.\nStatistical analysis An a priori alpha level was set at p &lt;= 0.05.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#code",
    "href": "LoadXsense.html#code",
    "title": "LoadXsense",
    "section": "Code",
    "text": "Code\n\nCode for loading the data\n\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Function to load Xsense data (and convert time to seconds)\nLoadXsenseData &lt;- function(directory) {\n  \n  \n  files &lt;- list.files(path = directory, full.names = TRUE)  # Searches each marker file\n  data_list &lt;- lapply(files, function(file) { # creates a list storing the data\n    data &lt;- read.csv(file, header = TRUE, skip = 10) # reads each csv file\n    return(data)\n  })\n  names(data_list) &lt;- paste0(\"data\", seq_along(data_list))  # Names elements to my preference\n  \n  ## Re-align time\n  min_time &lt;- min(sapply(data_list, function(df) min(df$SampleTimeFine))) #  Identify the minimum SampleTimeFine value across all data\n  adjusted_data_list &lt;- lapply(data_list, function(df) { # Subtract min_time from SampleTimeFine for each dataframe in data_list\n    df$SampleTimeFine &lt;- df$SampleTimeFine - min_time\n    df$A_abs &lt;- sqrt(df$FreeAcc_X^2 + df$FreeAcc_Y^2 + df$FreeAcc_Z^2)\n    df$TimeS &lt;- (df$SampleTimeFine / 1e6) - (df$SampleTimeFine[1] / 1e6) # Converts time to seconds\n    return(df)\n  })\n\n  # Removing first elements of longer list\n  # Find the minimum number of rows among all dataframes\n  min_rows &lt;- min(sapply(adjusted_data_list, nrow))\n\n  # Remove elements from the beginning of each dataframe in adjusted_data_list\n  adjusted_data_list &lt;- lapply(adjusted_data_list, function(df) {\n    if (nrow(df) &gt; min_rows) {\n      df &lt;- df[(nrow(df) - min_rows + 1):nrow(df), , drop = FALSE]  # Remove elements from the beginning\n    }\n    return(df)\n  })\n\n  # Define Markers dataframe\n  markers &lt;- data.frame(PacketCounter = adjusted_data_list[[1]]$PacketCounter,\n                        SampleTimeFine = adjusted_data_list[[1]]$SampleTimeFine,\n                        TimeS = adjusted_data_list[[1]]$TimeS)\n\n  # Add A_abs columns dynamically for each dataframe\n  for (i in 1:5) {\n    markers[paste0(\"A_abs\", i)] &lt;- adjusted_data_list[[i]]$A_abs\n  }\n\n  # Add FreeAcc_X, FreeAcc_Y, FreeAcc_Z columns for each dataframe\n  # for (i in 1:5) {\n  #   markers[paste0(\"A_X\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_X\n  #   markers[paste0(\"A_Y\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Y\n  #   markers[paste0(\"A_Z\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Z\n  # }\n\n  return(markers)  # Return the Markers dataframe\n}\n\n\n\nLoadMarkers &lt;- function(folderdir) {\n  markers &lt;- list()  # Initialize an empty list to store the dataframes\n  folders &lt;- list.dirs(folderdir, full.names = TRUE, recursive = FALSE)\n  \n  for (i in seq_along(folders)) { # Iterate through each folder\n    data &lt;- LoadXsenseData(folders[i])  # Load Xsense data from the current folder\n    markers[[paste0(\"markers\", i)]] &lt;- data  # Store the dataframe in the list with a dynamic name\n  }\n  \n  return(markers)  # Return the list of dataframes\n}\n\nmarkers_list &lt;- LoadMarkers(\"../../Logs\")\n\n\nlibrary(plotly)\n\n# Create a list to store the plots\nplots &lt;- lapply(markers_list, function(marker) {\n  # Plot Times against A_abs for the current dataframe\n  plot &lt;- plot_ly(marker, x = ~TimeS, y = ~A_abs5, type = 'scatter', mode = 'lines') %&gt;%\n    layout(xaxis = list(title = \"TimeS\"), yaxis = list(title = \"A_abs\"))\n  \n  return(plot)\n})\n\n# Combine plots into a single interactive plot\ncombined_plot &lt;- subplot(plots, nrows = length(plots))\n\n# Print the combined plot\ncombined_plot",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#load-the-data",
    "href": "LoadXsense.html#load-the-data",
    "title": "LoadXsense",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n# format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       file, hz, skiprow\n  bart = c(\"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- nameofpp[1]\n  hz &lt;- as.numeric(nameofpp[2])\n  skiprow &lt;- as.numeric(nameofpp[3])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#visualise",
    "href": "LoadXsense.html#visualise",
    "title": "LoadXsense",
    "section": "Visualise",
    "text": "Visualise\nlets also define a function that visualises the data\n\n# Some functions to visualise the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values again.\n\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(bart)\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#data-clipping",
    "href": "LoadXsense.html#data-clipping",
    "title": "LoadXsense",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a seperate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered1$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered1, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_a2 &lt;- function(df) {\nplot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 300,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\nreturn(plot)\n}\nplot_a2(filtered2[[1]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[2]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[3]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[4]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[5]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nThere remain two great peaks in the last two successful lifts. The subject dropped the bar and it hit the ground, where all the spead almost instanteniously dissapears, and thus A_abs1 peaks.Actually, the lift ends when the barbell is overhead, and thus the recording should stop here, regardless if the bar is dropped or delegated to front-rack. Therefore the second peak should somehow be identified and filtered out.\nThe arms lower when the bar is dropped or delegated to front-rack, but the peak acceleration of the arms is before the moment the bar hits the ground or the shoulders, and thus the onset of the final peak acceleration of the arms indicates the latest moment that the barbell was successfully overhead. Therefore, we can filter the data that comes after the onset of the final peak out.\nAfter visual inspection of the data the arbitrary value of 60 was selected to determine movement of the arms. The final peak where A_abs5 reaches 60 is filtered out. peaks that are within 0.8 seconds of each other are regarded as one, and the 0.5 seconds before the peak are also discarded\n\n\nCode\n# Additional package required\n# \n# library(zoo)\n# \n# # Define a function to find peaks\n# find_peaks &lt;- function(x) {\n#   # Find peaks where A_abs5 is less than 60\n#   peak_indices &lt;- which(diff(sign(diff(x))) == -2) + 1\n# \n#   # Remove peaks that are within 0.8 seconds of each other\n#   peak_indices &lt;- peak_indices[diff(df$time[peak_indices]) &gt; 0.8]\n# \n#   return(peak_indices)\n# }\n# \n# # Find peaks in A_abs5\n# peaks &lt;- find_peaks(df$A_abs5)\n# \n# # Find the second peak\n# second_peak &lt;- peaks[2]\n# \n# # Get the time of the second peak\n# time_of_second_peak &lt;- df$time[second_peak]\n# \n# # Print the time of the second peak\n# print(time_of_second_peak)",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#visualization",
    "href": "LoadXsense.html#visualization",
    "title": "LoadXsense",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html",
    "href": "manualfiltering.html",
    "title": "Data filtering",
    "section": "",
    "text": "This is the code used to identify and cut out each of the lifts from the IMU’s data. This process has been repeated for each subject, and the resulting data is stored as seperate R dataframe files for further analysis. The process is described below\n\nLets load all functions from the previous chapter. This is a hassle since they are stored in Quarto markdown language, and R only accept real R code.\n\n\nCode\n# Clean workspace\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Most dependencies are loaded by loading datafiltering.qmd.\n# To load only the chuncks containing functions we need parsermd\nlibrary(parsermd)\n\ntoload &lt;- c(\"load_data\",\"load_plots\", \"filter_data\", \"separate_lifts\", \"visualise_seperate_lifts\", \"loadXsenseData2\")\nrmd &lt;- parse_rmd(\"datafiltering.qmd\")\n\n\nfor (i in seq_along(toload)) {\n  setup_chunk &lt;- rmd_select(rmd, toload[i]) |&gt; \n    as_document()\n\n  setup_chunk &lt;- setup_chunk[-grep(\"```\", setup_chunk)]\n  setup_chunk\n#&gt; [1] \"library(tidyr)\"   \"library(stringr)\" \"\"                \n\n  eval(parse(text = setup_chunk))             \n}\nrm(rmd, i, setup_chunk, toload)\n\n\nI’ve collected metadata about all recorded lift in liftinfo.csv. This can be used to easialy load the lifts:\n\n\nCode\nmetadata &lt;- read.csv(\"../../Logs/metadata.csv\", header = TRUE, sep = \";\")\n\n\nHere is the code to cut each lift and store it as a R object\n\n\nCode\ndir = \"../../Logs/Usefull data\"\nfiles &lt;- list.files(path = dir, full.names = TRUE)\n\ndata &lt;- LoadXsenseData2(files[1])\nplot_a(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata1 &lt;- filterdata(data)\ndata1 &lt;- separatelifts(data1)\n\ni = 1\nplot_filtered_subplots(data1)\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nThis was used to loop throuhg all the lifts\n\n\nCode\ncat(\"lift \", i)\nproefpersoon &lt;- \"female1_2105\"\ncat(\"start time: \")\nstart_time &lt;- readline()\ncat(\"end time: \")\nend_time &lt;- readline()\ndf &lt;- data1[[i]]\n  \ntime_column &lt;- \"time\"\noutput_filename &lt;- paste0(proefpersoon, i, \".csv\")\n  \n# Filter the dataframe\nfiltered_df &lt;- df[df[[time_column]] &gt;= start_time & df[[time_column]] &lt;= end_time, ]\n# Save the filtered dataframe to a CSV file\nwrite.csv(filtered_df, output_filename, row.names = FALSE)\n\ni &lt;- i + 1\n\n\nThe next chapter will describe the analysis of the lifts.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual clipping</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#load-the-data",
    "href": "manualfiltering.html#load-the-data",
    "title": "Data filtering",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n# format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       file, hz, skiprow\n  bart = c(\"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- nameofpp[1]\n  hz &lt;- as.numeric(nameofpp[2])\n  skiprow &lt;- as.numeric(nameofpp[3])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#visualization",
    "href": "manualfiltering.html#visualization",
    "title": "Data filtering",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#data-clipping",
    "href": "manualfiltering.html#data-clipping",
    "title": "Data filtering",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a seperate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered1$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered1, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_a2 &lt;- function(df) {\nplot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 500,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\nreturn(plot)\n}\nplot1 &lt;- plot_a2(filtered2[[1]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nplot2 &lt;- plot_a2(filtered2[[2]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nplot3 &lt;- plot_a2(filtered2[[3]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nsubplot(plot1, plot2, plot3, nrows = 3)\n\n\n\n\n\n\nThis is great! As expected our filter is a bit conservative. It is hard to identify the exact start of the lift mathematicly. However, when we visualise the data a clear dip in acceleration can be seen just before the peaks. This should be the start of the lift - the dip - where the subject moves downward before propelling the bar upwards. This can be seen at T=31 for the firts lift, at t=192.5 for the second lift, and t=470 for the last lift.\nObviously, the lower leg IMU moves down the least. The upper leg IMU moves down a little, and the other IMU’s move down substantially more. selecting only marker 1, 3 and 5 should make the distinction even clearer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#manual-definition",
    "href": "manualfiltering.html#manual-definition",
    "title": "Data filtering",
    "section": "Manual definition",
    "text": "Manual definition\nNext chapter will describe how and what start and endpoints for each lift are selected.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "datafiltering.html",
    "href": "datafiltering.html",
    "title": "Data filtering",
    "section": "",
    "text": "Data filtering\nThis is the code used to filter and manage the output of the Xsense dot IMU data. The source code can be found on my github",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#load-the-data",
    "href": "datafiltering.html#load-the-data",
    "title": "Data filtering",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n\n\n\nCode\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- toString(nameofpp[2])\n  hz &lt;- as.numeric(nameofpp[3])\n  skiprow &lt;- as.numeric(nameofpp[4])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}\n\n\n\n\nCode\n#! Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n#! format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       name, file, hz, skiprow\n  bart = c(\"bart\", \"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"bart\", \"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\n\n\n\nCode\nLoadXsenseData2 &lt;- function(dir) {\n  hz &lt;- 60\n  skiprow &lt;- 7\n\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#visualization",
    "href": "datafiltering.html#visualization",
    "title": "Data filtering",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#data-clipping",
    "href": "datafiltering.html#data-clipping",
    "title": "Data filtering",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n\nLets call the function\n\n\nCode\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a separate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\n\nLets call the function\n\n\nCode\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_filtered_subplots &lt;- function(filtered2) {\n  plots &lt;- list()\n  for (i in seq_along(filtered2)) {\n    plots[[i]] &lt;- plot_ly(filtered2[[i]], x = ~time, y = ~A_abs1, name = paste0(i, \" marker 1\"), type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = paste0(i, \" marker 2\")) %&gt;%\n          add_trace(y = ~A_abs3, name = paste0(i, \" marker 3\")) %&gt;%\n          add_trace(y = ~A_abs4, name = paste0(i, \" marker 4\")) %&gt;%\n          add_trace(y = ~A_abs5, name = paste0(i, \" marker 5\")) %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 700,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\n  }\n  return(subplot(plots, nrows = length(filtered2)))\n}\n\n\nLets call the function\n\n\nCode\nplot_filtered_subplots(filtered2)\n\n\n\n\n\n\nThis is great! As expected our filter is a bit conservative. It is hard to identify the exact start of the lift mathematicly. However, when we visualise the data a clear dip in acceleration can be seen just before the peaks. This should be the start of the lift - the dip - where the subject moves downward before propelling the bar upwards. This can be seen at T=31 for the firts lift, at t=192.5 for the second lift, and t=470 for the last lift.\nObviously, the lower leg IMU moves down the least. The upper leg IMU moves down a little, and the other IMU’s move down substantially more. selecting only marker 1, 3 and 5 should make the distinction even clearer",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#manual-definition",
    "href": "datafiltering.html#manual-definition",
    "title": "Data filtering",
    "section": "Manual definition",
    "text": "Manual definition\nNext chapter will describe how and what start and endpoints for each lift are selected.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html",
    "href": "analysis_of_lifst.html",
    "title": "Analysis of lifts",
    "section": "",
    "text": "This is the code used to identify and cut out each of the lifts from the IMU’s data. This process has been repeated for each subject, and the resulting data is stored as seperate R dataframe files for further analysis. The process is described below\n\nLets load all functions from the previous chapter. This is a hassle since they are stored in Quarto markdown language, and R only accept real R code.\n\n\nCode\n# Clean workspace\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Most dependencies are loaded by loading datafiltering.qmd.\n# To load only the chuncks containing functions we need parsermd\nlibrary(parsermd)\n\ntoload &lt;- c(\"load_data\",\"load_plots\", \"filter_data\", \"separate_lifts\", \"visualise_seperate_lifts\")\nrmd &lt;- parse_rmd(\"datafiltering.qmd\")\n\n\nfor (i in seq_along(toload)) {\n  setup_chunk &lt;- rmd_select(rmd, toload[i]) |&gt; \n    as_document()\n\n  setup_chunk &lt;- setup_chunk[-grep(\"```\", setup_chunk)]\n  setup_chunk\n#&gt; [1] \"library(tidyr)\"   \"library(stringr)\" \"\"                \n\n  eval(parse(text = setup_chunk))             \n}\nrm(rmd, i, setup_chunk, toload)\n\n\nThe next chapter will describe the analysis of the lifts.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "This is the code used to identify and cut out each of the lifts from the IMU’s data. This process has been repeated for each subject, and the resulting data is stored as seperate R dataframe files for further analysis. The process is described below\n\nLets load all functions from the previous chapter. This is a hassle since they are stored in Quarto markdown language, and R only accept real R code.\n\n\nCode\n# Clean workspace\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Most dependencies are loaded by loading datafiltering.qmd.\n# To load only the chuncks containing functions we need parsermd\nlibrary(parsermd)\n\ntoload &lt;- c(\"load_data\",\"load_plots\", \"filter_data\", \"separate_lifts\", \"visualise_seperate_lifts\")\nrmd &lt;- parse_rmd(\"datafiltering.qmd\")\n\n\nfor (i in seq_along(toload)) {\n  setup_chunk &lt;- rmd_select(rmd, toload[i]) |&gt; \n    as_document()\n\n  setup_chunk &lt;- setup_chunk[-grep(\"```\", setup_chunk)]\n  setup_chunk\n#&gt; [1] \"library(tidyr)\"   \"library(stringr)\" \"\"                \n\n  eval(parse(text = setup_chunk))             \n}\nrm(rmd, i, setup_chunk, toload)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html#peak-accelerations",
    "href": "analysis_of_lifst.html#peak-accelerations",
    "title": "Analysis of lifts",
    "section": "Peak accelerations",
    "text": "Peak accelerations\nLets create a function to find the maximum value for a specific attribute for each lift (such as the absolute acceleration or gyr of one of the IMU’s)\n\n\nCode\n## Loading correct data\nfind_col_max &lt;- function(df_list, colname) {\n  # Initialize a matrix to store the maximum values\n  col_max &lt;- matrix(NA, nrow = length(df_list), ncol = 5)\n  colnames(col_max) &lt;- c(paste0(colname, \"1\"), paste0(colname, \"2\"), paste0(colname, \"3\"), paste0(colname, \"4\"), paste0(colname, \"5\"))\n  \n  for (i in seq_along(df_list)) {\n    df &lt;- df_list[[i]]\n    for (j in 1:5) {\n      col_max[i, j] &lt;- max(df[[paste0(colname, j)]], na.rm = TRUE)\n    }\n  }\n  \n  # Convert the matrix to a dataframe\n  col_max_df &lt;- as.data.frame(col_max)\n  \n  return(col_max_df)\n}\n\n\nLets check if the function works as intended by loading the absolute acceleration of the barbells (showing only first 10 rows)\n\n\nCode\nmale_max_abs &lt;- find_col_max(data_male, \"A_abs\")\nfemale_max_abs &lt;- find_col_max(data_female, \"A_abs\")\n\n# Showing only first 10 rows\nkable(male_max_abs[1:10, ])\n\n\n\n\n\n\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\n\n\n\n\n1\n19.45938\n123.70582\n68.15013\n37.94942\n31.23898\n\n\n2\n17.93894\n134.62920\n87.32489\n55.14171\n31.51992\n\n\n3\n16.34248\n119.93698\n79.20782\n42.85987\n27.21415\n\n\n4\n15.98943\n145.17490\n59.99045\n37.66462\n19.23075\n\n\n5\n16.22577\n79.54606\n62.68990\n32.99983\n25.64038\n\n\n6\n18.28350\n36.82861\n23.90038\n18.61690\n21.60688\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA.1\nNA\nNA\nNA\nNA\nNA\n\n\nNA.2\nNA\nNA\nNA\nNA\nNA\n\n\nNA.3\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nCode\nkable(female_max_abs[1:10, ])\n\n\n\n\n\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\n\n\n\n\n21.33489\n26.25516\n21.11224\n20.35712\n21.43023\n\n\n18.31731\n52.59376\n29.68279\n29.37173\n18.88949\n\n\n17.80586\n61.21685\n28.65790\n28.80041\n17.90354\n\n\n20.68555\n33.19173\n19.86114\n20.08488\n21.76857\n\n\n20.18132\n65.96100\n31.89304\n29.46926\n29.89478\n\n\n19.42459\n40.37175\n23.66470\n21.92713\n19.92161\n\n\n20.48714\n47.56603\n23.36424\n25.57460\n22.10430\n\n\n19.19975\n36.74904\n22.33372\n18.90032\n19.84985\n\n\n18.21275\n39.00130\n23.14487\n21.93431\n20.78825\n\n\n18.94983\n55.57859\n24.33905\n22.37700\n19.79378\n\n\n\n\n\nNow lets define a function to do statistical tests. This function has some logic so it automatically uses the correct t-test (studens, welch’s or mann-whithney based on the results of levenes test and shapiro-wilk tests)\n\n\nCode\nlibrary(car)\n\n# Define the function\ncompare_groups &lt;- function(column1, column2, group1_name = \"Male\", group2_name = \"Female\") {\n  # Create dataframes\n  group1 &lt;- data.frame(Value = column1, Group = group1_name)\n  group2 &lt;- data.frame(Value = column2, Group = group2_name)\n  \n  # Check for normality (Shapiro-Wilk test)\n  shapiro_test_group1 &lt;- shapiro.test(group1$Value)\n  shapiro_test_group2 &lt;- shapiro.test(group2$Value)\n  \n  # Print normality test results\n  cat(\"Shapiro-Wilk Test for Normality\\n\")\n  print(shapiro_test_group1)\n  print(shapiro_test_group2)\n  \n  # Combine the dataframes\n  combined_data &lt;- rbind(group1, group2)\n  \n  # Check for equal variance (Levene's test)\n  levene_test &lt;- leveneTest(Value ~ Group, data = combined_data)\n  \n  # Print Levene's test result\n  cat(\"Levene's Test for Equality of Variances\\n\")\n  print(levene_test)\n  \n  # Logic to run the appropriate test\n  if (shapiro_test_group1$p.value &gt; 0.05 && shapiro_test_group2$p.value &gt; 0.05) {\n    # Normal distribution\n    if (levene_test$`Pr(&gt;F)`[1] &gt; 0.05) {\n      # Equal variances\n      cat(\"Normal distribution and equal variances (Student's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = TRUE)\n    } else {\n      # Unequal variances\n      cat(\"Normal distribution, unequal variances (Welch's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = FALSE)\n    }\n  } else {\n    # No normality\n    cat(\"No normality (Mann-Whitney U test)\\n\")\n    t_test_result &lt;- wilcox.test(column1, column2)\n  }\n  print(t_test_result)\n}\n\n\nLets test the function with the male and female maximum absolute acceleration of the barbell IMU\n\n\nCode\n# Example usage\n# Assuming abs_max_male and abs_max_female are your dataframes and A_abs1 is the column to compare\nresult &lt;- compare_groups(male_max_abs$A_abs1, female_max_abs$A_abs1, group1_name = \"Male\", group2_name = \"Female\")\n\n\nShapiro-Wilk Test for Normality\n\n    Shapiro-Wilk normality test\n\ndata:  group1$Value\nW = 0.88825, p-value = 0.3091\n\n\n    Shapiro-Wilk normality test\n\ndata:  group2$Value\nW = 0.95406, p-value = 0.462\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Equality of Variances\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.0969 0.3058\n      23               \nNormal distribution and equal variances (Student's t-test)\n\n    Two Sample t-test\n\ndata:  column1 and column2\nt = -4.1223, df = 23, p-value = 0.000415\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.353086 -1.112275\nsample estimates:\nmean of x mean of y \n 17.37325  19.60593 \n\n\nThe next chapter will describe the analysis of the lifts.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  }
]