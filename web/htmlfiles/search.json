[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis",
    "section": "",
    "text": "Does sex influence the movement pattern of the push jerk?",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html",
    "href": "LoadXsense.html",
    "title": "LoadXsense",
    "section": "",
    "text": "Data visualisation and management\nThis is the code used to visualise and manage the output of the Xsense dot IMU data. The source code can be found on my github",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#correct-time",
    "href": "LoadXsense.html#correct-time",
    "title": "LoadXsense",
    "section": "",
    "text": "Code for loading the data\n\n\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Function to load Xsense data (and convert time to seconds)\nLoadXsenseData &lt;- function(directory) {\n  \n  \n  files &lt;- list.files(path = directory, full.names = TRUE)  # Searches each marker file\n  data_list &lt;- lapply(files, function(file) { # creates a list storing the data\n    data &lt;- read.csv(file, header = TRUE, skip = 10) # reads each csv file\n    return(data)\n  })\n  names(data_list) &lt;- paste0(\"data\", seq_along(data_list))  # Names elements to my preference\n  \n  ## Re-align time\n  min_time &lt;- min(sapply(data_list, function(df) min(df$SampleTimeFine))) #  Identify the minimum SampleTimeFine value across all data\n  adjusted_data_list &lt;- lapply(data_list, function(df) { # Subtract min_time from SampleTimeFine for each dataframe in data_list\n    df$SampleTimeFine &lt;- df$SampleTimeFine - min_time\n    df$A_abs &lt;- sqrt(df$FreeAcc_X^2 + df$FreeAcc_Y^2 + df$FreeAcc_Z^2)\n    df$TimeS &lt;- (df$SampleTimeFine / 1e6) - (df$SampleTimeFine[1] / 1e6) # Converts time to seconds\n    return(df)\n  })\n\n  # Removing first elements of longer list\n  # Find the minimum number of rows among all dataframes\n  min_rows &lt;- min(sapply(adjusted_data_list, nrow))\n\n  # Remove elements from the beginning of each dataframe in adjusted_data_list\n  adjusted_data_list &lt;- lapply(adjusted_data_list, function(df) {\n    if (nrow(df) &gt; min_rows) {\n      df &lt;- df[(nrow(df) - min_rows + 1):nrow(df), , drop = FALSE]  # Remove elements from the beginning\n    }\n    return(df)\n  })\n\n  # Define Markers dataframe\n  markers &lt;- data.frame(PacketCounter = adjusted_data_list[[1]]$PacketCounter,\n                        SampleTimeFine = adjusted_data_list[[1]]$SampleTimeFine,\n                        TimeS = adjusted_data_list[[1]]$TimeS)\n\n  # Add A_abs columns dynamically for each dataframe\n  for (i in 1:5) {\n    markers[paste0(\"A_abs\", i)] &lt;- adjusted_data_list[[i]]$A_abs\n  }\n\n  # Add FreeAcc_X, FreeAcc_Y, FreeAcc_Z columns for each dataframe\n  # for (i in 1:5) {\n  #   markers[paste0(\"A_X\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_X\n  #   markers[paste0(\"A_Y\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Y\n  #   markers[paste0(\"A_Z\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Z\n  # }\n\n  return(markers)  # Return the Markers dataframe\n}",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "thesis.html",
    "href": "thesis.html",
    "title": "2  From push press to push jerk: does sex influence the movement pattern?",
    "section": "",
    "text": "2.1 Introduction\nThe ability to generate high power -the product of force and velocity- is essential for both daily activities and occupational tasks. It is widely acknowledged to be correlated with overall good health, and retaining power is considered a significant indicator of healthy ageing. Moreover, in the real of sports, power output is regarded as a strong determinant of athletic success. Consequently, the evaluation of muscular power and the development of training strategies aimed at enhancing power are of considerable interest to sport coaches, healthcare workers and researchers alike, and is reflected in the considerable attention focused on the development of power in athletes, and the research examining specific techniques used to maximize power development.\nExploratory investigations have delved into the power performances across various athletic disciplines, reporting Olympic weightlifting (OLY) as one of the disciplines where athletes demonstrated some of the highest absolute and relative peak power outputs. Consequently, OLY has been proposed as an effective means of enhancing peak power capacities. OLY involves lifting a loaded barbell off the ground to an overhead locked position, in either the Clean and Jerk (C&J) or Snatch, requiring high force in limited time to overcome gravity (5)\nThese assertions are supported by the biomechanical mechanics inherent to OLY exercises. Firstly, OLY exercises mimic sport-specific movements by engaging in forceful triple extension patterns involving the hips, knees and ankles. Secondly, they facilitate the generation of high rates of force development and power output.\nFurther justification for the inclusion of OLY exercises in training programs is provided by research highlighting correlations between OLY performance and various athletic attributes. Studies have shown significant associations between the hang power clean and sprinting (r = -0.58, p&lt;0.01), jumping (r=0.41, p &lt; 0.05) and change of direction tasks (r = -0.41, p &lt; 0.05). Additionally, a recent meta-analysis emphasis the efficacy of incorporating OLY exercises and their derivatives into training regimens, particularly in improving jumping performance compared to traditional resistance training, with a notable ~5% difference (effect size [ES] = 0.64, p &lt; 0.001) (Hackett et al., 2016).\nStudies have delved into the mechanics underlying these correlations. OLY exercises trigger hormonal responses similar to those observed following conventional strength and hypertrophy protocols. Additionally, cross-sectional data suggests that OLY training induces a transformation from type IIX to type-IIA muscle fibers, accompanied by hypertrophy specifically in type II fibers. This transfer confers advantages for maximal force production. As a result, weightlifters exhibit approximately 15-20% higher isometric peak force and a 13-16% faster contractile rate of force development compared to other strength and power athletes.\nFurthermore, OLY training has been found to reduce the typical sex-related gap in neuromuscular strength and power expression. However, this apparent sex-related difference seems to amplify with advancing adult age, revealing that women undergo a more pronounced age-related decline in muscle shortening velocity and peak power output when compared with men.\nNumerous studies have investigated the impact of sex on strength and other indices of musclular performance, consistently reporting lower levels of strength in females, even after accounting for confounding factors such as body weight. Some research has examined the relationship between shoulder strength and the Push Jerk (PJ), a movement where a barbell is propelled overhead, reporting differences in the ratio between the Strict Press and PJ across sexes. Generally there are three distinct manners of moving a barbell overhead: the Strict Press (SP), where exclusively the upper-limb strength is utilized to move the barbell; The Push Press (PP), involving a slight downward dip to engage the power of the triple extension; or the Push Jerk (PJ), allowing for catching the barbell at a lower position. Studies have indicated differences in the ratio between the SP and PJ across sexes. This has raised the question at what relative intensity movement patters transition from SP to PP, and form PP to PJ, and wether these thresholds differ between males and females. Given that males exhibit a higher SP-to-PJ ratio, it is hypothesized that female will transition to altered movement patterns at lower relative intensities.\nThe primary objective of this study is to investigate the relative intensities at which the movement pattern shifts from performing an overhead barbell press to push press, and from a push press to push jerk. Additionally, this study aims to assess the magnitude of these differences between male and female participants.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "thesis.html#methods",
    "href": "thesis.html#methods",
    "title": "2  From push press to push jerk: does sex influence the movement pattern?",
    "section": "2.2 Methods",
    "text": "2.2 Methods\nA within-subject repeated measures research design was used to determine the kinetics of the S2O under different loads, and the corresponding shifts in preferred technique. IMU’s were placed on the right side of the barbell, lower arm, trunk, upper thighs and lower leg. Kinematic data was collected at [x Hz] and filtered with an [filter]\nParticipants were asked to press progressively loaded barbells overhead and instructed to SP as long as possible before switching to PP, and PP as long as possible before switching to PJ.\nDefining movements Technical aspects of the exercises have been well documented and defined elsewhere (19, 12). Differentiating visually can be error prone. Therefore, the movement performed is determined by the kinetics of the barbell, trunk, thighs and legs.\nStrict press Strict presses were defined as those movements where the barbell is pressed overhead in one upward motion, without any (prior) downward motion of the trunk or thighs. The lifter starts with the barbell in front-rack with their preferred grip width. The barbell should be presses overhead by extending the elbows and flexing the shoulders. The legs must not be involved. The lift is not disqualified if the barbell itself has downward motion at some point, as long as it eventually finishes overhead.\nPush press Generally the same set-up as in the strict press is used, but the movement starts with counter-movement: by dipping down and coming up the lifter can utilize the power of knee and hip extension, which generates additional upwards momentum. Push presses were defined as those movements where the barbell, trunk and possibly thighs have downward momentum proceeding the upward momentum.\nPush jerk Again, the same general set-up applies, but after accelerating the barbell upward, the athlete dips downward to catch the barbell in a lower position, thus requiring less upward momentum, and then stands up with the bar. PJ were defined as those movements where the momentum of the barbell, trunk or thighs starts downwards, and then reverses three times.\nTesting procedures Testing started with a warming-up protocol of two sets of 10 repetition of exercise specific drills: airsquats, front squats, SP, PP, PJ). After the warming up the participants were asked to rotate arround their vertical axis once to demarcate the starting of the testing in the IMU’s. Testing started with an empty 20kg barbell for the males, of 15kg barbell for the females, as is standard for olympic weightlifting. Participants were asked to press the barbell overhead, while retaining the SP and PP technique as long as possible. After each successful lift the barbell was loaded with an additional 5kg, unless the participants self-repoted an 1RM PJ of less then 80kg, in which case incremental steps of 2.5kg were used. Each participant was allowed reattempts, but testing stopped after more than two consecutive misses. Rest time between attempts was not allowed to exceed 2 minutes.\nStatistical analysis An a priori alpha level was set at p &lt;= 0.05.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#code",
    "href": "LoadXsense.html#code",
    "title": "LoadXsense",
    "section": "Code",
    "text": "Code\n\nCode for loading the data\n\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Function to load Xsense data (and convert time to seconds)\nLoadXsenseData &lt;- function(directory) {\n  \n  \n  files &lt;- list.files(path = directory, full.names = TRUE)  # Searches each marker file\n  data_list &lt;- lapply(files, function(file) { # creates a list storing the data\n    data &lt;- read.csv(file, header = TRUE, skip = 10) # reads each csv file\n    return(data)\n  })\n  names(data_list) &lt;- paste0(\"data\", seq_along(data_list))  # Names elements to my preference\n  \n  ## Re-align time\n  min_time &lt;- min(sapply(data_list, function(df) min(df$SampleTimeFine))) #  Identify the minimum SampleTimeFine value across all data\n  adjusted_data_list &lt;- lapply(data_list, function(df) { # Subtract min_time from SampleTimeFine for each dataframe in data_list\n    df$SampleTimeFine &lt;- df$SampleTimeFine - min_time\n    df$A_abs &lt;- sqrt(df$FreeAcc_X^2 + df$FreeAcc_Y^2 + df$FreeAcc_Z^2)\n    df$TimeS &lt;- (df$SampleTimeFine / 1e6) - (df$SampleTimeFine[1] / 1e6) # Converts time to seconds\n    return(df)\n  })\n\n  # Removing first elements of longer list\n  # Find the minimum number of rows among all dataframes\n  min_rows &lt;- min(sapply(adjusted_data_list, nrow))\n\n  # Remove elements from the beginning of each dataframe in adjusted_data_list\n  adjusted_data_list &lt;- lapply(adjusted_data_list, function(df) {\n    if (nrow(df) &gt; min_rows) {\n      df &lt;- df[(nrow(df) - min_rows + 1):nrow(df), , drop = FALSE]  # Remove elements from the beginning\n    }\n    return(df)\n  })\n\n  # Define Markers dataframe\n  markers &lt;- data.frame(PacketCounter = adjusted_data_list[[1]]$PacketCounter,\n                        SampleTimeFine = adjusted_data_list[[1]]$SampleTimeFine,\n                        TimeS = adjusted_data_list[[1]]$TimeS)\n\n  # Add A_abs columns dynamically for each dataframe\n  for (i in 1:5) {\n    markers[paste0(\"A_abs\", i)] &lt;- adjusted_data_list[[i]]$A_abs\n  }\n\n  # Add FreeAcc_X, FreeAcc_Y, FreeAcc_Z columns for each dataframe\n  # for (i in 1:5) {\n  #   markers[paste0(\"A_X\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_X\n  #   markers[paste0(\"A_Y\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Y\n  #   markers[paste0(\"A_Z\", i)] &lt;- adjusted_data_list[[i]]$FreeAcc_Z\n  # }\n\n  return(markers)  # Return the Markers dataframe\n}\n\n\n\nLoadMarkers &lt;- function(folderdir) {\n  markers &lt;- list()  # Initialize an empty list to store the dataframes\n  folders &lt;- list.dirs(folderdir, full.names = TRUE, recursive = FALSE)\n  \n  for (i in seq_along(folders)) { # Iterate through each folder\n    data &lt;- LoadXsenseData(folders[i])  # Load Xsense data from the current folder\n    markers[[paste0(\"markers\", i)]] &lt;- data  # Store the dataframe in the list with a dynamic name\n  }\n  \n  return(markers)  # Return the list of dataframes\n}\n\nmarkers_list &lt;- LoadMarkers(\"../../Logs\")\n\n\nlibrary(plotly)\n\n# Create a list to store the plots\nplots &lt;- lapply(markers_list, function(marker) {\n  # Plot Times against A_abs for the current dataframe\n  plot &lt;- plot_ly(marker, x = ~TimeS, y = ~A_abs5, type = 'scatter', mode = 'lines') %&gt;%\n    layout(xaxis = list(title = \"TimeS\"), yaxis = list(title = \"A_abs\"))\n  \n  return(plot)\n})\n\n# Combine plots into a single interactive plot\ncombined_plot &lt;- subplot(plots, nrows = length(plots))\n\n# Print the combined plot\ncombined_plot",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#load-the-data",
    "href": "LoadXsense.html#load-the-data",
    "title": "LoadXsense",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n# format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       file, hz, skiprow\n  bart = c(\"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- nameofpp[1]\n  hz &lt;- as.numeric(nameofpp[2])\n  skiprow &lt;- as.numeric(nameofpp[3])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#visualise",
    "href": "LoadXsense.html#visualise",
    "title": "LoadXsense",
    "section": "Visualise",
    "text": "Visualise\nlets also define a function that visualises the data\n\n# Some functions to visualise the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values again.\n\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(bart)\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#data-clipping",
    "href": "LoadXsense.html#data-clipping",
    "title": "LoadXsense",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a seperate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered1$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered1, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_a2 &lt;- function(df) {\nplot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 300,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\nreturn(plot)\n}\nplot_a2(filtered2[[1]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[2]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[3]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[4]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nCode\nplot_a2(filtered2[[5]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nThere remain two great peaks in the last two successful lifts. The subject dropped the bar and it hit the ground, where all the spead almost instanteniously dissapears, and thus A_abs1 peaks.Actually, the lift ends when the barbell is overhead, and thus the recording should stop here, regardless if the bar is dropped or delegated to front-rack. Therefore the second peak should somehow be identified and filtered out.\nThe arms lower when the bar is dropped or delegated to front-rack, but the peak acceleration of the arms is before the moment the bar hits the ground or the shoulders, and thus the onset of the final peak acceleration of the arms indicates the latest moment that the barbell was successfully overhead. Therefore, we can filter the data that comes after the onset of the final peak out.\nAfter visual inspection of the data the arbitrary value of 60 was selected to determine movement of the arms. The final peak where A_abs5 reaches 60 is filtered out. peaks that are within 0.8 seconds of each other are regarded as one, and the 0.5 seconds before the peak are also discarded\n\n\nCode\n# Additional package required\n# \n# library(zoo)\n# \n# # Define a function to find peaks\n# find_peaks &lt;- function(x) {\n#   # Find peaks where A_abs5 is less than 60\n#   peak_indices &lt;- which(diff(sign(diff(x))) == -2) + 1\n# \n#   # Remove peaks that are within 0.8 seconds of each other\n#   peak_indices &lt;- peak_indices[diff(df$time[peak_indices]) &gt; 0.8]\n# \n#   return(peak_indices)\n# }\n# \n# # Find peaks in A_abs5\n# peaks &lt;- find_peaks(df$A_abs5)\n# \n# # Find the second peak\n# second_peak &lt;- peaks[2]\n# \n# # Get the time of the second peak\n# time_of_second_peak &lt;- df$time[second_peak]\n# \n# # Print the time of the second peak\n# print(time_of_second_peak)",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "LoadXsense.html#visualization",
    "href": "LoadXsense.html#visualization",
    "title": "LoadXsense",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Scripts",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LoadXsense</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html",
    "href": "manualfiltering.html",
    "title": "Data filtering",
    "section": "",
    "text": "This is the code used to identify and cut out each of the lifts from the IMU’s data. This process has been repeated for each subject, and the resulting data is stored as seperate R dataframe files for further analysis. The process is described below\n\nLets load all functions from the previous chapter. This is a hassle since they are stored in Quarto markdown language, and R only accept real R code.\n\n\nCode\n# Clean workspace\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Most dependencies are loaded by loading datafiltering.qmd.\n# To load only the chuncks containing functions we need parsermd\nlibrary(parsermd)\n\ntoload &lt;- c(\"load_data\",\"load_plots\", \"filter_data\", \"separate_lifts\", \"visualise_seperate_lifts\", \"loadXsenseData2\")\nrmd &lt;- parse_rmd(\"datafiltering.qmd\")\n\n\nfor (i in seq_along(toload)) {\n  setup_chunk &lt;- rmd_select(rmd, toload[i]) |&gt; \n    as_document()\n\n  setup_chunk &lt;- setup_chunk[-grep(\"```\", setup_chunk)]\n  setup_chunk\n#&gt; [1] \"library(tidyr)\"   \"library(stringr)\" \"\"                \n\n  eval(parse(text = setup_chunk))             \n}\nrm(rmd, i, setup_chunk, toload)\n\n\nHere is the code to cut each lift and store it as a R object\n\n\nCode\ndir = \"../../Logs/Usefull data\"\nfiles &lt;- list.files(path = dir, full.names = TRUE)\ny = 1\ndata &lt;- LoadXsenseData2(files[y])\nplot_a(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndata1 &lt;- filterdata(data)\ndata1 &lt;- separatelifts(data1)\n\ni = 1\nplot_filtered_subplots(data1)\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\n\n\n\n\nThis was used to loop throuhg all the lifts\n\n\nCode\n#! A mistake of mine, This filters the analysed data (with the absolute values) instead of the raw imu data. \ncat(\"lift \", i)\nproefpersoon &lt;- files[y]\ncat(\"start time: \")\nstart_time &lt;- readline()\ncat(\"end time: \")\nend_time &lt;- readline()\ndf &lt;- data\n  \ntime_column &lt;- \"time\"\noutput_filename &lt;- paste0(proefpersoon, i, \".csv\")\n  \n# Filter the dataframe\nfiltered_df &lt;- df[df[[time_column]] &gt;= start_time & df[[time_column]] &lt;= end_time, ]\n# Save the filtered dataframe to a CSV file\nwrite.csv(filtered_df, output_filename, row.names = FALSE)\n\ni &lt;- i + 1\n\n\nThe next chapter will describe the analysis of the lifts.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual clipping</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#load-the-data",
    "href": "manualfiltering.html#load-the-data",
    "title": "Data filtering",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n# format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       file, hz, skiprow\n  bart = c(\"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- nameofpp[1]\n  hz &lt;- as.numeric(nameofpp[2])\n  skiprow &lt;- as.numeric(nameofpp[3])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#visualization",
    "href": "manualfiltering.html#visualization",
    "title": "Data filtering",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#data-clipping",
    "href": "manualfiltering.html#data-clipping",
    "title": "Data filtering",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting1)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a seperate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered1$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered1, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_a2 &lt;- function(df) {\nplot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 500,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\nreturn(plot)\n}\nplot1 &lt;- plot_a2(filtered2[[1]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nplot2 &lt;- plot_a2(filtered2[[2]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nplot3 &lt;- plot_a2(filtered2[[3]])\n\n\nWarning: Specifying width/height in layout() is now deprecated.\nPlease specify in ggplotly() or plot_ly()\n\n\nCode\nsubplot(plot1, plot2, plot3, nrows = 3)\n\n\n\n\n\n\nThis is great! As expected our filter is a bit conservative. It is hard to identify the exact start of the lift mathematicly. However, when we visualise the data a clear dip in acceleration can be seen just before the peaks. This should be the start of the lift - the dip - where the subject moves downward before propelling the bar upwards. This can be seen at T=31 for the firts lift, at t=192.5 for the second lift, and t=470 for the last lift.\nObviously, the lower leg IMU moves down the least. The upper leg IMU moves down a little, and the other IMU’s move down substantially more. selecting only marker 1, 3 and 5 should make the distinction even clearer",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "manualfiltering.html#manual-definition",
    "href": "manualfiltering.html#manual-definition",
    "title": "Data filtering",
    "section": "Manual definition",
    "text": "Manual definition\nNext chapter will describe how and what start and endpoints for each lift are selected.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Manual filtering</span>"
    ]
  },
  {
    "objectID": "datafiltering.html",
    "href": "datafiltering.html",
    "title": "Data filtering",
    "section": "",
    "text": "Data filtering\nThis is the code used to filter and manage the output of the Xsense dot IMU data. The source code can be found on my github",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#load-the-data",
    "href": "datafiltering.html#load-the-data",
    "title": "Data filtering",
    "section": "Load the data",
    "text": "Load the data\nLets start by defining a function to correctly load measurements:\nNote that IMU’s do not start and stop measureing at the exact same time; even after synchronization the amount of elements per IMU (the length of measurement) differs. In my implementation of temporal relaignment I assumed that the time in SampletimeFine was synchronized, and excluded first or last elements accordingly to ensure dataframes are of equal length. This eases calculation since R prefers to calculate over lists of equal length.\n\n\nCode\n# Clean workspace and load dependencies\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n\n\n\nCode\nLoadXsenseData &lt;- function(nameofpp) {\n  dir &lt;- toString(nameofpp[2])\n  hz &lt;- as.numeric(nameofpp[3])\n  skiprow &lt;- as.numeric(nameofpp[4])\n  #dir &lt;- bart\n  #hz &lt;- 60\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}\n\n\n\n\nCode\n#! Here i defined what test subject refers to what directorty\n#! should probably think about PID here later on. \n#! format: name, hz, skipheading\n\npp_info &lt;- data.frame (\n  #       name, file, hz, skiprow\n  bart = c(\"bart\", \"../../Logs/old/20240429_163145_bart/\", 60, 7),\n  other = c(\"bart\", \"../../Logs/new/20240502_192335/\", 60, 10)\n)\n\n\n\n\nCode\nLoadXsenseData2 &lt;- function(dir) {\n  hz &lt;- 60\n  skiprow &lt;- 7\n\n\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n\n  # Read CSV files of each directory\n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i], header = TRUE, skip = skiprow)\n  }\n\n  # Ensure all dataframes have the same number of rows\n  min_rows &lt;- min(sapply(data, nrow))\n  data &lt;- lapply(data, function(df) {\n    df &lt;- df[1:min_rows, , drop = FALSE]\n    return(df)\n  })\n\n  # Adjust time\n  for (i in seq_along(data)) {\n    rows &lt;- nrow(data[[i]])\n    data[[i]]$TimeS &lt;- ((1/hz) * (1:rows))\n  }\n\n  # Initialize toreturn data frame with time column\n  toreturn &lt;- data.frame(time = data[[1]]$TimeS)\n\n  # Calculate absolute values\n  for (i in 1:length(data)) {\n    if (\"FreeAcc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"FreeAcc_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$FreeAcc_X^2 + data[[i]]$FreeAcc_Y^2 + data[[i]]$FreeAcc_Z^2)\n    }\n    if (\"Acc_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"A_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Acc_X^2 + data[[i]]$Acc_Y^2 + data[[i]]$Acc_Z^2)\n    }\n    if (\"Gyr_X\" %in% names(data[[i]])) {\n      col_name &lt;- paste0(\"Gyr_abs\", i)\n      toreturn[[col_name]] &lt;- sqrt(data[[i]]$Gyr_X^2 + data[[i]]$Gyr_Y^2 + data[[i]]$Gyr_Z^2)\n    }\n  }\n\n  # Order the attributes of the dataframe\n  toreturn_sorted &lt;- toreturn[, sort(names(toreturn))]\n\n  return(toreturn_sorted)\n}",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#visualization",
    "href": "datafiltering.html#visualization",
    "title": "Data filtering",
    "section": "Visualization",
    "text": "Visualization\nlets also define some functions to visualize the data\n\n\nCode\n# Some functions to visualize the acceleration and the Gyr\n\nplot_a &lt;- function(df) {\n  if (\"A_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~A_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~A_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~A_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~A_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n    if (\"FreeAcc_abs1\" %in% names(df)) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~FreeAcc_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~FreeAcc_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~FreeAcc_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~FreeAcc_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~FreeAcc_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"FreeAcc_abs Values\")) |&gt;\n        bslib::card(full_screen = TRUE)\n  }\n  \n  return(plot)\n}\n\nplot_gyr &lt;- function(df) {\n  plot &lt;- plot_ly(df, x = ~time, y = ~Gyr_abs1, name = \"marker1\", type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~Gyr_abs2, name = \"marker 2\") %&gt;%\n          add_trace(y = ~Gyr_abs3, name = \"marker 3\") %&gt;%\n          add_trace(y = ~Gyr_abs4, name = \"marker 4\") %&gt;%\n          add_trace(y = ~Gyr_abs5, name = \"marker 5\") %&gt;%\n          layout(title = \"Absolute Gyr\",\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"Gyr Values\"))\n  return(plot)\n}\n\n\nplot_all_columns &lt;- function(df) {\n  if (!\"time\" %in% names(df)) {\n    stop(\"The dataframe must contain a 'time' column for the x-axis.\")\n  }\n  plot &lt;- plot_ly(df, x = ~time, type = 'scatter', mode = 'lines')\n  for (col in setdiff(names(df), \"time\")) {\n    plot &lt;- plot %&gt;% add_trace(y = df[[col]], name = col)\n  }\n  plot &lt;- plot %&gt;%\n    layout(title = \"All Columns Plot\",\n           xaxis = list(title = \"Time\"),\n           yaxis = list(title = \"Values\"))\n  return(plot)\n}\n\n#! Maybe include a plotting function that takes the dataframe and the attribute to plot, assuming 5 markers?\n\n\n\nInitial test\nLets load some data and see how it looks:\nnote: to increase performance I stored the calculated values and read them. This is faster than calculating all absolute values each time the program runs\n\n\n\nCode\n# Storing the calculated data\n  # meting1 &lt;- LoadXsenseData(pp_info[1:3,1])\n  # write.csv(meting1, file = \"example1.csv\", row.names = FALSE)\n\n# Loading the calulated data\nmeting1 &lt;- read.csv(\"example1.csv\")\n\n# Plot the calculated data\nplot_a(meting1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_gyr(meting1)\n\n\n\n\n\n\n\nThis look absolutely great! that the absolute acceleration approaches gravitational constant very well! It is a tiny bit higher than the expected 9.8 due to the noise. Since the absolute is taken, all noise that is not in opposite direction of the gravity increases the measured acceleration at rest. I’m overwhelmed by the precision of the IMU’s here!",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#data-clipping",
    "href": "datafiltering.html#data-clipping",
    "title": "Data filtering",
    "section": "Data clipping",
    "text": "Data clipping\nThe protocol was such that the barbell IMU only moved while the participant was executing a jerk, or while the barbell was being loaded. Thus, the barbell IMU data seems a wise place to identify the moments at which the jerk was executed.\nLetst start by calculating the range of the baseline acceleration that we measured between t = 900 and t = 1100\n\n\nCode\n# Find and print the minimum and maximum values from the range t = 900 - 1100\ncat(\"minimum value of A_abs: \", min(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nminimum value of A_abs:  0.02195406\n\n\nCode\ncat(\"maximum value of A_abs: \",max(meting1[meting1$time &gt;= 900 & meting1$time &lt;= 1100, ]))\n\n\nmaximum value of A_abs:  1100\n\n\n\nLets increase the interval slightly so that we can use it to automatically determine where movement occurs. We could exclude all cases where the barbell IMU measures an A_abs within the 8.5 - 11.2 inteval.However, when the barbell accelerates or decelerates the A_abs crosses the interval. Lets be conservative and assume that if it crosses the the interval, it does so for less than 360 elements (6 seconds! highly conservative but it works just fine!)\n\n\nCode\n# New variable to work with\nmeting_filtered &lt;- meting1\n\n# Compute the run-length encoding\nrle_sequence &lt;- rle(meting_filtered$A_abs1 &gt;= 8.5 & meting_filtered$A_abs1 &lt;= 11.2)\n\n# Identify the start and end indices of consecutive sequences where condition is TRUE\nstart_indices &lt;- cumsum(rle_sequence$lengths) - rle_sequence$lengths + 1\nend_indices &lt;- cumsum(rle_sequence$lengths)\n\n# Identify the consecutive sequences where the condition holds for more than 360 rows\ncondition_indices &lt;- which(rle_sequence$values & rle_sequence$lengths &gt;= 360)\n\n# Iterate over the consecutive sequences and replace the values of A_abs2 with NA\nfor (i in condition_indices) {\n  start_index &lt;- start_indices[i]\n  end_index &lt;- end_indices[i]\n  meting_filtered$A_abs1[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs2[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs3[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs4[start_index:end_index] &lt;- NA\n  meting_filtered$A_abs5[start_index:end_index] &lt;- NA\n}\n\nplot_a(meting_filtered)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nrm(meting_filtered, rle_sequence, start_indices, end_indices, start_index, end_index, condition_indices, i)\n\n\nThe barbell IMU also registers acceleration while it is loaded. Since the subject was asked to sit still when resting, and the bar was only loaded while resting, it seems safe to assume that the IMU on the lower leg remained stationary (but with a more variable baseline) when no lift was exercised.\nAfter visual inspection the interval of 8.5 - 11.2 seemed to suffice, again under the assumption of 360 elements.\nDue to my misunderstanding of the IMU’s configuration, some measurements measure the FreeAcceleration and others measure the Acceleration. Difference being whether the gravitation is accounted for. To ensure te function works in both cases, the interval is decreased by 9.81 if the dataframe contains a attribute named FreeAcc, indicating that gravity is not measured. This way the function should work in most cases.\n\n\nCode\nfilterdata &lt;- function(meting) {\n  # All columns where the conditions as described are true are removed. However, time is unchanged, since replancements work on indices. If an entire colum where to be removed, this would cause errors when plotting the data\n  \n  # Calculate run-length encoding when gravity is measured\n  if (\"A_abs1\" %in% names(meting)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= 9.2 & meting$A_abs1 &lt;= 10.3)\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= 9.0 & meting$A_abs2 &lt;= 11.5)\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= 9.0 & meting$A_abs3 &lt;= 11.5)\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= 8.0 & meting$A_abs4 &lt;= 11.5)\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= 7.2 & meting$A_abs5 &lt;= 15.7)\n  }\n  # Calculate run-length encoding when gravity is not measured\n  if (\"FreeAcc_abs1\" %in% names(meting)) {\n    rle_sequence_A_abs1 &lt;- rle(meting$A_abs1 &gt;= (9.2-9.8) & meting$A_abs1 &lt;= (10.3-9.8))\n    rle_sequence_A_abs2 &lt;- rle(meting$A_abs2 &gt;= (9.0-9.8) & meting$A_abs2 &lt;= (11.5-9.8))\n    rle_sequence_A_abs3 &lt;- rle(meting$A_abs3 &gt;= (9.0-9.8) & meting$A_abs3 &lt;= (11.5-9.8))\n    rle_sequence_A_abs4 &lt;- rle(meting$A_abs4 &gt;= (8.0-9.8) & meting$A_abs4 &lt;= (11.5-9.8))\n    rle_sequence_A_abs5 &lt;- rle(meting$A_abs5 &gt;= (7.2-9.8) & meting$A_abs5 &lt;= (15.7-9.8))\n  }\n  \n  # Identify the start and end indices\n  #! Might write a loop for this later on (Everything up until the functions return can be looped!)\n  start_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths) - rle_sequence_A_abs1$lengths + 1\n  end_indices_A_abs1 &lt;- cumsum(rle_sequence_A_abs1$lengths)\n  start_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths) - rle_sequence_A_abs2$lengths + 1\n  end_indices_A_abs2 &lt;- cumsum(rle_sequence_A_abs2$lengths)\n  start_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths) - rle_sequence_A_abs3$lengths + 1\n  end_indices_A_abs3 &lt;- cumsum(rle_sequence_A_abs3$lengths)\n  start_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths) - rle_sequence_A_abs4$lengths + 1\n  end_indices_A_abs4 &lt;- cumsum(rle_sequence_A_abs4$lengths)\n  start_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths) - rle_sequence_A_abs5$lengths + 1\n  end_indices_A_abs5 &lt;- cumsum(rle_sequence_A_abs5$lengths)\n  \n  # Identify more than 360 consecutive indices\n  condition_indices_A_abs1 &lt;- which(rle_sequence_A_abs1$values & rle_sequence_A_abs1$lengths &gt;= 300)\n  condition_indices_A_abs2 &lt;- which(rle_sequence_A_abs2$values & rle_sequence_A_abs2$lengths &gt;= 360)\n  condition_indices_A_abs3 &lt;- which(rle_sequence_A_abs3$values & rle_sequence_A_abs3$lengths &gt;= 360)\n  condition_indices_A_abs4 &lt;- which(rle_sequence_A_abs4$values & rle_sequence_A_abs4$lengths &gt;= 360)\n  condition_indices_A_abs5 &lt;- which(rle_sequence_A_abs5$values & rle_sequence_A_abs5$lengths &gt;= 800)\n  \n  # Replace with NA if condition A_abs1 = true\n  for (i in condition_indices_A_abs1) {\n    start_index &lt;- start_indices_A_abs1[i]\n    end_index &lt;- end_indices_A_abs1[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n    # Replace with NA if condition A_abs2 = true\n  for (i in condition_indices_A_abs2) {\n    start_index &lt;- start_indices_A_abs2[i]\n    end_index &lt;- end_indices_A_abs2[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs3 = true\n  for (i in condition_indices_A_abs3) {\n    start_index &lt;- start_indices_A_abs3[i]\n    end_index &lt;- end_indices_A_abs3[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n  # Replace with NA if condition A_abs4 = true\n  for (i in condition_indices_A_abs4) {\n    start_index &lt;- start_indices_A_abs4[i]\n    end_index &lt;- end_indices_A_abs4[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n  }\n# Replace with NA if condition A_abs5 = true\nfor (i in condition_indices_A_abs5) {\n  start_index &lt;- start_indices_A_abs5[i]\n  end_index &lt;- end_indices_A_abs5[i]\n  meting[start_index:end_index, !(names(meting) %in% \"time\")] &lt;- NA\n}\n  meting &lt;- meting[-1, ]\n  return(meting)\n}\n\n\nLets call the function\n\n\nCode\n# Call the function with your dataframe as argument\nfiltered1 &lt;- filterdata(meting1)\nplot_a(filtered1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe assumptions for this automatic filter might be a little conservative, but overall the filter seems to work pretty well. When compared to the raw files it might just keep a little to much. When looking for the maximum absolute values this would work perfect, but when looking at average values this method might not be precise enough. Then we would need to A. increase precision or B. decide a start and endpoint for the lift manualy.\n\nNow lets put each lift in a separate dataframe.\n\n\nCode\nseparatelifts &lt;- function(filtered) {\n  # Identify continuous NA portions\n  na_ranges &lt;- cumsum(is.na(filtered$A_abs1))\n  # Split dataframe based on NA\n  na_segments &lt;- split(filtered, na_ranges)\n  # Remove NA segments\n  valid_segments &lt;- na_segments[!sapply(na_segments, function(x) all(is.na(x$A_abs1)))]\n  # Optional: Rename the dataframes for clarity\n  names(valid_segments) &lt;- paste0(seq_along(valid_segments))\n  return(valid_segments)\n}\n\n\nLets call the function\n\n\nCode\nfiltered2 &lt;- separatelifts(filtered1)\n\n\n\n\nCode\nplot_filtered_subplots &lt;- function(filtered2) {\n  plots &lt;- list()\n  for (i in seq_along(filtered2)) {\n    plots[[i]] &lt;- plot_ly(filtered2[[i]], x = ~time, y = ~A_abs1, name = paste0(i, \" marker 1\"), type = \"scatter\", mode = \"lines\") %&gt;%\n          add_trace(y = ~A_abs2, name = paste0(i, \" marker 2\")) %&gt;%\n          add_trace(y = ~A_abs3, name = paste0(i, \" marker 3\")) %&gt;%\n          add_trace(y = ~A_abs4, name = paste0(i, \" marker 4\")) %&gt;%\n          add_trace(y = ~A_abs5, name = paste0(i, \" marker 5\")) %&gt;%\n          layout(title = \"Absolute accelerations\",\n                 height = 700,\n                 xaxis = list(title = \"Time\"),\n                 yaxis = list(title = \"A_abs Values\"))\n  }\n  return(subplot(plots, nrows = length(filtered2)))\n}\n\n\nLets call the function\n\n\nCode\nplot_filtered_subplots(filtered2)\n\n\n\n\n\n\nThis is great! As expected our filter is a bit conservative. It is hard to identify the exact start of the lift mathematicly. However, when we visualise the data a clear dip in acceleration can be seen just before the peaks. This should be the start of the lift - the dip - where the subject moves downward before propelling the bar upwards. This can be seen at T=31 for the firts lift, at t=192.5 for the second lift, and t=470 for the last lift.\nObviously, the lower leg IMU moves down the least. The upper leg IMU moves down a little, and the other IMU’s move down substantially more. selecting only marker 1, 3 and 5 should make the distinction even clearer",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "datafiltering.html#manual-definition",
    "href": "datafiltering.html#manual-definition",
    "title": "Data filtering",
    "section": "Manual definition",
    "text": "Manual definition\nNext chapter will describe how and what start and endpoints for each lift are selected.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Handling IMU data</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html",
    "href": "analysis_of_lifst.html",
    "title": "Analysis of lifts",
    "section": "",
    "text": "This is the code used to analyse the biomechanical characteristics of each lift.\n\nLets load all lift data and calculate peak and average linear and angular accelerations\n\n\nCode\nloadLift &lt;- function(dir) {\n  files &lt;- list.files(path = dir, full.names = TRUE)\n  data &lt;- list()\n  \n  for (i in seq_along(files)) {\n    data[[i]] &lt;- read.csv(files[i])\n  }\n  return(data)\n}\n\n# Load the data\ndata_male &lt;- loadLift(\"../../lifts/male\")\ndata_female &lt;- loadLift(\"../../lifts/female\")\n\n\n\n\n# Lets create a function to find the maximum value for a specific attribute for each lift (such as the absolute acceleration or gyr of one of the IMU's)\n\n\n## Loading correct data\nfind_col_max &lt;- function(df_list, colname) {\n  # Initialize a matrix to store the maximum values\n  col_max &lt;- matrix(NA, nrow = length(df_list), ncol = 5)\n  colnames(col_max) &lt;- c(paste0(colname, \"1\"), paste0(colname, \"2\"), paste0(colname, \"3\"), paste0(colname, \"4\"), paste0(colname, \"5\"))\n  \n  for (i in seq_along(df_list)) {\n    df &lt;- df_list[[i]]\n    for (j in 1:5) {\n      col_max[i, j] &lt;- max(df[[paste0(colname, j)]], na.rm = TRUE)\n    }\n  }\n  # Convert the matrix to a dataframe\n  col_max_df &lt;- as.data.frame(col_max)\n  \n  return(col_max_df)\n}\n\n## Lets check if the function works as intended by loading the absolute acceleration of the barbells (showing only first 10 rows)\n\n\nmale_max_abs &lt;- find_col_max(data_male, \"A_abs\")\nmale_max_gyr &lt;- find_col_max(data_male, \"Gyr_abs\")\nfemale_max_abs &lt;- find_col_max(data_female, \"A_abs\")\nfemale_max_gyr &lt;- find_col_max(data_female, \"Gyr_abs\")\n\nmale_max &lt;- cbind(male_max_abs, male_max_gyr)\nfemale_max &lt;- cbind(female_max_abs, female_max_gyr)\n# Add a group column to each dataframe\nmale_max &lt;- male_max %&gt;% mutate(group = \"male\")\nfemale_max &lt;- female_max %&gt;% mutate(group = \"female\")\n\n# Combine the dataframes\ndata &lt;- bind_rows(male_max, female_max)\n\n\n# Showing only first 10 rows\nkable(data[1:10, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\ngroup\n\n\n\n\n19.45938\n123.70582\n68.15013\n37.94942\n31.23898\n30.70867\n447.3661\n378.9496\n152.0784\n405.7006\nmale\n\n\n19.45938\n123.70582\n68.15013\n37.94942\n31.23898\n30.70867\n447.3661\n378.9496\n152.0784\n405.7006\nmale\n\n\n17.93894\n134.62920\n87.32489\n55.14171\n31.51992\n12.95801\n510.0824\n415.3523\n167.5220\n439.2996\nmale\n\n\n17.93894\n134.62920\n87.32489\n55.14171\n31.51992\n12.95801\n510.0824\n415.3523\n167.5220\n439.2996\nmale\n\n\n16.34248\n119.93698\n79.20782\n42.85987\n27.21415\n20.19276\n571.6062\n416.3854\n151.0167\n416.2550\nmale\n\n\n16.34248\n119.93698\n79.20782\n42.85987\n27.21415\n20.19276\n571.6062\n416.3854\n151.0167\n416.2550\nmale\n\n\n15.98943\n145.17490\n59.99045\n37.66462\n19.23075\n16.90985\n472.4933\n408.2328\n189.7104\n408.4898\nmale\n\n\n15.98943\n145.17490\n59.99045\n37.66462\n19.23075\n16.90985\n472.4933\n408.2328\n189.7104\n408.4898\nmale\n\n\n16.22577\n79.54606\n62.68990\n32.99983\n25.64038\n45.76053\n591.9665\n391.2921\n176.5874\n480.1031\nmale\n\n\n16.22577\n79.54606\n62.68990\n32.99983\n25.64038\n45.76053\n591.9665\n391.2921\n176.5874\n480.1031\nmale\n\n\n\n\n\nNow lets define a function to do statistical tests. This function has some logic so it automatically uses the correct t-test (studens, welch’s or mann-whithney based on the results of levenes test and shapiro-wilk tests)\n\n\nCode\nlibrary(car)\n\n# Define the function\ncompare_groups &lt;- function(column1, column2, group1_name = \"Male\", group2_name = \"Female\") {\n  # Create dataframes\n  group1 &lt;- data.frame(Value = column1, Group = group1_name)\n  group2 &lt;- data.frame(Value = column2, Group = group2_name)\n  \n  # Check for normality (Shapiro-Wilk test)\n  shapiro_test_group1 &lt;- shapiro.test(group1$Value)\n  shapiro_test_group2 &lt;- shapiro.test(group2$Value)\n  \n  # Print normality test results\n  cat(\"Shapiro-Wilk Test for Normality\\n\")\n  print(shapiro_test_group1)\n  print(shapiro_test_group2)\n  \n  # Combine the dataframes\n  combined_data &lt;- rbind(group1, group2)\n  \n  # Check for equal variance (Levene's test)\n  levene_test &lt;- leveneTest(Value ~ Group, data = combined_data)\n  \n  # Print Levene's test result\n  cat(\"Levene's Test for Equality of Variances\\n\")\n  print(levene_test)\n  \n  # Logic to run the appropriate test\n  if (shapiro_test_group1$p.value &gt; 0.05 && shapiro_test_group2$p.value &gt; 0.05) {\n    # Normal distribution\n    if (levene_test$`Pr(&gt;F)`[1] &gt; 0.05) {\n      # Equal variances\n      cat(\"Normal distribution and equal variances (Student's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = TRUE)\n    } else {\n      # Unequal variances\n      cat(\"Normal distribution, unequal variances (Welch's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = FALSE)\n    }\n  } else {\n    # No normality\n    cat(\"No normality (Mann-Whitney U test)\\n\")\n    t_test_result &lt;- wilcox.test(column1, column2)\n  }\n  print(t_test_result)\n}\n\n\nLets test the function with the male and female maximum absolute acceleration of the barbell IMU\n\n\nCode\n# Example usage\n# Assuming abs_max_male and abs_max_female are your dataframes and A_abs1 is the column to compare\nresult &lt;- compare_groups(data$A_abs1, data$A_abs1, group1_name = \"male\", group2_name = \"female\")\n\n\nShapiro-Wilk Test for Normality\n\n    Shapiro-Wilk normality test\n\ndata:  group1$Value\nW = 0.93966, p-value = 0.08073\n\n\n    Shapiro-Wilk normality test\n\ndata:  group2$Value\nW = 0.93966, p-value = 0.08073\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Equality of Variances\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1       0      1\n      60               \nNormal distribution and equal variances (Student's t-test)\n\n    Two Sample t-test\n\ndata:  column1 and column2\nt = 0, df = 60, p-value = 1\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8151582  0.8151582\nsample estimates:\nmean of x mean of y \n 18.74167  18.74167 \n\n\nObviously, we do not want to do many T-tests due to inflated familywise error rates. Thus we should perform a MANVOA #### #### ### #### ####\n\n\nCode\n# Define a custom function to format mean and standard deviation\nformat_mean_sd &lt;- function(x) {\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  sd_x &lt;- sd(x, na.rm = TRUE)\n  sprintf(\"%.2f ± %.2f\", mean_x, sd_x)\n}\n\n# Calculate mean, standard deviation, and count for each column by group\nresults &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    across(where(is.numeric), format_mean_sd)\n  )\n\n# Display the results in a nice table\nresults %&gt;%\n  kable(caption = \"Summary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\")\n\n\n\nSummary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nn\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\n\n\n\n\nfemale\n19.00 ± NA\n19.61 ± 1.08\n47.11 ± 16.97\n25.36 ± 6.29\n23.30 ± 3.84\n21.68 ± 3.33\n310.10 ± 134.92\n371.86 ± 125.60\n278.59 ± 118.32\n160.80 ± 32.10\n438.74 ± 35.54\n\n\nmale\n12.00 ± NA\n17.37 ± 1.33\n106.64 ± 38.95\n63.54 ± 20.96\n37.54 ± 11.42\n26.08 ± 4.76\n99.40 ± 173.40\n498.98 ± 70.24\n425.90 ± 57.43\n157.01 ± 28.01\n418.80 ± 37.12\n\n\n\n\n\nNow the statistics for the particiapnt’s characteristics\nLets confirm normality and equality of variances\nLets run an MANOVA and calculate effect sizes\n\n\nCode\n# Load necessary packages\nlibrary(car)\nlibrary(MVN)\nlibrary(purrr)\nlibrary(effectsize)  # For effect size calculation\n\n\nWarning: package 'effectsize' was built under R version 4.3.3\n\n\nCode\n# Assuming 'data' is your dataframe and 'Sex' column indicates 'male' or 'female'\n# Separate numerical columns and the 'Sex' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\ngroup_data &lt;- data$group\n\n# Check for normality for each numerical column and each group (male, female)\nnormality_results &lt;- lapply(names(numerical_data), function(col) {\n  male_normality &lt;- shapiro.test(numerical_data[group_data == \"male\", col])$p.value\n  female_normality &lt;- shapiro.test(numerical_data[group_data == \"female\", col])$p.value\n  return(c(male_normality, female_normality))\n})\n# Combine normality results into a dataframe\nnormality_results_df &lt;- do.call(rbind, normality_results)\ncolnames(normality_results_df) &lt;- c(\"Male_p_value\", \"Female_p_value\")\nrownames(normality_results_df) &lt;- names(numerical_data)\nprint(normality_results_df)\n\n\n         Male_p_value Female_p_value\nA_abs1   2.854878e-02   4.619671e-01\nA_abs2   1.931507e-02   7.249080e-02\nA_abs3   3.517357e-02   1.858413e-04\nA_abs4   1.928614e-01   8.951810e-03\nA_abs5   8.402953e-02   3.976160e-04\nGyr_abs1 2.489356e-05   2.471050e-03\nGyr_abs2 2.069149e-01   3.043955e-05\nGyr_abs3 5.393877e-04   2.329259e-06\nGyr_abs4 5.615732e-02   1.014058e-04\nGyr_abs5 1.944619e-01   1.859017e-02\n\n\nCode\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n# Perform Levene's test for each numerical column\nfor (col in names(data)[numerical_columns]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$group)\n  levene_results[[col]] &lt;- test_result\n}\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nCode\n# Extract \"F value\" and \"Pr(&gt;F)\" columns from each test result and bind them into a dataframe\nlevene_results_df &lt;- map_dfr(levene_results, ~{\n  data.frame(\n    F_value = .x$F[1],    # Extract the first value of F\n    Pr_value = .x$\"Pr(&gt;F)\"[1]  # Extract the first value of Pr(&gt;F)\n  )\n})\n\n# View the dataframe\nprint(levene_results_df)\n\n\n      F_value    Pr_value\n1  1.83995079 0.185423775\n2  3.63208611 0.066631868\n3  9.09645837 0.005283311\n4  5.75789660 0.023062104\n5  3.46179709 0.072970011\n6  0.02008831 0.888270779\n7  0.14657819 0.704618992\n8  0.47705054 0.495252947\n9  0.08428623 0.773636728\n10 0.00715751 0.933159109\n\n\nCode\nlevene_p_values &lt;- levene_results_df$Pr_value\n# Check if all p-values &gt; 0.05 for normality assumption\nall_normal &lt;- all(normality_results_df &gt; 0.05) & all(levene_p_values &lt; 0.05)\n\nif (all_normal) {\n  # Perform MANOVA\n  formula &lt;- as.formula(paste(\"cbind(\", paste(names(numerical_data), collapse = \", \"), \") ~ group\"))\n  manova_result &lt;- manova(formula, data = data)\n  manova_summary &lt;- summary(manova_result, test = \"Pillai\")\n  print(manova_summary)\n  \n  # Check if the MANOVA is significant\n  if (manova_summary$stats[1, \"Pr(&gt;F)\"] &lt; 0.05) {\n    print(\"MANOVA is significant. Performing univariate ANOVAs:\")\n    # Perform univariate ANOVAs for each dependent variable\n    univariate_results &lt;- summary.aov(manova_result)\n    print(univariate_results)\n    \n    # Extracting p-values and means for interpretation\n    for (var in names(numerical_data)) {\n      print(paste(\"Analyzing variable:\", var))\n      aov_result &lt;- aov(as.formula(paste(var, \"~ group\")), data = data)\n      print(summary(aov_result))\n      print(model.tables(aov_result, \"means\"), digits = 3)\n      \n      # Calculate effect size (eta-squared)\n      eta_squared_result &lt;- eta_squared(aov_result)\n      print(eta_squared_result)\n    }\n  } else {\n    print(\"MANOVA is not significant. No further tests are performed.\")\n  }\n} else {\n  # If normality is not met, use Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract p-value and calculate effect size (not applicable for Kruskal-Wallis)\n    p_value &lt;- kruskal_result$p.value\n    print(paste(\"P-value:\", p_value))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"P-value: 0.000352650198177613\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"P-value: 0.000352650198177613\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 16.487, df = 1, p-value = 4.898e-05\n\n[1] \"P-value: 4.8978154674227e-05\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 9.523, df = 1, p-value = 0.002029\n\n[1] \"P-value: 0.00202908222598898\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9354, df = 1, p-value = 0.01484\n\n[1] \"P-value: 0.0148395949690251\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 11.086, df = 1, p-value = 0.0008698\n\n[1] \"P-value: 0.000869804231618486\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 11.633, df = 1, p-value = 0.0006478\n\n[1] \"P-value: 0.000647777430091623\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 13.955, df = 1, p-value = 0.0001873\n\n[1] \"P-value: 0.000187257491153609\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 0.42207, df = 1, p-value = 0.5159\n\n[1] \"P-value: 0.515904079141032\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 1.4839, df = 1, p-value = 0.2232\n\n[1] \"P-value: 0.22317263688255\"\n\n\nCode\n# Initialize an empty dataframe to store the results\nkruskal_results_df &lt;- data.frame(\n  Variable = character(),\n  Test_Statistic = numeric(),\n  Degrees_of_Freedom = numeric(),\n  P_Value = numeric(),\n  stringsAsFactors = FALSE\n)\n\nif (!all_normal) {\n  # If normality assumptions are not met, perform Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract relevant information and add to the dataframe\n    p_value &lt;- kruskal_result$p.value\n    test_statistic &lt;- kruskal_result$statistic\n    degrees_freedom &lt;- kruskal_result$parameter\n    kruskal_results_df &lt;- rbind(kruskal_results_df, data.frame(\n      Variable = var,\n      Test_Statistic = test_statistic,\n      Degrees_of_Freedom = degrees_freedom,\n      P_Value = p_value\n    ))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 16.487, df = 1, p-value = 4.898e-05\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 9.523, df = 1, p-value = 0.002029\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9354, df = 1, p-value = 0.01484\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 11.086, df = 1, p-value = 0.0008698\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 11.633, df = 1, p-value = 0.0006478\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 13.955, df = 1, p-value = 0.0001873\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 0.42207, df = 1, p-value = 0.5159\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 1.4839, df = 1, p-value = 0.2232\n\n\nCode\n# Print the dataframe of Kruskal-Wallis test results\nprint(kruskal_results_df)\n\n\n                            Variable Test_Statistic Degrees_of_Freedom\nKruskal-Wallis chi-squared    A_abs1     12.7677318                  1\nKruskal-Wallis chi-squared1   A_abs2     12.7677318                  1\nKruskal-Wallis chi-squared2   A_abs3     16.4872569                  1\nKruskal-Wallis chi-squared3   A_abs4      9.5230396                  1\nKruskal-Wallis chi-squared4   A_abs5      5.9354125                  1\nKruskal-Wallis chi-squared5 Gyr_abs1     11.0860316                  1\nKruskal-Wallis chi-squared6 Gyr_abs2     11.6334085                  1\nKruskal-Wallis chi-squared7 Gyr_abs3     13.9548143                  1\nKruskal-Wallis chi-squared8 Gyr_abs4      0.4220738                  1\nKruskal-Wallis chi-squared9 Gyr_abs5      1.4838531                  1\n                                 P_Value\nKruskal-Wallis chi-squared  3.526502e-04\nKruskal-Wallis chi-squared1 3.526502e-04\nKruskal-Wallis chi-squared2 4.897815e-05\nKruskal-Wallis chi-squared3 2.029082e-03\nKruskal-Wallis chi-squared4 1.483959e-02\nKruskal-Wallis chi-squared5 8.698042e-04\nKruskal-Wallis chi-squared6 6.477774e-04\nKruskal-Wallis chi-squared7 1.872575e-04\nKruskal-Wallis chi-squared8 5.159041e-01\nKruskal-Wallis chi-squared9 2.231726e-01\n\n\nCode\n# Print the dataframe without row names and the \"Degrees_of_Freedom\" column, with P-values formatted\nkruskal_results_df$Variable[1] &lt;- \"Barbell PLA\"\nkruskal_results_df$Variable[2] &lt;- \"Lower leg PLA\"\nkruskal_results_df$Variable[3] &lt;- \"Upper leg PLA\"\nkruskal_results_df$Variable[4] &lt;- \"Torso PLA\"\nkruskal_results_df$Variable[5] &lt;- \"Upper arm PLA\"\nkruskal_results_df$Variable[6] &lt;- \"Barbell PAA\"\nkruskal_results_df$Variable[7] &lt;- \"Lower leg PAA\"\nkruskal_results_df$Variable[8] &lt;- \"Upper leg PAA\"\nkruskal_results_df$Variable[9] &lt;- \"Torso PAA\"\nkruskal_results_df$Variable[10] &lt;- \"Upper arm PAA\"\nprint(format(kruskal_results_df[, -which(names(kruskal_results_df) == \"Degrees_of_Freedom\")], scientific = FALSE), row.names = FALSE)\n\n\n      Variable Test_Statistic       P_Value\n   Barbell PLA     12.7677318 0.00035265020\n Lower leg PLA     12.7677318 0.00035265020\n Upper leg PLA     16.4872569 0.00004897815\n     Torso PLA      9.5230396 0.00202908223\n Upper arm PLA      5.9354125 0.01483959497\n   Barbell PAA     11.0860316 0.00086980423\n Lower leg PAA     11.6334085 0.00064777743\n Upper leg PAA     13.9548143 0.00018725749\n     Torso PAA      0.4220738 0.51590407914\n Upper arm PAA      1.4838531 0.22317263688\n\n\nCalculate the Point-biserial correlation coefficient\n\n\nCode\n# Initialize an empty vector to store the point-biserial correlation coefficients\npoint_biserial_coeffs &lt;- numeric()\n\n# Loop through each column in the dataset (excluding the grouping variable)\nfor (col in names(data)[!names(data) %in% c(\"group\")]) {\n  # Calculate the point-biserial correlation coefficient\n  point_biserial &lt;- cor(data[[col]], as.numeric(data$group == \"female\"))  # Convert \"female\" to 1 and \"male\" to 0\n  # Store the coefficient in the vector\n  point_biserial_coeffs &lt;- c(point_biserial_coeffs, point_biserial)\n}\n\n# Create a dataframe to display the results\npb_results_df &lt;- data.frame(\n  Variable = names(data)[!names(data) %in% c(\"group\")],  # Variable names (excluding grouping variable)\n  Point_Biserial_Coefficient = point_biserial_coeffs  # Point-biserial correlation coefficients\n)\n\n# Print the results dataframe\npb_results_df$Variable[1] &lt;- \"Barbell PLA\"\npb_results_df$Variable[2] &lt;- \"Lower leg PLA\"\npb_results_df$Variable[3] &lt;- \"Upper leg PLA\"\npb_results_df$Variable[4] &lt;- \"Torso PLA\"\npb_results_df$Variable[5] &lt;- \"Upper arm PLA\"\npb_results_df$Variable[6] &lt;- \"Barbell PAA\"\npb_results_df$Variable[7] &lt;- \"Lower leg PAA\"\npb_results_df$Variable[8] &lt;- \"Upper leg PAA\"\npb_results_df$Variable[9] &lt;- \"Torso PAA\"\npb_results_df$Variable[10] &lt;- \"Upper arm PAA\"\nprint(pb_results_df)\n\n\n        Variable Point_Biserial_Coefficient\n1    Barbell PLA                 0.68903217\n2  Lower leg PLA                -0.73738706\n3  Upper leg PLA                -0.81182778\n4      Torso PLA                -0.68372236\n5  Upper arm PLA                -0.49035474\n6    Barbell PAA                 0.57577712\n7  Lower leg PAA                -0.50994398\n8  Upper leg PAA                -0.59694920\n9      Torso PAA                 0.06232652\n10 Upper arm PAA                 0.26762921",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "This is the code used to identify and cut out each of the lifts from the IMU’s data. This process has been repeated for each subject, and the resulting data is stored as seperate R dataframe files for further analysis. The process is described below\n\nLets load all functions from the previous chapter. This is a hassle since they are stored in Quarto markdown language, and R only accept real R code.\n\n\nCode\n# Clean workspace\nrm(list = ls())\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(lubridate)\n\n# Most dependencies are loaded by loading datafiltering.qmd.\n# To load only the chuncks containing functions we need parsermd\nlibrary(parsermd)\n\ntoload &lt;- c(\"load_data\",\"load_plots\", \"filter_data\", \"separate_lifts\", \"visualise_seperate_lifts\")\nrmd &lt;- parse_rmd(\"datafiltering.qmd\")\n\n\nfor (i in seq_along(toload)) {\n  setup_chunk &lt;- rmd_select(rmd, toload[i]) |&gt; \n    as_document()\n\n  setup_chunk &lt;- setup_chunk[-grep(\"```\", setup_chunk)]\n  setup_chunk\n#&gt; [1] \"library(tidyr)\"   \"library(stringr)\" \"\"                \n\n  eval(parse(text = setup_chunk))             \n}\nrm(rmd, i, setup_chunk, toload)",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html#peak-accelerations",
    "href": "analysis_of_lifst.html#peak-accelerations",
    "title": "Analysis of lifts",
    "section": "Peak accelerations",
    "text": "Peak accelerations\nLets create a function to find the maximum value for a specific attribute for each lift (such as the absolute acceleration or gyr of one of the IMU’s)\n\n\nCode\n## Loading correct data\nfind_col_max &lt;- function(df_list, colname) {\n  # Initialize a matrix to store the maximum values\n  col_max &lt;- matrix(NA, nrow = length(df_list), ncol = 5)\n  colnames(col_max) &lt;- c(paste0(colname, \"1\"), paste0(colname, \"2\"), paste0(colname, \"3\"), paste0(colname, \"4\"), paste0(colname, \"5\"))\n  \n  for (i in seq_along(df_list)) {\n    df &lt;- df_list[[i]]\n    for (j in 1:5) {\n      col_max[i, j] &lt;- max(df[[paste0(colname, j)]], na.rm = TRUE)\n    }\n  }\n  # Convert the matrix to a dataframe\n  col_max_df &lt;- as.data.frame(col_max)\n  \n  return(col_max_df)\n}\n\n\nLets check if the function works as intended by loading the absolute acceleration of the barbells (showing only first 10 rows)\n\n\nCode\nmale_max_abs &lt;- find_col_max(data_male, \"A_abs\")\nmale_max_gyr &lt;- find_col_max(data_male, \"Gyr_abs\")\nfemale_max_abs &lt;- find_col_max(data_female, \"A_abs\")\nfemale_max_gyr &lt;- find_col_max(data_female, \"Gyr_abs\")\n\nmale_max &lt;- cbind(male_max_abs, male_max_gyr)\nfemale_max &lt;- cbind(female_max_abs, female_max_gyr)\n# Add a group column to each dataframe\nmale_max &lt;- male_max %&gt;% mutate(group = \"male\")\nfemale_max &lt;- female_max %&gt;% mutate(group = \"female\")\n\n# Combine the dataframes\ndata &lt;- bind_rows(male_max, female_max)\n\n\n# Showing only first 10 rows\nkable(data[1:10, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\ngroup\n\n\n\n\n19.45938\n123.70582\n68.15013\n37.94942\n31.23898\n30.70867\n447.3661\n378.9496\n152.0784\n405.7006\nmale\n\n\n19.45938\n123.70582\n68.15013\n37.94942\n31.23898\n30.70867\n447.3661\n378.9496\n152.0784\n405.7006\nmale\n\n\n17.93894\n134.62920\n87.32489\n55.14171\n31.51992\n12.95801\n510.0824\n415.3523\n167.5220\n439.2996\nmale\n\n\n17.93894\n134.62920\n87.32489\n55.14171\n31.51992\n12.95801\n510.0824\n415.3523\n167.5220\n439.2996\nmale\n\n\n16.34248\n119.93698\n79.20782\n42.85987\n27.21415\n20.19276\n571.6062\n416.3854\n151.0167\n416.2550\nmale\n\n\n16.34248\n119.93698\n79.20782\n42.85987\n27.21415\n20.19276\n571.6062\n416.3854\n151.0167\n416.2550\nmale\n\n\n15.98943\n145.17490\n59.99045\n37.66462\n19.23075\n16.90985\n472.4933\n408.2328\n189.7104\n408.4898\nmale\n\n\n15.98943\n145.17490\n59.99045\n37.66462\n19.23075\n16.90985\n472.4933\n408.2328\n189.7104\n408.4898\nmale\n\n\n16.22577\n79.54606\n62.68990\n32.99983\n25.64038\n45.76053\n591.9665\n391.2921\n176.5874\n480.1031\nmale\n\n\n16.22577\n79.54606\n62.68990\n32.99983\n25.64038\n45.76053\n591.9665\n391.2921\n176.5874\n480.1031\nmale\n\n\n\n\n\nNow lets define a function to do statistical tests. This function has some logic so it automatically uses the correct t-test (studens, welch’s or mann-whithney based on the results of levenes test and shapiro-wilk tests)\n\n\nCode\nlibrary(car)\n\n# Define the function\ncompare_groups &lt;- function(column1, column2, group1_name = \"Male\", group2_name = \"Female\") {\n  # Create dataframes\n  group1 &lt;- data.frame(Value = column1, Group = group1_name)\n  group2 &lt;- data.frame(Value = column2, Group = group2_name)\n  \n  # Check for normality (Shapiro-Wilk test)\n  shapiro_test_group1 &lt;- shapiro.test(group1$Value)\n  shapiro_test_group2 &lt;- shapiro.test(group2$Value)\n  \n  # Print normality test results\n  cat(\"Shapiro-Wilk Test for Normality\\n\")\n  print(shapiro_test_group1)\n  print(shapiro_test_group2)\n  \n  # Combine the dataframes\n  combined_data &lt;- rbind(group1, group2)\n  \n  # Check for equal variance (Levene's test)\n  levene_test &lt;- leveneTest(Value ~ Group, data = combined_data)\n  \n  # Print Levene's test result\n  cat(\"Levene's Test for Equality of Variances\\n\")\n  print(levene_test)\n  \n  # Logic to run the appropriate test\n  if (shapiro_test_group1$p.value &gt; 0.05 && shapiro_test_group2$p.value &gt; 0.05) {\n    # Normal distribution\n    if (levene_test$`Pr(&gt;F)`[1] &gt; 0.05) {\n      # Equal variances\n      cat(\"Normal distribution and equal variances (Student's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = TRUE)\n    } else {\n      # Unequal variances\n      cat(\"Normal distribution, unequal variances (Welch's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = FALSE)\n    }\n  } else {\n    # No normality\n    cat(\"No normality (Mann-Whitney U test)\\n\")\n    t_test_result &lt;- wilcox.test(column1, column2)\n  }\n  print(t_test_result)\n}\n\n\nLets test the function with the male and female maximum absolute acceleration of the barbell IMU\n\n\nCode\n# Example usage\n# Assuming abs_max_male and abs_max_female are your dataframes and A_abs1 is the column to compare\nresult &lt;- compare_groups(data$A_abs1, data$A_abs1, group1_name = \"male\", group2_name = \"female\")\n\n\nShapiro-Wilk Test for Normality\n\n    Shapiro-Wilk normality test\n\ndata:  group1$Value\nW = 0.93966, p-value = 0.08073\n\n\n    Shapiro-Wilk normality test\n\ndata:  group2$Value\nW = 0.93966, p-value = 0.08073\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Equality of Variances\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1       0      1\n      60               \nNormal distribution and equal variances (Student's t-test)\n\n    Two Sample t-test\n\ndata:  column1 and column2\nt = 0, df = 60, p-value = 1\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8151582  0.8151582\nsample estimates:\nmean of x mean of y \n 18.74167  18.74167 \n\n\nObviously, we do not want to do many T-tests due to inflated familywise error rates. Thus we should perform a MANVOA #### #### ### #### ####\n\n\nCode\n# Define a custom function to format mean and standard deviation\nformat_mean_sd &lt;- function(x) {\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  sd_x &lt;- sd(x, na.rm = TRUE)\n  sprintf(\"%.2f ± %.2f\", mean_x, sd_x)\n}\n\n# Calculate mean, standard deviation, and count for each column by group\nresults &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    across(where(is.numeric), format_mean_sd)\n  )\n\n# Display the results in a nice table\nresults %&gt;%\n  kable(caption = \"Summary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\")\n\n\n\nSummary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nn\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\n\n\n\n\nfemale\n19.00 ± NA\n19.61 ± 1.08\n47.11 ± 16.97\n25.36 ± 6.29\n23.30 ± 3.84\n21.68 ± 3.33\n310.10 ± 134.92\n371.86 ± 125.60\n278.59 ± 118.32\n160.80 ± 32.10\n438.74 ± 35.54\n\n\nmale\n12.00 ± NA\n17.37 ± 1.33\n106.64 ± 38.95\n63.54 ± 20.96\n37.54 ± 11.42\n26.08 ± 4.76\n99.40 ± 173.40\n498.98 ± 70.24\n425.90 ± 57.43\n157.01 ± 28.01\n418.80 ± 37.12\n\n\n\n\n\nNow the statistics for the particiapnt’s characteristics\nLets confirm normality and equality of variances\nLets run an MANOVA and calculate effect sizes\n\n\nCode\n# Load necessary packages\nlibrary(car)\nlibrary(MVN)\nlibrary(purrr)\nlibrary(effectsize)  # For effect size calculation\n\n\nWarning: package 'effectsize' was built under R version 4.3.3\n\n\nCode\n# Assuming 'data' is your dataframe and 'Sex' column indicates 'male' or 'female'\n# Separate numerical columns and the 'Sex' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\ngroup_data &lt;- data$group\n\n# Check for normality for each numerical column and each group (male, female)\nnormality_results &lt;- lapply(names(numerical_data), function(col) {\n  male_normality &lt;- shapiro.test(numerical_data[group_data == \"male\", col])$p.value\n  female_normality &lt;- shapiro.test(numerical_data[group_data == \"female\", col])$p.value\n  return(c(male_normality, female_normality))\n})\n# Combine normality results into a dataframe\nnormality_results_df &lt;- do.call(rbind, normality_results)\ncolnames(normality_results_df) &lt;- c(\"Male_p_value\", \"Female_p_value\")\nrownames(normality_results_df) &lt;- names(numerical_data)\nprint(normality_results_df)\n\n\n         Male_p_value Female_p_value\nA_abs1   2.854878e-02   4.619671e-01\nA_abs2   1.931507e-02   7.249080e-02\nA_abs3   3.517357e-02   1.858413e-04\nA_abs4   1.928614e-01   8.951810e-03\nA_abs5   8.402953e-02   3.976160e-04\nGyr_abs1 2.489356e-05   2.471050e-03\nGyr_abs2 2.069149e-01   3.043955e-05\nGyr_abs3 5.393877e-04   2.329259e-06\nGyr_abs4 5.615732e-02   1.014058e-04\nGyr_abs5 1.944619e-01   1.859017e-02\n\n\nCode\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n# Perform Levene's test for each numerical column\nfor (col in names(data)[numerical_columns]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$group)\n  levene_results[[col]] &lt;- test_result\n}\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nCode\n# Extract \"F value\" and \"Pr(&gt;F)\" columns from each test result and bind them into a dataframe\nlevene_results_df &lt;- map_dfr(levene_results, ~{\n  data.frame(\n    F_value = .x$F[1],    # Extract the first value of F\n    Pr_value = .x$\"Pr(&gt;F)\"[1]  # Extract the first value of Pr(&gt;F)\n  )\n})\n\n# View the dataframe\nprint(levene_results_df)\n\n\n      F_value    Pr_value\n1  1.83995079 0.185423775\n2  3.63208611 0.066631868\n3  9.09645837 0.005283311\n4  5.75789660 0.023062104\n5  3.46179709 0.072970011\n6  0.02008831 0.888270779\n7  0.14657819 0.704618992\n8  0.47705054 0.495252947\n9  0.08428623 0.773636728\n10 0.00715751 0.933159109\n\n\nCode\nlevene_p_values &lt;- levene_results_df$Pr_value\n# Check if all p-values &gt; 0.05 for normality assumption\nall_normal &lt;- all(normality_results_df &gt; 0.05) & all(levene_p_values &lt; 0.05)\n\nif (all_normal) {\n  # Perform MANOVA\n  formula &lt;- as.formula(paste(\"cbind(\", paste(names(numerical_data), collapse = \", \"), \") ~ group\"))\n  manova_result &lt;- manova(formula, data = data)\n  manova_summary &lt;- summary(manova_result, test = \"Pillai\")\n  print(manova_summary)\n  \n  # Check if the MANOVA is significant\n  if (manova_summary$stats[1, \"Pr(&gt;F)\"] &lt; 0.05) {\n    print(\"MANOVA is significant. Performing univariate ANOVAs:\")\n    # Perform univariate ANOVAs for each dependent variable\n    univariate_results &lt;- summary.aov(manova_result)\n    print(univariate_results)\n    \n    # Extracting p-values and means for interpretation\n    for (var in names(numerical_data)) {\n      print(paste(\"Analyzing variable:\", var))\n      aov_result &lt;- aov(as.formula(paste(var, \"~ group\")), data = data)\n      print(summary(aov_result))\n      print(model.tables(aov_result, \"means\"), digits = 3)\n      \n      # Calculate effect size (eta-squared)\n      eta_squared_result &lt;- eta_squared(aov_result)\n      print(eta_squared_result)\n    }\n  } else {\n    print(\"MANOVA is not significant. No further tests are performed.\")\n  }\n} else {\n  # If normality is not met, use Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract p-value and calculate effect size (not applicable for Kruskal-Wallis)\n    p_value &lt;- kruskal_result$p.value\n    print(paste(\"P-value:\", p_value))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"P-value: 0.000352650198177613\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"P-value: 0.000352650198177613\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 16.487, df = 1, p-value = 4.898e-05\n\n[1] \"P-value: 4.8978154674227e-05\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 9.523, df = 1, p-value = 0.002029\n\n[1] \"P-value: 0.00202908222598898\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9354, df = 1, p-value = 0.01484\n\n[1] \"P-value: 0.0148395949690251\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 11.086, df = 1, p-value = 0.0008698\n\n[1] \"P-value: 0.000869804231618486\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 11.633, df = 1, p-value = 0.0006478\n\n[1] \"P-value: 0.000647777430091623\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 13.955, df = 1, p-value = 0.0001873\n\n[1] \"P-value: 0.000187257491153609\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 0.42207, df = 1, p-value = 0.5159\n\n[1] \"P-value: 0.515904079141032\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 1.4839, df = 1, p-value = 0.2232\n\n[1] \"P-value: 0.22317263688255\"\n\n\nCode\n# Initialize an empty dataframe to store the results\nkruskal_results_df &lt;- data.frame(\n  Variable = character(),\n  Test_Statistic = numeric(),\n  Degrees_of_Freedom = numeric(),\n  P_Value = numeric(),\n  stringsAsFactors = FALSE\n)\n\nif (!all_normal) {\n  # If normality assumptions are not met, perform Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract relevant information and add to the dataframe\n    p_value &lt;- kruskal_result$p.value\n    test_statistic &lt;- kruskal_result$statistic\n    degrees_freedom &lt;- kruskal_result$parameter\n    kruskal_results_df &lt;- rbind(kruskal_results_df, data.frame(\n      Variable = var,\n      Test_Statistic = test_statistic,\n      Degrees_of_Freedom = degrees_freedom,\n      P_Value = p_value\n    ))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 12.768, df = 1, p-value = 0.0003527\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 16.487, df = 1, p-value = 4.898e-05\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 9.523, df = 1, p-value = 0.002029\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9354, df = 1, p-value = 0.01484\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 11.086, df = 1, p-value = 0.0008698\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 11.633, df = 1, p-value = 0.0006478\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 13.955, df = 1, p-value = 0.0001873\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 0.42207, df = 1, p-value = 0.5159\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 1.4839, df = 1, p-value = 0.2232\n\n\nCode\n# Print the dataframe of Kruskal-Wallis test results\nprint(kruskal_results_df)\n\n\n                            Variable Test_Statistic Degrees_of_Freedom\nKruskal-Wallis chi-squared    A_abs1     12.7677318                  1\nKruskal-Wallis chi-squared1   A_abs2     12.7677318                  1\nKruskal-Wallis chi-squared2   A_abs3     16.4872569                  1\nKruskal-Wallis chi-squared3   A_abs4      9.5230396                  1\nKruskal-Wallis chi-squared4   A_abs5      5.9354125                  1\nKruskal-Wallis chi-squared5 Gyr_abs1     11.0860316                  1\nKruskal-Wallis chi-squared6 Gyr_abs2     11.6334085                  1\nKruskal-Wallis chi-squared7 Gyr_abs3     13.9548143                  1\nKruskal-Wallis chi-squared8 Gyr_abs4      0.4220738                  1\nKruskal-Wallis chi-squared9 Gyr_abs5      1.4838531                  1\n                                 P_Value\nKruskal-Wallis chi-squared  3.526502e-04\nKruskal-Wallis chi-squared1 3.526502e-04\nKruskal-Wallis chi-squared2 4.897815e-05\nKruskal-Wallis chi-squared3 2.029082e-03\nKruskal-Wallis chi-squared4 1.483959e-02\nKruskal-Wallis chi-squared5 8.698042e-04\nKruskal-Wallis chi-squared6 6.477774e-04\nKruskal-Wallis chi-squared7 1.872575e-04\nKruskal-Wallis chi-squared8 5.159041e-01\nKruskal-Wallis chi-squared9 2.231726e-01\n\n\nCode\n# Print the dataframe without row names and the \"Degrees_of_Freedom\" column, with P-values formatted\nkruskal_results_df$Variable[1] &lt;- \"Barbell PLA\"\nkruskal_results_df$Variable[2] &lt;- \"Lower leg PLA\"\nkruskal_results_df$Variable[3] &lt;- \"Upper leg PLA\"\nkruskal_results_df$Variable[4] &lt;- \"Torso PLA\"\nkruskal_results_df$Variable[5] &lt;- \"Upper arm PLA\"\nkruskal_results_df$Variable[6] &lt;- \"Barbell PAA\"\nkruskal_results_df$Variable[7] &lt;- \"Lower leg PAA\"\nkruskal_results_df$Variable[8] &lt;- \"Upper leg PAA\"\nkruskal_results_df$Variable[9] &lt;- \"Torso PAA\"\nkruskal_results_df$Variable[10] &lt;- \"Upper arm PAA\"\nprint(format(kruskal_results_df[, -which(names(kruskal_results_df) == \"Degrees_of_Freedom\")], scientific = FALSE), row.names = FALSE)\n\n\n      Variable Test_Statistic       P_Value\n   Barbell PLA     12.7677318 0.00035265020\n Lower leg PLA     12.7677318 0.00035265020\n Upper leg PLA     16.4872569 0.00004897815\n     Torso PLA      9.5230396 0.00202908223\n Upper arm PLA      5.9354125 0.01483959497\n   Barbell PAA     11.0860316 0.00086980423\n Lower leg PAA     11.6334085 0.00064777743\n Upper leg PAA     13.9548143 0.00018725749\n     Torso PAA      0.4220738 0.51590407914\n Upper arm PAA      1.4838531 0.22317263688\n\n\nCalculate the Point-biserial correlation coefficient\n\n\nCode\n# Initialize an empty vector to store the point-biserial correlation coefficients\npoint_biserial_coeffs &lt;- numeric()\n\n# Loop through each column in the dataset (excluding the grouping variable)\nfor (col in names(data)[!names(data) %in% c(\"group\")]) {\n  # Calculate the point-biserial correlation coefficient\n  point_biserial &lt;- cor(data[[col]], as.numeric(data$group == \"female\"))  # Convert \"female\" to 1 and \"male\" to 0\n  # Store the coefficient in the vector\n  point_biserial_coeffs &lt;- c(point_biserial_coeffs, point_biserial)\n}\n\n# Create a dataframe to display the results\npb_results_df &lt;- data.frame(\n  Variable = names(data)[!names(data) %in% c(\"group\")],  # Variable names (excluding grouping variable)\n  Point_Biserial_Coefficient = point_biserial_coeffs  # Point-biserial correlation coefficients\n)\n\n# Print the results dataframe\npb_results_df$Variable[1] &lt;- \"Barbell PLA\"\npb_results_df$Variable[2] &lt;- \"Lower leg PLA\"\npb_results_df$Variable[3] &lt;- \"Upper leg PLA\"\npb_results_df$Variable[4] &lt;- \"Torso PLA\"\npb_results_df$Variable[5] &lt;- \"Upper arm PLA\"\npb_results_df$Variable[6] &lt;- \"Barbell PAA\"\npb_results_df$Variable[7] &lt;- \"Lower leg PAA\"\npb_results_df$Variable[8] &lt;- \"Upper leg PAA\"\npb_results_df$Variable[9] &lt;- \"Torso PAA\"\npb_results_df$Variable[10] &lt;- \"Upper arm PAA\"\nprint(pb_results_df)\n\n\n        Variable Point_Biserial_Coefficient\n1    Barbell PLA                 0.68903217\n2  Lower leg PLA                -0.73738706\n3  Upper leg PLA                -0.81182778\n4      Torso PLA                -0.68372236\n5  Upper arm PLA                -0.49035474\n6    Barbell PAA                 0.57577712\n7  Lower leg PAA                -0.50994398\n8  Upper leg PAA                -0.59694920\n9      Torso PAA                 0.06232652\n10 Upper arm PAA                 0.26762921",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Thesis",
    "section": "Methods",
    "text": "Methods\nA within-subject repeated measures research design was used to determine the kinetics of the S2O under different loads, and the corresponding shifts in preferred technique. IMU’s were placed on the right side of the barbell, lower arm, trunk, upper thighs and lower leg. Kinematic data was collected at [x Hz] and filtered with an [filter]\nParticipants were asked to press progressively loaded barbells overhead and instructed to SP as long as possible before switching to PP, and PP as long as possible before switching to PJ.\nDefining movements Technical aspects of the exercises have been well documented and defined elsewhere (19, 12). Differentiating visually can be error prone. Therefore, the movement performed is determined by the kinetics of the barbell, trunk, thighs and legs.\nStrict press Strict presses were defined as those movements where the barbell is pressed overhead in one upward motion, without any (prior) downward motion of the trunk or thighs. The lifter starts with the barbell in front-rack with their preferred grip width. The barbell should be presses overhead by extending the elbows and flexing the shoulders. The legs must not be involved. The lift is not disqualified if the barbell itself has downward motion at some point, as long as it eventually finishes overhead.\nPush press Generally the same set-up as in the strict press is used, but the movement starts with counter-movement: by dipping down and coming up the lifter can utilize the power of knee and hip extension, which generates additional upwards momentum. Push presses were defined as those movements where the barbell, trunk and possibly thighs have downward momentum proceeding the upward momentum.\nPush jerk Again, the same general set-up applies, but after accelerating the barbell upward, the athlete dips downward to catch the barbell in a lower position, thus requiring less upward momentum, and then stands up with the bar. PJ were defined as those movements where the momentum of the barbell, trunk or thighs starts downwards, and then reverses three times.\nTesting procedures Testing started with a warming-up protocol of two sets of 10 repetition of exercise specific drills: airsquats, front squats, SP, PP, PJ). After the warming up the participants were asked to rotate arround their vertical axis once to demarcate the starting of the testing in the IMU’s. Testing started with an empty 20kg barbell for the males, of 15kg barbell for the females, as is standard for olympic weightlifting. Participants were asked to press the barbell overhead, while retaining the SP and PP technique as long as possible. After each successful lift the barbell was loaded with an additional 5kg, unless the participants self-repoted an 1RM PJ of less then 80kg, in which case incremental steps of 2.5kg were used. Each participant was allowed reattempts, but testing stopped after more than two consecutive misses. Rest time between attempts was not allowed to exceed 2 minutes.\nStatistical analysis An a priori alpha level was set at p &lt;= 0.05.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Thesis",
    "section": "Introduction",
    "text": "Introduction\nThe ability to generate high power -the product of force and velocity- is essential for both daily activities and occupational tasks. It is widely acknowledged to be correlated with overall good health, and retaining power is considered a significant indicator of healthy ageing. Moreover, in the real of sports, power output is regarded as a strong determinant of athletic success. Consequently, the evaluation of muscular power and the development of training strategies aimed at enhancing power are of considerable interest to sport coaches, healthcare workers and researchers alike, and is reflected in the considerable attention focused on the development of power in athletes, and the research examining specific techniques used to maximize power development.\nExploratory investigations have delved into the power performances across various athletic disciplines, reporting Olympic weightlifting (OLY) as one of the disciplines where athletes demonstrated some of the highest absolute and relative peak power outputs. Consequently, OLY has been proposed as an effective means of enhancing peak power capacities. OLY involves lifting a loaded barbell off the ground to an overhead locked position, in either the Clean and Jerk (C&J) or Snatch, requiring high force in limited time to overcome gravity (5)\nThese assertions are supported by the biomechanical mechanics inherent to OLY exercises. Firstly, OLY exercises mimic sport-specific movements by engaging in forceful triple extension patterns involving the hips, knees and ankles. Secondly, they facilitate the generation of high rates of force development and power output.\nFurther justification for the inclusion of OLY exercises in training programs is provided by research highlighting correlations between OLY performance and various athletic attributes. Studies have shown significant associations between the hang power clean and sprinting (r = -0.58, p&lt;0.01), jumping (r=0.41, p &lt; 0.05) and change of direction tasks (r = -0.41, p &lt; 0.05). Additionally, a recent meta-analysis emphasis the efficacy of incorporating OLY exercises and their derivatives into training regimens, particularly in improving jumping performance compared to traditional resistance training, with a notable ~5% difference (effect size [ES] = 0.64, p &lt; 0.001) (Hackett et al., 2016).\nStudies have delved into the mechanics underlying these correlations. OLY exercises trigger hormonal responses similar to those observed following conventional strength and hypertrophy protocols. Additionally, cross-sectional data suggests that OLY training induces a transformation from type IIX to type-IIA muscle fibers, accompanied by hypertrophy specifically in type II fibers. This transfer confers advantages for maximal force production. As a result, weightlifters exhibit approximately 15-20% higher isometric peak force and a 13-16% faster contractile rate of force development compared to other strength and power athletes.\nFurthermore, OLY training has been found to reduce the typical sex-related gap in neuromuscular strength and power expression. However, this apparent sex-related difference seems to amplify with advancing adult age, revealing that women undergo a more pronounced age-related decline in muscle shortening velocity and peak power output when compared with men.\nNumerous studies have investigated the impact of sex on strength and other indices of musclular performance, consistently reporting lower levels of strength in females, even after accounting for confounding factors such as body weight. Some research has examined the relationship between shoulder strength and the Push Jerk (PJ), a movement where a barbell is propelled overhead, reporting differences in the ratio between the Strict Press and PJ across sexes. Generally there are three distinct manners of moving a barbell overhead: the Strict Press (SP), where exclusively the upper-limb strength is utilized to move the barbell; The Push Press (PP), involving a slight downward dip to engage the power of the triple extension; or the Push Jerk (PJ), allowing for catching the barbell at a lower position. Studies have indicated differences in the ratio between the SP and PJ across sexes. This has raised the question at what relative intensity movement patters transition from SP to PP, and form PP to PJ, and wether these thresholds differ between males and females. Given that males exhibit a higher SP-to-PJ ratio, it is hypothesized that female will transition to altered movement patterns at lower relative intensities.\nThe primary objective of this study is to investigate the relative intensities at which the movement pattern shifts from performing an overhead barbell press to push press, and from a push press to push jerk. Additionally, this study aims to assess the magnitude of these differences between male and female participants.",
    "crumbs": [
      "Thesis",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Full text</span>"
    ]
  },
  {
    "objectID": "participants.html",
    "href": "participants.html",
    "title": "Participants",
    "section": "",
    "text": "Analysis of participants’ characteristics\nLets analyse participants characteristics for equality of variance, normality.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst_mean_values.html#peak-accelerations",
    "href": "analysis_of_lifst_mean_values.html#peak-accelerations",
    "title": "Analysis of lifts mean values",
    "section": "Peak accelerations",
    "text": "Peak accelerations\nLets create a function to find the maximum value for a specific attribute for each lift (such as the absolute acceleration or gyr of one of the IMU’s)\n\n\nCode\nfind_col_means &lt;- function(df_list, colname) {\n  # Initialize a matrix to store the column means\n  col_means &lt;- matrix(NA, nrow = length(df_list), ncol = 5)\n  colnames(col_means) &lt;- c(paste0(colname, \"1\"), paste0(colname, \"2\"), paste0(colname, \"3\"), paste0(colname, \"4\"), paste0(colname, \"5\"))\n  \n  for (i in seq_along(df_list)) {\n    df &lt;- df_list[[i]]\n    for (j in 1:5) {\n      col_means[i, j] &lt;- mean(df[[paste0(colname, j)]], na.rm = TRUE)\n    }\n  }\n  # Convert the matrix to a dataframe\n  col_means_df &lt;- as.data.frame(col_means)\n  \n  return(col_means_df)\n}\n\n\nLets check if the function works as intended by loading the absolute acceleration of the barbells (showing only first 10 rows)\n\n\nCode\n# Find the column means for male group\nmale_avg_abs &lt;- find_col_means(data_male, \"A_abs\")\nmale_avg_gyr &lt;- find_col_means(data_male, \"Gyr_abs\")\n\n# Find the column means for female group\nfemale_avg_abs &lt;- find_col_means(data_female, \"A_abs\")\nfemale_avg_gyr &lt;- find_col_means(data_female, \"Gyr_abs\")\n\n\nmale_avg &lt;- cbind(male_avg_abs, male_avg_gyr)\nfemale_avg &lt;- cbind(female_avg_abs, female_avg_gyr)\n# Add a group column to each dataframe\nmale_avg &lt;- male_avg %&gt;% mutate(group = \"male\")\nfemale_avg &lt;- female_avg %&gt;% mutate(group = \"female\")\n\n# Combine the dataframes\ndata &lt;- bind_rows(male_avg, female_avg)\n\n\n# Showing only first 10 rows\nkable(data[1:10, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\ngroup\n\n\n\n\n9.868109\n13.17767\n12.78585\n10.94101\n11.43829\n7.107748\n89.39781\n103.86579\n41.85146\n87.08400\nmale\n\n\n9.868109\n13.17767\n12.78585\n10.94101\n11.43829\n7.107748\n89.39781\n103.86579\n41.85146\n87.08400\nmale\n\n\n9.812543\n14.07092\n13.09411\n11.05630\n11.00833\n8.275944\n98.22886\n100.09508\n43.27235\n82.99422\nmale\n\n\n9.812543\n14.07092\n13.09411\n11.05630\n11.00833\n8.275944\n98.22886\n100.09508\n43.27235\n82.99422\nmale\n\n\n9.785642\n13.84012\n13.31589\n11.13770\n11.05129\n6.760488\n89.39743\n93.60903\n41.06197\n82.98281\nmale\n\n\n9.785642\n13.84012\n13.31589\n11.13770\n11.05129\n6.760488\n89.39743\n93.60903\n41.06197\n82.98281\nmale\n\n\n9.778011\n14.00533\n13.22083\n11.01086\n10.83578\n4.812203\n98.42327\n88.54224\n40.06628\n69.36684\nmale\n\n\n9.778011\n14.00533\n13.22083\n11.01086\n10.83578\n4.812203\n98.42327\n88.54224\n40.06628\n69.36684\nmale\n\n\n8.276597\n15.05094\n13.53958\n11.29214\n11.17770\n8.077316\n129.77213\n97.32199\n44.18114\n118.39520\nmale\n\n\n8.276597\n15.05094\n13.53958\n11.29214\n11.17770\n8.077316\n129.77213\n97.32199\n44.18114\n118.39520\nmale\n\n\n\n\n\nNow lets define a function to do statistical tests. This function has some logic so it automatically uses the correct t-test (studens, welch’s or mann-whithney based on the results of levenes test and shapiro-wilk tests)\n\n\nCode\nlibrary(car)\n\n# Define the function\ncompare_groups &lt;- function(column1, column2, group1_name = \"Male\", group2_name = \"Female\") {\n  # Create dataframes\n  group1 &lt;- data.frame(Value = column1, Group = group1_name)\n  group2 &lt;- data.frame(Value = column2, Group = group2_name)\n  \n  # Check for normality (Shapiro-Wilk test)\n  shapiro_test_group1 &lt;- shapiro.test(group1$Value)\n  shapiro_test_group2 &lt;- shapiro.test(group2$Value)\n  \n  # Print normality test results\n  cat(\"Shapiro-Wilk Test for Normality\\n\")\n  print(shapiro_test_group1)\n  print(shapiro_test_group2)\n  \n  # Combine the dataframes\n  combined_data &lt;- rbind(group1, group2)\n  \n  # Check for equal variance (Levene's test)\n  levene_test &lt;- leveneTest(Value ~ Group, data = combined_data)\n  \n  # Print Levene's test result\n  cat(\"Levene's Test for Equality of Variances\\n\")\n  print(levene_test)\n  \n  # Logic to run the appropriate test\n  if (shapiro_test_group1$p.value &gt; 0.05 && shapiro_test_group2$p.value &gt; 0.05) {\n    # Normal distribution\n    if (levene_test$`Pr(&gt;F)`[1] &gt; 0.05) {\n      # Equal variances\n      cat(\"Normal distribution and equal variances (Student's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = TRUE)\n    } else {\n      # Unequal variances\n      cat(\"Normal distribution, unequal variances (Welch's t-test)\\n\")\n      t_test_result &lt;- t.test(column1, column2, var.equal = FALSE)\n    }\n  } else {\n    # No normality\n    cat(\"No normality (Mann-Whitney U test)\\n\")\n    t_test_result &lt;- wilcox.test(column1, column2)\n  }\n  print(t_test_result)\n}\n\n\nLets test the function with the male and female maximum absolute acceleration of the barbell IMU\n\n\nCode\n# Example usage\n# Assuming abs_max_male and abs_max_female are your dataframes and A_abs1 is the column to compare\nresult &lt;- compare_groups(data$A_abs1, data$A_abs1, group1_name = \"male\", group2_name = \"female\")\n\n\nShapiro-Wilk Test for Normality\n\n    Shapiro-Wilk normality test\n\ndata:  group1$Value\nW = 0.54891, p-value = 1.288e-08\n\n\n    Shapiro-Wilk normality test\n\ndata:  group2$Value\nW = 0.54891, p-value = 1.288e-08\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Equality of Variances\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1       0      1\n      60               \nNo normality (Mann-Whitney U test)\n\n\nWarning in wilcox.test.default(column1, column2): cannot compute exact p-value\nwith ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  column1 and column2\nW = 480.5, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nObviously, we do not want to do many T-tests due to inflated familywise error rates. Thus we should perform a MANVOA #### #### ### #### ####\n\n\nCode\n# Define a custom function to format mean and standard deviation\nformat_mean_sd &lt;- function(x) {\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  sd_x &lt;- sd(x, na.rm = TRUE)\n  sprintf(\"%.2f ± %.2f\", mean_x, sd_x)\n}\n\n# Calculate mean, standard deviation, and count for each column by group\nresults &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  summarise(\n    n = n(),\n    across(where(is.numeric), format_mean_sd)\n  )\n\n# Display the results in a nice table\nresults %&gt;%\n  kable(caption = \"Summary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\")\n\n\n\nSummary Statistics by Group (Number of Participants, Mean ± Standard Deviation)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nn\nA_abs1\nA_abs2\nA_abs3\nA_abs4\nA_abs5\nGyr_abs1\nGyr_abs2\nGyr_abs3\nGyr_abs4\nGyr_abs5\n\n\n\n\nfemale\n19.00 ± NA\n9.73 ± 0.14\n12.16 ± 0.64\n11.12 ± 0.27\n10.56 ± 0.15\n10.84 ± 0.25\n44.71 ± 7.68\n73.40 ± 13.26\n63.84 ± 14.58\n34.84 ± 5.99\n89.52 ± 15.88\n\n\nmale\n12.00 ± NA\n9.53 ± 0.59\n13.64 ± 1.06\n12.87 ± 0.78\n10.98 ± 0.28\n11.08 ± 0.20\n14.61 ± 17.81\n94.85 ± 20.29\n97.77 ± 5.63\n39.82 ± 5.48\n86.26 ± 16.13\n\n\n\n\n\nNow the statistics for the particiapnt’s characteristics\nLets confirm normality and equality of variances\nLets run an MANOVA and calculate effect sizes\n\n\nCode\n# Load necessary packages\nlibrary(car)\nlibrary(MVN)\nlibrary(purrr)\nlibrary(effectsize)  # For effect size calculation\n\n\nWarning: package 'effectsize' was built under R version 4.3.3\n\n\nCode\n# Assuming 'data' is your dataframe and 'Sex' column indicates 'male' or 'female'\n# Separate numerical columns and the 'Sex' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\ngroup_data &lt;- data$group\n\n# Check for normality for each numerical column and each group (male, female)\nnormality_results &lt;- lapply(names(numerical_data), function(col) {\n  male_normality &lt;- shapiro.test(numerical_data[group_data == \"male\", col])$p.value\n  female_normality &lt;- shapiro.test(numerical_data[group_data == \"female\", col])$p.value\n  return(c(male_normality, female_normality))\n})\n# Combine normality results into a dataframe\nnormality_results_df &lt;- do.call(rbind, normality_results)\ncolnames(normality_results_df) &lt;- c(\"Male_p_value\", \"Female_p_value\")\nrownames(normality_results_df) &lt;- names(numerical_data)\nprint(normality_results_df)\n\n\n         Male_p_value Female_p_value\nA_abs1   5.501996e-05    0.504435357\nA_abs2   7.149120e-02    0.281492897\nA_abs3   1.645330e-03    0.027044509\nA_abs4   1.702656e-02    0.461098454\nA_abs5   1.029700e-01    0.154558903\nGyr_abs1 2.745057e-05    0.339941501\nGyr_abs2 5.932461e-02    0.015986803\nGyr_abs3 1.072579e-01    0.005786293\nGyr_abs4 7.850112e-04    0.631027461\nGyr_abs5 5.472550e-03    0.162971907\n\n\nCode\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n# Perform Levene's test for each numerical column\nfor (col in names(data)[numerical_columns]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$group)\n  levene_results[[col]] &lt;- test_result\n}\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nCode\n# Extract \"F value\" and \"Pr(&gt;F)\" columns from each test result and bind them into a dataframe\nlevene_results_df &lt;- map_dfr(levene_results, ~{\n  data.frame(\n    F_value = .x$F[1],    # Extract the first value of F\n    Pr_value = .x$\"Pr(&gt;F)\"[1]  # Extract the first value of Pr(&gt;F)\n  )\n})\n\n# View the dataframe\nprint(levene_results_df)\n\n\n     F_value   Pr_value\n1  2.2237647 0.14669981\n2  1.5492879 0.22320604\n3  3.4201089 0.07462357\n4  1.2658581 0.26977118\n5  1.0881306 0.30550397\n6  0.2832777 0.59861540\n7  1.1257677 0.29743678\n8  2.7473097 0.10819881\n9  0.4381282 0.51325311\n10 0.4849665 0.49172573\n\n\nCode\nlevene_p_values &lt;- levene_results_df$Pr_value\n# Check if all p-values &gt; 0.05 for normality assumption\nall_normal &lt;- all(normality_results_df &gt; 0.05) & all(levene_p_values &lt; 0.05)\n\nif (all_normal) {\n  # Perform MANOVA\n  formula &lt;- as.formula(paste(\"cbind(\", paste(names(numerical_data), collapse = \", \"), \") ~ group\"))\n  manova_result &lt;- manova(formula, data = data)\n  manova_summary &lt;- summary(manova_result, test = \"Pillai\")\n  print(manova_summary)\n  \n  # Check if the MANOVA is significant\n  if (manova_summary$stats[1, \"Pr(&gt;F)\"] &lt; 0.05) {\n    print(\"MANOVA is significant. Performing univariate ANOVAs:\")\n    # Perform univariate ANOVAs for each dependent variable\n    univariate_results &lt;- summary.aov(manova_result)\n    print(univariate_results)\n    \n    # Extracting p-values and means for interpretation\n    for (var in names(numerical_data)) {\n      print(paste(\"Analyzing variable:\", var))\n      aov_result &lt;- aov(as.formula(paste(var, \"~ group\")), data = data)\n      print(summary(aov_result))\n      print(model.tables(aov_result, \"means\"), digits = 3)\n      \n      # Calculate effect size (eta-squared)\n      eta_squared_result &lt;- eta_squared(aov_result)\n      print(eta_squared_result)\n    }\n  } else {\n    print(\"MANOVA is not significant. No further tests are performed.\")\n  }\n} else {\n  # If normality is not met, use Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract p-value and calculate effect size (not applicable for Kruskal-Wallis)\n    p_value &lt;- kruskal_result$p.value\n    print(paste(\"P-value:\", p_value))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 0.32289, df = 1, p-value = 0.5699\n\n[1] \"P-value: 0.56987648706177\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 10.023, df = 1, p-value = 0.001546\n\n[1] \"P-value: 0.0015461888770745\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 18.51, df = 1, p-value = 1.69e-05\n\n[1] \"P-value: 1.69004707588289e-05\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 11.624, df = 1, p-value = 0.0006511\n\n[1] \"P-value: 0.000651058298461034\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9306, df = 1, p-value = 0.01488\n\n[1] \"P-value: 0.014880021347098\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 12.184, df = 1, p-value = 0.000482\n\n[1] \"P-value: 0.000481979293686005\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 6.3326, df = 1, p-value = 0.01185\n\n[1] \"P-value: 0.011853959762315\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 17.139, df = 1, p-value = 3.473e-05\n\n[1] \"P-value: 3.4732658676603e-05\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 4.8038, df = 1, p-value = 0.0284\n\n[1] \"P-value: 0.0283970253776627\"\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 0.79734, df = 1, p-value = 0.3719\n\n[1] \"P-value: 0.371890258961034\"\n\n\nCode\n# Initialize an empty dataframe to store the results\nkruskal_results_df &lt;- data.frame(\n  Variable = character(),\n  Test_Statistic = numeric(),\n  Degrees_of_Freedom = numeric(),\n  P_Value = numeric(),\n  stringsAsFactors = FALSE\n)\n\nif (!all_normal) {\n  # If normality assumptions are not met, perform Multivariate Kruskal-Wallis test for each numerical column\n  for (var in names(numerical_data)) {\n    print(paste(\"Performing Multivariate Kruskal-Wallis test for variable:\", var))\n    kruskal_result &lt;- kruskal.test(as.formula(paste(var, \"~ group\")), data = data)\n    print(kruskal_result)\n    \n    # Extract relevant information and add to the dataframe\n    p_value &lt;- kruskal_result$p.value\n    test_statistic &lt;- kruskal_result$statistic\n    degrees_freedom &lt;- kruskal_result$parameter\n    kruskal_results_df &lt;- rbind(kruskal_results_df, data.frame(\n      Variable = var,\n      Test_Statistic = test_statistic,\n      Degrees_of_Freedom = degrees_freedom,\n      P_Value = p_value\n    ))\n  }\n}\n\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs1 by group\nKruskal-Wallis chi-squared = 0.32289, df = 1, p-value = 0.5699\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs2 by group\nKruskal-Wallis chi-squared = 10.023, df = 1, p-value = 0.001546\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs3 by group\nKruskal-Wallis chi-squared = 18.51, df = 1, p-value = 1.69e-05\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs4 by group\nKruskal-Wallis chi-squared = 11.624, df = 1, p-value = 0.0006511\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: A_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  A_abs5 by group\nKruskal-Wallis chi-squared = 5.9306, df = 1, p-value = 0.01488\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs1\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs1 by group\nKruskal-Wallis chi-squared = 12.184, df = 1, p-value = 0.000482\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs2\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs2 by group\nKruskal-Wallis chi-squared = 6.3326, df = 1, p-value = 0.01185\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs3\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs3 by group\nKruskal-Wallis chi-squared = 17.139, df = 1, p-value = 3.473e-05\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs4\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs4 by group\nKruskal-Wallis chi-squared = 4.8038, df = 1, p-value = 0.0284\n\n[1] \"Performing Multivariate Kruskal-Wallis test for variable: Gyr_abs5\"\n\n    Kruskal-Wallis rank sum test\n\ndata:  Gyr_abs5 by group\nKruskal-Wallis chi-squared = 0.79734, df = 1, p-value = 0.3719\n\n\nCode\n# Print the dataframe of Kruskal-Wallis test results\nprint(kruskal_results_df)\n\n\n                            Variable Test_Statistic Degrees_of_Freedom\nKruskal-Wallis chi-squared    A_abs1      0.3228892                  1\nKruskal-Wallis chi-squared1   A_abs2     10.0227447                  1\nKruskal-Wallis chi-squared2   A_abs3     18.5101182                  1\nKruskal-Wallis chi-squared3   A_abs4     11.6240116                  1\nKruskal-Wallis chi-squared4   A_abs5      5.9306181                  1\nKruskal-Wallis chi-squared5 Gyr_abs1     12.1841255                  1\nKruskal-Wallis chi-squared6 Gyr_abs2      6.3325823                  1\nKruskal-Wallis chi-squared7 Gyr_abs3     17.1394864                  1\nKruskal-Wallis chi-squared8 Gyr_abs4      4.8038007                  1\nKruskal-Wallis chi-squared9 Gyr_abs5      0.7973387                  1\n                                 P_Value\nKruskal-Wallis chi-squared  5.698765e-01\nKruskal-Wallis chi-squared1 1.546189e-03\nKruskal-Wallis chi-squared2 1.690047e-05\nKruskal-Wallis chi-squared3 6.510583e-04\nKruskal-Wallis chi-squared4 1.488002e-02\nKruskal-Wallis chi-squared5 4.819793e-04\nKruskal-Wallis chi-squared6 1.185396e-02\nKruskal-Wallis chi-squared7 3.473266e-05\nKruskal-Wallis chi-squared8 2.839703e-02\nKruskal-Wallis chi-squared9 3.718903e-01\n\n\nCode\n# Print the dataframe without row names and the \"Degrees_of_Freedom\" column, with P-values formatted\nkruskal_results_df$Variable[1] &lt;- \"Barbell ALA\"\nkruskal_results_df$Variable[2] &lt;- \"Lower leg ALA\"\nkruskal_results_df$Variable[3] &lt;- \"Upper leg ALA\"\nkruskal_results_df$Variable[4] &lt;- \"Torso ALA\"\nkruskal_results_df$Variable[5] &lt;- \"Upper arm ALA\"\nkruskal_results_df$Variable[6] &lt;- \"Barbell AAA\"\nkruskal_results_df$Variable[7] &lt;- \"Lower leg AAA\"\nkruskal_results_df$Variable[8] &lt;- \"Upper leg AAA\"\nkruskal_results_df$Variable[9] &lt;- \"Torso AAA\"\nkruskal_results_df$Variable[10] &lt;- \"Upper arm AAA\"\nprint(format(kruskal_results_df[, -which(names(kruskal_results_df) == \"Degrees_of_Freedom\")], scientific = FALSE), row.names = FALSE)\n\n\n      Variable Test_Statistic       P_Value\n   Barbell ALA      0.3228892 0.56987648706\n Lower leg ALA     10.0227447 0.00154618888\n Upper leg ALA     18.5101182 0.00001690047\n     Torso ALA     11.6240116 0.00065105830\n Upper arm ALA      5.9306181 0.01488002135\n   Barbell AAA     12.1841255 0.00048197929\n Lower leg AAA      6.3325823 0.01185395976\n Upper leg AAA     17.1394864 0.00003473266\n     Torso AAA      4.8038007 0.02839702538\n Upper arm AAA      0.7973387 0.37189025896\n\n\nCalculate the Point-biserial correlation coefficient\n\n\nCode\n# Initialize an empty vector to store the point-biserial correlation coefficients\npoint_biserial_coeffs &lt;- numeric()\n\n# Loop through each column in the dataset (excluding the grouping variable)\nfor (col in names(data)[!names(data) %in% c(\"group\")]) {\n  # Calculate the point-biserial correlation coefficient\n  point_biserial &lt;- cor(data[[col]], as.numeric(data$group == \"female\"))  # Convert \"female\" to 1 and \"male\" to 0\n  # Store the coefficient in the vector\n  point_biserial_coeffs &lt;- c(point_biserial_coeffs, point_biserial)\n}\n\n# Create a dataframe to display the results\npb_results_df &lt;- data.frame(\n  Variable = names(data)[!names(data) %in% c(\"group\")],  # Variable names (excluding grouping variable)\n  Point_Biserial_Coefficient = point_biserial_coeffs  # Point-biserial correlation coefficients\n)\n\n# Print the results dataframe\npb_results_df$Variable[1] &lt;- \"Barbell ALA\"\npb_results_df$Variable[2] &lt;- \"Lower leg ALA\"\npb_results_df$Variable[3] &lt;- \"Upper leg ALA\"\npb_results_df$Variable[4] &lt;- \"Torso ALA\"\npb_results_df$Variable[5] &lt;- \"Upper arm ALA\"\npb_results_df$Variable[6] &lt;- \"Barbell AAA\"\npb_results_df$Variable[7] &lt;- \"Lower leg AAA\"\npb_results_df$Variable[8] &lt;- \"Upper leg AAA\"\npb_results_df$Variable[9] &lt;- \"Torso AAA\"\npb_results_df$Variable[10] &lt;- \"Upper arm AAA\"\nprint(pb_results_df)\n\n\n        Variable Point_Biserial_Coefficient\n1    Barbell ALA                  0.2561495\n2  Lower leg ALA                 -0.6699079\n3  Upper leg ALA                 -0.8584492\n4      Torso ALA                 -0.7068735\n5  Upper arm ALA                 -0.4662353\n6    Barbell AAA                  0.7709157\n7  Lower leg AAA                 -0.5525890\n8  Upper leg AAA                 -0.8183856\n9      Torso AAA                 -0.3966161\n10 Upper arm AAA                  0.1021446",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of lifts mean values</span>"
    ]
  },
  {
    "objectID": "participants.html#analysis-of-participants-characteristics",
    "href": "participants.html#analysis-of-participants-characteristics",
    "title": "Participants",
    "section": "",
    "text": "Start by calculating means and std. I took some steps to format it to [mean ± std] format\n\n\nCode\n# Load participant data\ndata &lt;- read.csv(\"../../Logs/participants.csv\", header = TRUE, sep = \";\")\n\n# Calculate the number of participants overall and by sex\ncount_summary &lt;- data %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(count = n()) %&gt;%\n  bind_rows(summarise(data, Sex = \"Overall\", count = n()))\n\n# Calculate the summary statistics for all subjects\noverall_summary &lt;- data %&gt;%\n  summarise(across(c(Age, Height, Mass, Experience, Reported1rm), \n                   list(mean = ~ round(mean(.x, na.rm = TRUE),2), \n                        sd = ~ round(sd(.x, na.rm = TRUE)))),2)\n\n# Calculate the summary statistics by sex\nsex_summary &lt;- data %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(across(c(Age, Height, Mass, Experience, Reported1rm), \n                   list(mean = ~ round(mean(.x, na.rm = TRUE),2), \n                        sd = ~ round(sd(.x, na.rm = TRUE)))),2)\n\n# Combine the summaries into a single table\nsummary_table &lt;- bind_rows(\n  overall_summary %&gt;% mutate(Sex = \"Overall\"),\n  sex_summary\n)\n\n# Reshape the table for better visualization\nsummary_table &lt;- summary_table %&gt;%\n  pivot_longer(cols = -Sex, \n               names_to = c(\"variable\", \".value\"), \n               names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = Sex, values_from = c(mean, sd))\n\n# Combine mean and sd into a single column\nsummary_table &lt;- summary_table %&gt;%\n  mutate(across(ends_with(\"_Overall\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         Overall = paste0(mean_Overall, \" ± \", sd_Overall)) %&gt;%\n  mutate(across(ends_with(\"_male\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         male = paste0(mean_male, \" ± \", sd_male)) %&gt;%\n  mutate(across(ends_with(\"_female\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         female = paste0(mean_female, \" ± \", sd_female)) %&gt;%\n  select(variable, Overall, male, female)\n\n# Prepare the participant counts in the correct format\nparticipant_counts &lt;- count_summary %&gt;%\n  pivot_wider(names_from = Sex, values_from = count) %&gt;%\n  mutate(variable = \"count\",\n         Overall = as.character(Overall),\n         male = as.character(male),\n         female = as.character(female))\n\n# Combine the participant counts with the summary statistics\nfinal_summary_table &lt;- bind_rows(participant_counts, summary_table)\n\n# Correct the NA values for combined mean ± sd columns\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  mutate(Overall = ifelse(is.na(Overall), \"\", Overall),\n         male = ifelse(is.na(male), \"\", male),\n         female = ifelse(is.na(female), \"\", female))\n\n# Change the order of the columns, edit some names\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  select(variable, Overall, male, female)\nfinal_summary_table$variable[1] &lt;- \"Participants\"\nfinal_summary_table$variable[2] &lt;- \"Age (years)\"\nfinal_summary_table$variable[3] &lt;- \"Height (cm)\"\nfinal_summary_table$variable[4] &lt;- \"Body mass (kg)\"\nfinal_summary_table$variable[5] &lt;- \"Experience (months)\"\nfinal_summary_table$variable[6] &lt;- \"Reported 1RM (kg)\"\n\nlibrary(gtExtras)\n# Print the table using knitr::kable\nknitr::kable(final_summary_table[1:6,], caption = \"Table X: summary of participant characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: summary of participant characteristics\n\n\nvariable\nOverall\nmale\nfemale\n\n\n\n\nParticipants\n22\n11\n11\n\n\nAge (years)\n30.5 ± 7\n31.45 ± 7\n29.55 ± 7\n\n\nHeight (cm)\n177.82 ± 4\n180.27 ± 4\n175.36 ± 4\n\n\nBody mass (kg)\n81.21 ± 11\n87.82 ± 6\n74.61 ± 11\n\n\nExperience (months)\n4.82 ± 3\n5.27 ± 3\n4.36 ± 3\n\n\nReported 1RM (kg)\n78.27 ± 22\n94.09 ± 18\n62.45 ± 11\n\n\n\n\n\n\n\n\n\nCode\nlibrary(car)\n\n# Get the numerical columns from the dataframe\nnum_cols &lt;- sapply(data, is.numeric)\n\n# Exclude the column 'Sex' as it is categorical\nnum_cols &lt;- num_cols & names(data) != \"Sex\"\n\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n\n# Perform Levene's test for each numerical column\nfor (col in names(data)[num_cols]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$Sex)\n  levene_results[[col]] &lt;- test_result\n}\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nCode\n# View the results\nlevene_results\n\n\n$Age\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1   0.079 0.7816\n      20               \n\n$Height\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0606  0.808\n      20               \n\n$Mass\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  1  4.0563 0.05766 .\n      20                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Experience\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0172  0.897\n      20               \n\n$Reported1rm\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.9076 0.1825\n      20               \n\n$Performed1rm\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.9722 0.1756\n      20               \n\n\nNow the statistics for the particiapnt’s characteristics\nLets confirm normality\n\n\nCode\nlibrary(car)\n\ndata_male &lt;- data %&gt;%\n  filter(Sex == \"male\")\n\ndata_female &lt;- data %&gt;%\n  filter(Sex == \"female\")\n\nshapiro_test_male &lt;- list()\nshapiro_test_female &lt;- list()\n\n\nfor (i in colnames(data)[3:8]) { # Adjust the column indices based on actual data structure\n  shapiro_test_male[[i]] &lt;- shapiro.test(data_male[[i]])\n  shapiro_test_female[[i]] &lt;- shapiro.test(data_female[[i]])\n}\n\nshapiro_test_male\n\n\n$Age\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.95224, p-value = 0.6726\n\n\n$Height\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.90849, p-value = 0.234\n\n\n$Mass\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.9239, p-value = 0.3525\n\n\n$Experience\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.92886, p-value = 0.3995\n\n\n$Reported1rm\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.87801, p-value = 0.09814\n\n\n$Performed1rm\n\n    Shapiro-Wilk normality test\n\ndata:  data_male[[i]]\nW = 0.88721, p-value = 0.1283\n\n\nCode\nshapiro_test_female\n\n\n$Age\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.84165, p-value = 0.0332\n\n\n$Height\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.93811, p-value = 0.4986\n\n\n$Mass\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.9007, p-value = 0.1885\n\n\n$Experience\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.87052, p-value = 0.07873\n\n\n$Reported1rm\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.89542, p-value = 0.1624\n\n\n$Performed1rm\n\n    Shapiro-Wilk normality test\n\ndata:  data_female[[i]]\nW = 0.89601, p-value = 0.1651\n\n\nLets run an MANOVA and calculate effect sizes\n\n\nCode\n# Load necessary packages\nlibrary(car)\nlibrary(MVN)\nlibrary(effectsize)  # For effect size calculation\n\n\nWarning: package 'effectsize' was built under R version 4.3.3\n\n\nCode\n# Assuming 'data' is your dataframe and 'Sex' column indicates 'male' or 'female'\n# Separate numerical columns and the 'Sex' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\ngroup_data &lt;- data$Sex\n\n# Check for normality for each numerical column and each group (male, female)\nnormality_results &lt;- lapply(names(numerical_data), function(col) {\n  male_normality &lt;- shapiro.test(numerical_data[group_data == \"male\", col])$p.value\n  female_normality &lt;- shapiro.test(numerical_data[group_data == \"female\", col])$p.value\n  return(c(male_normality, female_normality))\n})\n\n\n\n# Combine normality results into a dataframe\nnormality_results_df &lt;- do.call(rbind, normality_results)\ncolnames(normality_results_df) &lt;- c(\"Male_p_value\", \"Female_p_value\")\nrownames(normality_results_df) &lt;- names(numerical_data)\n\nprint(normality_results_df)\n\n\n             Male_p_value Female_p_value\nAge            0.67264372     0.03319722\nHeight         0.23403241     0.49855953\nMass           0.35248794     0.18850993\nExperience     0.39946177     0.07873431\nReported1rm    0.09814232     0.16238117\nPerformed1rm   0.12833269     0.16514631\n\n\nCode\n# Check if all p-values &gt; 0.05 for normality assumption\nall_normal &lt;- all(normality_results_df &gt; 0.05)\n\nif (all_normal) {\n  # Perform MANOVA\n  formula &lt;- as.formula(paste(\"cbind(\", paste(names(numerical_data), collapse = \", \"), \") ~ Sex\"))\n  manova_result &lt;- manova(formula, data = data)\n  manova_summary &lt;- summary(manova_result, test = \"Pillai\")\n  print(manova_summary)\n  \n  # Check if the MANOVA is significant\n  if (manova_summary$stats[1, \"Pr(&gt;F)\"] &lt; 0.05) {\n    print(\"MANOVA is significant. Performing univariate ANOVAs:\")\n    # Perform univariate ANOVAs for each dependent variable\n    univariate_results &lt;- summary.aov(manova_result)\n    print(univariate_results)\n    \n    # Extracting p-values and means for interpretation\n    for (var in names(numerical_data)) {\n      print(paste(\"Analyzing variable:\", var))\n      aov_result &lt;- aov(as.formula(paste(var, \"~ Sex\")), data = data)\n      print(summary(aov_result))\n      print(model.tables(aov_result, \"means\"), digits = 3)\n      \n      # Calculate effect size (eta-squared)\n      eta_squared_result &lt;- eta_squared(aov_result)\n      print(eta_squared_result)\n    }\n  } else {\n    print(\"MANOVA is not significant. No further tests are performed.\")\n  }\n} else {\n  # If normality is not met, use Kruskal-Wallis test for each numerical column\n  print(\"Normality assumption not met. Performing Kruskal-Wallis tests:\")\n  kruskal_results &lt;- lapply(names(numerical_data), function(col) {\n    kruskal.test(numerical_data[, col] ~ group_data)$p.value\n  })\n  kruskal_results_df &lt;- data.frame(\n    Variable = names(numerical_data),\n    Kruskal_p_value = unlist(kruskal_results)\n  )\n  print(kruskal_results_df)\n}\n\n\n[1] \"Normality assumption not met. Performing Kruskal-Wallis tests:\"\n      Variable Kruskal_p_value\n1          Age    0.4685902173\n2       Height    0.0059154501\n3         Mass    0.0113054170\n4   Experience    0.3334035750\n5  Reported1rm    0.0002461724\n6 Performed1rm    0.0001541449\n\n\nUsing t-tests\n\n\nCode\n# # Load necessary packages\n# library(car)\n# library(MVN)\n# \n# # Assuming 'data' is your dataframe and 'Sex' column indicates 'male' or 'female'\n# # Separate numerical columns and the 'Sex' column\n# numerical_columns &lt;- sapply(data, is.numeric)\n# numerical_data &lt;- data[, numerical_columns]\n# group_data &lt;- data$Sex\n# \n# # Check for normality for each numerical column and each group (male, female)\n# normality_results &lt;- lapply(names(numerical_data), function(col) {\n#   male_normality &lt;- shapiro.test(numerical_data[group_data == \"male\", col])$p.value\n#   female_normality &lt;- shapiro.test(numerical_data[group_data == \"female\", col])$p.value\n#   return(c(male_normality, female_normality))\n# })\n# \n# # Combine normality results into a dataframe\n# normality_results_df &lt;- do.call(rbind, normality_results)\n# colnames(normality_results_df) &lt;- c(\"Male_p_value\", \"Female_p_value\")\n# rownames(normality_results_df) &lt;- names(numerical_data)\n# \n# print(normality_results_df)\n# \n# # Check if all p-values &gt; 0.05 for normality assumption\n# all_normal &lt;- all(normality_results_df &gt; 0.05)\n# \n# if (all_normal) {\n#   # Perform t-tests for each numerical column\n#   t_test_results &lt;- lapply(names(numerical_data), function(var) {\n#     t_test_result &lt;- t.test(numerical_data[data$Sex == \"male\", var], \n#                              numerical_data[data$Sex == \"female\", var])\n#     return(t_test_result)\n#   })\n# \n#   # Extract p-values from t-test results\n#   p_values &lt;- sapply(t_test_results, function(result) result$p.value)\n# \n#   # Correct for multiple comparisons if necessary\n#   # Bonferroni correction for example\n#   alpha &lt;- 0.05\n#   num_tests &lt;- length(p_values)\n#   corrected_alpha &lt;- alpha / num_tests\n#   significant_tests &lt;- p_values &lt; corrected_alpha\n# \n#   # Print significant tests\n#   print(\"Significant t-tests:\")\n#   print(names(numerical_data)[significant_tests])\n# \n#   # Function to calculate Cohen's d\n#   calculate_cohens_d &lt;- function(t_test_result) {\n#     d &lt;- (mean(t_test_result$estimate[1]) - mean(t_test_result$estimate[2])) / \n#       sqrt(((t_test_result$estimate[1] + t_test_result$estimate[2]) / 2) * \n#              (1 - ((t_test_result$parameter + 1) / t_test_result$parameter)) *\n#              (1 / t_test_result$parameter))\n#     return(d)\n#   }\n# \n#   # Calculate Cohen's d for significant tests\n#   cohens_d_results &lt;- lapply(t_test_results[significant_tests], calculate_cohens_d)\n#   names(cohens_d_results) &lt;- names(t_test_results)[significant_tests]\n# \n#   # Calculate Hedges' g for significant tests\n#   # Assuming unequal sample sizes\n#   n1 &lt;- sum(data$Sex == \"male\")\n#   n2 &lt;- sum(data$Sex == \"female\")\n#   hedges_g_results &lt;- lapply(cohens_d_results, function(cohen_d) {\n#     g &lt;- cohen_d * (1 - (3 / (4 * (n1 + n2) - 9)))\n#     return(g)\n#   })\n# \n#   print(\"Hedges' g for significant tests:\")\n#   print(hedges_g_results)\n# \n# } else {\n#   print(\"Normality assumption not met. Further tests cannot be performed.\")\n# }",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#some-simple-descriptives",
    "href": "participants.html#some-simple-descriptives",
    "title": "Participants",
    "section": "Some simple descriptives",
    "text": "Some simple descriptives\nStart by calculating means and std. I took some steps to format it to [mean ± std] format\n\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Calculate the number of participants overall and by sex\ncount_summary &lt;- data %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(count = n()) %&gt;%\n  bind_rows(summarise(data, Sex = \"Overall\", count = n()))\n\n# Calculate the summary statistics for all subjects\noverall_summary &lt;- data %&gt;%\n  summarise(across(c(Age, Height, Mass, Experience, Reported1rm), \n                   list(mean = ~ round(mean(.x, na.rm = TRUE), 2), \n                        sd = ~ round(sd(.x, na.rm = TRUE), 2))))\n\n# Calculate the summary statistics by sex\nsex_summary &lt;- data %&gt;%\n  group_by(Sex) %&gt;%\n  summarise(across(c(Age, Height, Mass, Experience, Reported1rm), \n                   list(mean = ~ round(mean(.x, na.rm = TRUE), 2), \n                        sd = ~ round(sd(.x, na.rm = TRUE), 2))))\n\n# Combine the summaries into a single table\nsummary_table &lt;- bind_rows(\n  overall_summary %&gt;% mutate(Sex = \"Overall\"),\n  sex_summary\n)\n\n# Reshape the table for better visualization\nsummary_table &lt;- summary_table %&gt;%\n  pivot_longer(cols = -Sex, \n               names_to = c(\"variable\", \".value\"), \n               names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = Sex, values_from = c(mean, sd))\n\n# Combine mean and sd into a single column\nsummary_table &lt;- summary_table %&gt;%\n  mutate(across(ends_with(\"_Overall\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         Overall = paste0(mean_Overall, \" ± \", sd_Overall)) %&gt;%\n  mutate(across(ends_with(\"_male\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         male = paste0(mean_male, \" ± \", sd_male)) %&gt;%\n  mutate(across(ends_with(\"_female\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         female = paste0(mean_female, \" ± \", sd_female)) %&gt;%\n  select(variable, Overall, male, female)\n\n# Prepare the participant counts in the correct format\nparticipant_counts &lt;- count_summary %&gt;%\n  pivot_wider(names_from = Sex, values_from = count) %&gt;%\n  mutate(variable = \"Participants\",\n         Overall = as.character(Overall),\n         male = as.character(male),\n         female = as.character(female))\n\n# Combine the participant counts with the summary statistics\nfinal_summary_table &lt;- bind_rows(participant_counts, summary_table)\n\n# Correct the NA values for combined mean ± sd columns\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  mutate(Overall = ifelse(is.na(Overall), \"\", Overall),\n         male = ifelse(is.na(male), \"\", male),\n         female = ifelse(is.na(female), \"\", female))\n\n# Change the order of the columns, edit some names\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  select(variable, Overall, male, female)\nfinal_summary_table$variable[1] &lt;- \"Participants\"\nfinal_summary_table$variable[2] &lt;- \"Age (years)\"\nfinal_summary_table$variable[3] &lt;- \"Height (cm)\"\nfinal_summary_table$variable[4] &lt;- \"Body mass (kg)\"\nfinal_summary_table$variable[5] &lt;- \"Experience (years)\"\nfinal_summary_table$variable[6] &lt;- \"Reported 1RM (kg)\"\n\n# Print the table using knitr::kable\nknitr::kable(final_summary_table, caption = \"Table X: Summary of Participant Characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Summary of Participant Characteristics\n\n\nvariable\nOverall\nmale\nfemale\n\n\n\n\nParticipants\n22\n11\n11\n\n\nAge (years)\n30.14 ± 6.55\n31.45 ± 6.53\n28.82 ± 6.6\n\n\nHeight (cm)\n177.82 ± 4.44\n180.27 ± 3.69\n175.36 ± 3.8\n\n\nBody mass (kg)\n80.8 ± 10.48\n87.82 ± 5.81\n73.79 ± 9.41\n\n\nExperience (years)\n4.82 ± 2.58\n5.27 ± 2.61\n4.36 ± 2.58\n\n\nReported 1RM (kg)\n78.27 ± 21.72\n94.09 ± 18.14\n62.45 ± 10.55\n\n\n\n\n\n\n\nAs expected some differences between genders seem to exists. Differences on height, body mass and reported 1rm are expected. However, age and experience should be similar to (try to) eliminate confounding.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#levenes-test",
    "href": "participants.html#levenes-test",
    "title": "Participants",
    "section": "Levene’s test",
    "text": "Levene’s test\nLets run levene’s test to determine equality of variance\n\n\nCode\nlibrary(car)\n\n# Get the numerical columns from the dataframe\nnum_cols &lt;- sapply(data, is.numeric)\n\n# Exclude the column 'Sex' as it is categorical\nnum_cols &lt;- num_cols & names(data) != \"Sex\"\n\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n\n# Perform Levene's test for each numerical column\nfor (col in names(data)[num_cols]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$Sex)\n  levene_results[[col]] &lt;- test_result\n}\n\n# Extract the necessary information from the test results\nresults_table &lt;- data.frame(\n  Variable = character(),\n  F_Value = numeric(),\n  Pr_Greater_F = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (col in names(levene_results)) {\n  test_result &lt;- levene_results[[col]]\n  results_table &lt;- rbind(results_table, data.frame(\n    Variable = col,\n    F_Value = round(test_result$`F value`[1],2),\n    Pr_Greater_F = round(test_result$`Pr(&gt;F)`[1],2)\n  ))\n}\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: Levene's test results on participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Levene's test results on participants' characteristics\n\n\nVariable\nF_Value\nPr_Greater_F\n\n\n\n\nAge\n0.42\n0.52\n\n\nHeight\n0.06\n0.81\n\n\nMass\n3.53\n0.08\n\n\nExperience\n0.02\n0.90\n\n\nReported1rm\n1.91\n0.18\n\n\nPerformed1rm\n1.97\n0.18\n\n\n\n\n\n\n\nNone of the P-values (PR_Greater_F) are significant. Mass seems to have the greatest relative difference in variances at p=0.08",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#shapiro-wilk-test",
    "href": "participants.html#shapiro-wilk-test",
    "title": "Participants",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\nTest the normality\n\n\nCode\nlibrary(car)\n\n# Filter the data by sex\ndata_male &lt;- data %&gt;% filter(Sex == \"male\")\ndata_female &lt;- data %&gt;% filter(Sex == \"female\")\n\n# Initialize lists to store the test results\nshapiro_test_male &lt;- list()\nshapiro_test_female &lt;- list()\n\n# Perform Shapiro-Wilk test for each specified column\nfor (i in colnames(data)[3:8]) { # Adjust the column indices based on actual data structure\n  shapiro_test_male[[i]] &lt;- shapiro.test(data_male[[i]])\n  shapiro_test_female[[i]] &lt;- shapiro.test(data_female[[i]])\n}\n\n# Extract the necessary information from the test results\nresults_male &lt;- data.frame(\n  Variable = character(),\n  Statistic_Male = numeric(),\n  P_Value_Male = numeric(),\n  stringsAsFactors = FALSE\n)\n\nresults_female &lt;- data.frame(\n  Variable = character(),\n  Statistic_Female = numeric(),\n  P_Value_Female = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (col in names(shapiro_test_male)) {\n  test_result_male &lt;- shapiro_test_male[[col]]\n  results_male &lt;- rbind(results_male, data.frame(\n    Variable = col,\n    Statistic_Male = round(test_result_male$statistic,2),\n    P_Value_Male = round(test_result_male$p.value,2)\n  ))\n}\n\nfor (col in names(shapiro_test_female)) {\n  test_result_female &lt;- shapiro_test_female[[col]]\n  results_female &lt;- rbind(results_female, data.frame(\n    Variable = col,\n    Statistic_Female = round(test_result_female$statistic,2),\n    P_Value_Female = round(test_result_female$p.value,2)\n  ))\n}\n\n# Combine the results into one table\nresults_table &lt;- merge(results_male, results_female, by = \"Variable\")\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: Shapiro-Wilk's test results on participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Shapiro-Wilk's test results on participants' characteristics\n\n\nVariable\nStatistic_Male\nP_Value_Male\nStatistic_Female\nP_Value_Female\n\n\n\n\nAge\n0.95\n0.67\n0.77\n0.00\n\n\nExperience\n0.93\n0.40\n0.87\n0.08\n\n\nHeight\n0.91\n0.23\n0.94\n0.50\n\n\nMass\n0.92\n0.35\n0.92\n0.30\n\n\nPerformed1rm\n0.89\n0.13\n0.90\n0.17\n\n\nReported1rm\n0.88\n0.10\n0.90\n0.16\n\n\n\n\n\n\n\nFemale age doesn’t seem to be normally distributed. (P_Value_Female = 0.001568)\nLets plot a histogram for female age to further inspect the data\n\n\nCode\n# Create the histogram with a custom title and x-axis label\nhist(data$Age[data$Sex == \"female\"],\n     main = \"Histogram of Female Age\",\n     xlab = \"Age (years)\")\n\n\n\n\n\n\n\n\n\nApperently there is a overrepresentation of younger female participants.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#differences-in-means-between-sexes",
    "href": "participants.html#differences-in-means-between-sexes",
    "title": "Participants",
    "section": "Differences in means between sexes",
    "text": "Differences in means between sexes\nLets run a simple t-test to determine different means between the sex groups.\n\n\nCode\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(broom)\nlibrary(tidyr)\n\n# Get the numeric columns from the dataframe, excluding 'Sex'\nnum_cols &lt;- sapply(data, is.numeric)\nnum_cols &lt;- num_cols & names(data) != \"Sex\"\n\n# Initialize a list to store the t-test results\nt_test_results &lt;- list()\n\n# Perform t-test for each numeric column\nfor (col in names(data)[num_cols]) {\n  t_test_result &lt;- t.test(data[[col]] ~ data$Sex)\n  t_test_results[[col]] &lt;- t_test_result\n}\n\n# Extract the necessary information from the t-test results\nresults_table &lt;- data.frame(\n  Variable = character(),\n  Mean_Male = numeric(),\n  Mean_Female = numeric(),\n  t_Value = numeric(),\n  df = numeric(),\n  P_Value = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (col in names(t_test_results)) {\n  test_result &lt;- t_test_results[[col]]\n  male_mean &lt;- mean(data[[col]][data$Sex == \"male\"], na.rm = TRUE)\n  female_mean &lt;- mean(data[[col]][data$Sex == \"female\"], na.rm = TRUE)\n  \n  results_table &lt;- rbind(results_table, data.frame(\n    Variable = col,\n    Mean_Male = round(male_mean,2),\n    Mean_Female = round(female_mean,2),\n    t_Value = round(test_result$statistic,2),\n    # df = round(test_result$parameter,2),\n    P_Value = round(test_result$p.value,2)\n  ))\n}\nrownames(results_table) &lt;- NULL\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: t-test results on participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: t-test results on participants' characteristics\n\n\nVariable\nMean_Male\nMean_Female\nt_Value\nP_Value\n\n\n\n\nAge\n31.45\n28.82\n-0.94\n0.36\n\n\nHeight\n180.27\n175.36\n-3.07\n0.01\n\n\nMass\n87.82\n73.79\n-4.21\n0.00\n\n\nExperience\n5.27\n4.36\n-0.82\n0.42\n\n\nReported1rm\n94.09\n62.45\n-5.00\n0.00\n\n\nPerformed1rm\n93.55\n62.09\n-5.25\n0.00\n\n\n\n\n\n\n\nAs expected, there are differences in means between sexes in height, bodymass, reported 1RM and performed 1RM. Luckily, no significant differences in mean Age or Experience seem to exist. However, the distribution of age seems to be skewed to younger participants for females.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#skewness-and-kurtosis",
    "href": "participants.html#skewness-and-kurtosis",
    "title": "Participants",
    "section": "Skewness and kurtosis",
    "text": "Skewness and kurtosis\nLets continue by determining skewness and curtosis for the variables:\n\n\nCode\nlibrary(moments)\n\n# Get the numeric columns from the dataframe, excluding 'Sex'\nnum_cols &lt;- sapply(data, is.numeric)\nnum_cols &lt;- num_cols & names(data) != \"Sex\"\n\n# Initialize a data frame to store the results\nresults_table &lt;- data.frame(\n  Variable = character(),\n  Sex = character(),\n  Skewness = numeric(),\n  Kurtosis = numeric(),\n  Skewness_Z = numeric(),\n  Kurtosis_Z = numeric(),\n  stringsAsFactors = FALSE\n)\n\n# Calculate skewness and kurtosis for each numeric column and each sex\nfor (col in names(data)[num_cols]) {\n  for (sex in c(\"male\", \"female\")) {\n    subset_data &lt;- data[[col]][data$Sex == sex]\n    \n    skewness_value &lt;- round(skewness(subset_data, na.rm = TRUE),2)\n    kurtosis_value &lt;- round(kurtosis(subset_data, na.rm = TRUE),2)\n    \n    n &lt;- length(subset_data)\n    skewness_se &lt;- sqrt(6/n)\n    kurtosis_se &lt;- sqrt(24/n)\n    \n    skewness_z &lt;- round(skewness_value / skewness_se,2)\n    kurtosis_z &lt;- round((kurtosis_value - 3) / kurtosis_se,2)  # Adjust kurtosis by subtracting 3 for excess kurtosis\n\n    results_table &lt;- rbind(results_table, data.frame(\n      Variable = col,\n      Sex = sex,\n      Skewness = skewness_value,\n      Kurtosis = kurtosis_value,\n      Skewness_Z = skewness_z,\n      Kurtosis_Z = kurtosis_z\n    ))\n  }\n}\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: skewness and kurtosis of participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: skewness and kurtosis of participants' characteristics\n\n\nVariable\nSex\nSkewness\nKurtosis\nSkewness_Z\nKurtosis_Z\n\n\n\n\nAge\nmale\n0.25\n1.96\n0.34\n-0.70\n\n\nAge\nfemale\n1.72\n5.23\n2.33\n1.51\n\n\nHeight\nmale\n0.45\n2.82\n0.61\n-0.12\n\n\nHeight\nfemale\n0.27\n2.78\n0.37\n-0.15\n\n\nMass\nmale\n0.47\n3.21\n0.64\n0.14\n\n\nMass\nfemale\n0.27\n1.71\n0.37\n-0.87\n\n\nExperience\nmale\n0.68\n3.20\n0.92\n0.14\n\n\nExperience\nfemale\n0.90\n2.96\n1.22\n-0.03\n\n\nReported1rm\nmale\n0.81\n2.40\n1.10\n-0.41\n\n\nReported1rm\nfemale\n0.85\n3.00\n1.15\n0.00\n\n\nPerformed1rm\nmale\n0.89\n2.66\n1.21\n-0.23\n\n\nPerformed1rm\nfemale\n1.03\n3.53\n1.39\n0.36\n\n\n\n\n\n\n\nOnly female age skewness is outside the [-2:2] interval. This seems to be in line with previous findings for skewness. All other values are within the accepted interval.",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "participants.html#histogram",
    "href": "participants.html#histogram",
    "title": "Participants",
    "section": "Histogram",
    "text": "Histogram\nOne more check for normality: lets plot histograms of all variables, grouped by sex.\n\n\nCode\nlibrary(ggplot2)\n\n# Identify numeric columns excluding 'Sex'\nnum_cols &lt;- names(data)[sapply(data, is.numeric) & names(data) != \"Sex\"]\n\n# Plot histograms for each numeric column, grouped by sex\nfor (col in num_cols) {\n  # Convert to numeric in case some columns are not numeric\n  data[[col]] &lt;- as.numeric(data[[col]])\n  \n  plot &lt;- ggplot(data, aes_string(x = col, fill = \"Sex\")) +\n    geom_histogram(alpha = 0.6, position = \"identity\") +\n    labs(title = paste(\"Histogram of\", col, \"Grouped by Sex\"),\n         x = col,\n         y = \"Frequency\") +\n    theme_minimal() +\n    facet_wrap(~Sex, scales = \"free_y\") +\n    theme(legend.position = \"top\") +\n    guides(fill = guide_legend(title = \"Sex\"))\n  \n  print(plot)\n}",
    "crumbs": [
      "Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Participants</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html#descriptive-statistics",
    "href": "analysis_of_lifst.html#descriptive-statistics",
    "title": "Analysis of lifts",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nLets check assumptions by - Reporting descriptives [mean ± std] - Levene’s test for equality of variances - Shapiro-Wilk’s test for normality - Z-value for skewness - Z-value for kurtosis - Histograms - Q-Q plots\n\nSome descriptives\nStart by calculating means and std. I took some steps to format it to [mean ± std] format\n\n\nCode\n# Calculate the number of participants overall and by sex\ncount_summary &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  summarise(count = n()) %&gt;%\n  bind_rows(summarise(data, group = \"Overall\", count = n()))\n\n# Calculate the summary statistics for all subjects\ncolumn_names &lt;- setdiff(names(data), \"group\")\noverall_summary &lt;- data %&gt;%\n  summarise(across(column_names, \n                   list(mean = ~ round(mean(.x, na.rm = TRUE), 2), \n                        sd = ~ sprintf(\"%.2f\", sd(.x, na.rm = TRUE)))), 2)\n\n# Calculate the summary statistics by sex\nsex_summary &lt;- data %&gt;%\n  group_by(group) %&gt;%\n  summarise(across(column_names, \n                   list(mean = ~ round(mean(.x, na.rm = TRUE), 2), \n                        sd = ~ sprintf(\"%.2f\", sd(.x, na.rm = TRUE)))), 2)\n\n# Combine the summaries into a single table\nsummary_table &lt;- bind_rows(\n  overall_summary %&gt;% mutate(group = \"Overall\"),\n  sex_summary\n)\n\n# Reshape the table for better visualization\nsummary_table &lt;- summary_table %&gt;%\n  pivot_longer(cols = -group, \n               names_to = c(\"variable\", \".value\"), \n               names_sep = \"_\") %&gt;%\n  pivot_wider(names_from = group, values_from = c(mean, sd))\n\n# Combine mean and sd into a single column\nsummary_table &lt;- summary_table %&gt;%\n  mutate(across(ends_with(\"_Overall\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         Overall = paste0(mean_Overall, \" ± \", sd_Overall)) %&gt;%\n  mutate(across(ends_with(\"_male\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         male = paste0(mean_male, \" ± \", sd_male)) %&gt;%\n  mutate(across(ends_with(\"_female\"), ~ ifelse(is.na(.), \"\", as.character(.))),\n         female = paste0(mean_female, \" ± \", sd_female)) %&gt;%\n  select(variable, Overall, male, female)\n\n# Prepare the participant counts in the correct format\nparticipant_counts &lt;- count_summary %&gt;%\n  pivot_wider(names_from = group, values_from = count) %&gt;%\n  mutate(variable = \"count\",\n         Overall = as.character(Overall),\n         male = as.character(male),\n         female = as.character(female))\n\n# Combine the participant counts with the summary statistics\nfinal_summary_table &lt;- bind_rows(participant_counts, summary_table)\n\n# Correct the NA values for combined mean ± sd columns\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  mutate(Overall = ifelse(is.na(Overall), \"\", Overall),\n         male = ifelse(is.na(male), \"\", male),\n         female = ifelse(is.na(female), \"\", female))\n\n# Change the order of the columns, edit some names\nfinal_summary_table &lt;- final_summary_table %&gt;%\n  select(variable, Overall, male, female)\n\nlibrary(gtExtras)\n# Print the table using knitr::kable\nknitr::kable(final_summary_table[1:21,], caption = \"Table X: summary of PLA PAA ALA and AAA\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: summary of PLA PAA ALA and AAA\n\n\nvariable\nOverall\nmale\nfemale\n\n\n\n\ncount\n22\n11\n11\n\n\nPLA1\n19.35 ± 1.66\n18.76 ± 1.93\n19.95 ± 1.13\n\n\nPLA2\n58.67 ± 37.82\n77.56 ± 45.20\n39.78 ± 13.24\n\n\nPLA3\n34.46 ± 21.43\n45.36 ± 26.22\n23.56 ± 3.92\n\n\nPLA4\n26.78 ± 9.47\n31.15 ± 11.49\n22.41 ± 3.76\n\n\nPLA5\n23.32 ± 4.13\n24.69 ± 4.60\n21.96 ± 3.25\n\n\nPAA1\n231.94 ± 173.71\n155.97 ± 154.23\n307.91 ± 163.97\n\n\nPAA2\n383.05 ± 131.57\n413.92 ± 109.16\n352.19 ± 149.47\n\n\nPAA3\n297.16 ± 100.29\n334.23 ± 113.69\n260.09 ± 71.92\n\n\nPAA4\n152.82 ± 22.52\n155.86 ± 21.72\n149.77 ± 23.93\n\n\nPAA5\n432.3 ± 36.49\n427.12 ± 39.88\n437.49 ± 33.86\n\n\nALA1\n9.9 ± 0.17\n9.95 ± 0.18\n9.85 ± 0.16\n\n\nALA2\n12.74 ± 1.13\n13.36 ± 1.19\n12.12 ± 0.63\n\n\nALA3\n11.84 ± 1.17\n12.53 ± 1.34\n11.15 ± 0.19\n\n\nALA4\n10.86 ± 0.41\n11.05 ± 0.46\n10.67 ± 0.25\n\n\nALA5\n11.12 ± 0.34\n11.22 ± 0.38\n11.01 ± 0.27\n\n\nAAA1\n37.41 ± 19.68\n28.11 ± 21.38\n46.7 ± 12.88\n\n\nAAA2\n82.24 ± 15.10\n89.43 ± 13.09\n75.04 ± 13.91\n\n\nAAA3\n75.96 ± 21.19\n85.61 ± 23.30\n66.31 ± 13.98\n\n\nAAA4\n37.09 ± 6.33\n39.51 ± 5.69\n34.66 ± 6.23\n\n\nAAA5\n92.55 ± 12.31\n92.15 ± 11.98\n92.94 ± 13.21\n\n\n\n\n\n\n\n\n\nLevene’s test\nLets run levene’s test to determine equality of variance\n\n\nCode\nlibrary(car)\n\n# Get the numerical columns from the dataframe\nnum_cols &lt;- sapply(data, is.numeric)\n\n# Exclude the column 'Sex' as it is categorical\nnum_cols &lt;- num_cols & names(data) != \"group\"\n\n# Initialize a list to store the test results\nlevene_results &lt;- list()\n\n# Perform Levene's test for each numerical column\nfor (col in names(data)[num_cols]) {\n  test_result &lt;- leveneTest(data[[col]] ~ data$group)\n  levene_results[[col]] &lt;- test_result\n}\n\n# Extract the necessary information from the test results\nresults_table &lt;- data.frame(\n  Variable = character(),\n  F_Value = numeric(),\n  Pr_Greater_F = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (col in names(levene_results)) {\n  test_result &lt;- levene_results[[col]]\n  results_table &lt;- rbind(results_table, data.frame(\n    Variable = col,\n    F_Value = round(test_result$`F value`[1],2),\n    Pr_Greater_F = round(test_result$`Pr(&gt;F)`[1],2)\n  ))\n}\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: Levene's test results on participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Levene's test results on participants' characteristics\n\n\nVariable\nF_Value\nPr_Greater_F\n\n\n\n\nPLA1\n2.70\n0.12\n\n\nPLA2\n12.99\n0.00\n\n\nPLA3\n12.59\n0.00\n\n\nPLA4\n8.43\n0.01\n\n\nPLA5\n1.80\n0.20\n\n\nPAA1\n0.08\n0.78\n\n\nPAA2\n0.17\n0.68\n\n\nPAA3\n3.55\n0.07\n\n\nPAA4\n0.15\n0.70\n\n\nPAA5\n0.14\n0.71\n\n\nALA1\n0.37\n0.55\n\n\nALA2\n4.08\n0.06\n\n\nALA3\n16.41\n0.00\n\n\nALA4\n4.95\n0.04\n\n\nALA5\n0.31\n0.58\n\n\nAAA1\n3.35\n0.08\n\n\nAAA2\n0.10\n0.76\n\n\nAAA3\n3.86\n0.06\n\n\nAAA4\n0.08\n0.78\n\n\nAAA5\n0.05\n0.83\n\n\n\n\n\n\n\nThe P-values(PR_Greater_F) of PLA3, PLA4 are significant. All others are not.",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html#shapiro-wilk-test",
    "href": "analysis_of_lifst.html#shapiro-wilk-test",
    "title": "Analysis of lifts",
    "section": "Shapiro-Wilk test",
    "text": "Shapiro-Wilk test\nTest the normality\n\n\nCode\nlibrary(car)\n\n# Filter the data by sex\ndata_male &lt;- data %&gt;% filter(group == \"male\")\ndata_female &lt;- data %&gt;% filter(group == \"female\")\n\n# Initialize lists to store the test results\nshapiro_test_male &lt;- list()\nshapiro_test_female &lt;- list()\n\n# Perform Shapiro-Wilk test for each specified column\nfor (i in colnames(data)[1:20]) { # Adjust the column indices based on actual data structure\n  shapiro_test_male[[i]] &lt;- shapiro.test(data_male[[i]])\n  shapiro_test_female[[i]] &lt;- shapiro.test(data_female[[i]])\n}\n\n# Extract the necessary information from the test results\nresults_male &lt;- data.frame(\n  Variable = character(),\n  Statistic_Male = numeric(),\n  P_Value_Male = numeric(),\n  stringsAsFactors = FALSE\n)\n\nresults_female &lt;- data.frame(\n  Variable = character(),\n  Statistic_Female = numeric(),\n  P_Value_Female = numeric(),\n  stringsAsFactors = FALSE\n)\n\nfor (col in names(shapiro_test_male)) {\n  test_result_male &lt;- shapiro_test_male[[col]]\n  results_male &lt;- rbind(results_male, data.frame(\n    Variable = col,\n    Statistic_Male = round(test_result_male$statistic,2),\n    P_Value_Male = round(test_result_male$p.value,2)\n  ))\n}\n\nfor (col in names(shapiro_test_female)) {\n  test_result_female &lt;- shapiro_test_female[[col]]\n  results_female &lt;- rbind(results_female, data.frame(\n    Variable = col,\n    Statistic_Female = round(test_result_female$statistic,2),\n    P_Value_Female = round(test_result_female$p.value,2)\n  ))\n}\n\n# Combine the results into one table\nresults_table &lt;- merge(results_male, results_female, by = \"Variable\")\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: Shapiro-Wilk's test results on participants' characteristics\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Shapiro-Wilk's test results on participants' characteristics\n\n\nVariable\nStatistic_Male\nP_Value_Male\nStatistic_Female\nP_Value_Female\n\n\n\n\nAAA1\n0.82\n0.02\n0.84\n0.03\n\n\nAAA2\n0.91\n0.26\n0.93\n0.37\n\n\nAAA3\n0.93\n0.41\n0.90\n0.19\n\n\nAAA4\n0.95\n0.66\n0.92\n0.32\n\n\nAAA5\n0.90\n0.19\n0.94\n0.50\n\n\nALA1\n0.90\n0.18\n0.91\n0.25\n\n\nALA2\n0.91\n0.26\n0.97\n0.92\n\n\nALA3\n0.77\n0.00\n0.96\n0.71\n\n\nALA4\n0.86\n0.06\n0.89\n0.13\n\n\nALA5\n0.92\n0.33\n0.93\n0.44\n\n\nPAA1\n0.87\n0.07\n0.79\n0.01\n\n\nPAA2\n0.89\n0.13\n0.63\n0.00\n\n\nPAA3\n0.84\n0.04\n0.70\n0.00\n\n\nPAA4\n0.93\n0.36\n0.96\n0.81\n\n\nPAA5\n0.91\n0.27\n0.87\n0.08\n\n\nPLA1\n0.91\n0.24\n0.95\n0.60\n\n\nPLA2\n0.87\n0.08\n0.85\n0.04\n\n\nPLA3\n0.83\n0.03\n0.86\n0.06\n\n\nPLA4\n0.92\n0.28\n0.79\n0.01\n\n\nPLA5\n0.88\n0.09\n0.83\n0.03\n\n\n\n\n\n\n\nWith lots of significant P-values, this data does not seem to be very normaly distributed\n\nSkewness and kurtosis\nLets continue by determining skewness and kurtosis for the variables:\n\n\nCode\nlibrary(moments)\n\n# Get the numeric columns from the dataframe, excluding 'Sex'\nnum_cols &lt;- sapply(data, is.numeric)\nnum_cols &lt;- num_cols & names(data) != \"group\"\n\n# Initialize a data frame to store the results\nresults_table &lt;- data.frame(\n  Variable = character(),\n  Sex = character(),\n  Skewness = numeric(),\n  Kurtosis = numeric(),\n  Skewness_Z = numeric(),\n  Kurtosis_Z = numeric(),\n  stringsAsFactors = FALSE\n)\n\n# Calculate skewness and kurtosis for each numeric column and each sex\nfor (col in names(data)[num_cols]) {\n  for (sex in c(\"male\", \"female\")) {\n    subset_data &lt;- data[[col]][data$group == sex]\n    \n    skewness_value &lt;- round(skewness(subset_data, na.rm = TRUE),2)\n    kurtosis_value &lt;- round(kurtosis(subset_data, na.rm = TRUE),2)\n    \n    n &lt;- length(subset_data)\n    skewness_se &lt;- sqrt(6/n)\n    kurtosis_se &lt;- sqrt(24/n)\n    \n    skewness_z &lt;- round(skewness_value / skewness_se,2)\n    kurtosis_z &lt;- round((kurtosis_value - 3) / kurtosis_se,2)  # Adjust kurtosis by subtracting 3 for excess kurtosis\n\n    results_table &lt;- rbind(results_table, data.frame(\n      Variable = col,\n      Sex = sex,\n      Skewness = skewness_value,\n      Kurtosis = kurtosis_value,\n      Skewness_Z = skewness_z,\n      Kurtosis_Z = kurtosis_z\n    ))\n  }\n}\n\n# Print the results in a nice table\nknitr::kable(results_table, caption = \"Table X: skewness and kurtosis of PLA PAA ALA AAA\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: skewness and kurtosis of PLA PAA ALA AAA\n\n\nVariable\nSex\nSkewness\nKurtosis\nSkewness_Z\nKurtosis_Z\n\n\n\n\nPLA1\nmale\n-0.28\n1.64\n-0.38\n-0.92\n\n\nPLA1\nfemale\n-0.50\n2.27\n-0.68\n-0.49\n\n\nPLA2\nmale\n0.34\n1.46\n0.46\n-1.04\n\n\nPLA2\nfemale\n1.00\n2.72\n1.35\n-0.19\n\n\nPLA3\nmale\n0.38\n1.49\n0.51\n-1.02\n\n\nPLA3\nfemale\n1.02\n2.85\n1.38\n-0.10\n\n\nPLA4\nmale\n0.72\n2.62\n0.97\n-0.26\n\n\nPLA4\nfemale\n1.05\n2.50\n1.42\n-0.34\n\n\nPLA5\nmale\n0.41\n1.60\n0.56\n-0.95\n\n\nPLA5\nfemale\n1.38\n4.39\n1.87\n0.94\n\n\nPAA1\nmale\n0.70\n2.41\n0.95\n-0.40\n\n\nPAA1\nfemale\n1.17\n2.98\n1.58\n-0.01\n\n\nPAA2\nmale\n0.24\n1.63\n0.32\n-0.93\n\n\nPAA2\nfemale\n2.32\n7.35\n3.14\n2.94\n\n\nPAA3\nmale\n0.32\n1.88\n0.43\n-0.76\n\n\nPAA3\nfemale\n1.86\n5.43\n2.52\n1.65\n\n\nPAA4\nmale\n-0.85\n4.05\n-1.15\n0.71\n\n\nPAA4\nfemale\n-0.54\n3.01\n-0.73\n0.01\n\n\nPAA5\nmale\n0.88\n3.95\n1.19\n0.64\n\n\nPAA5\nfemale\n1.27\n4.09\n1.72\n0.74\n\n\nALA1\nmale\n0.71\n2.27\n0.96\n-0.49\n\n\nALA1\nfemale\n0.83\n3.00\n1.12\n0.00\n\n\nALA2\nmale\n0.03\n1.59\n0.04\n-0.95\n\n\nALA2\nfemale\n0.35\n2.51\n0.47\n-0.33\n\n\nALA3\nmale\n0.12\n1.12\n0.16\n-1.27\n\n\nALA3\nfemale\n0.60\n3.25\n0.81\n0.17\n\n\nALA4\nmale\n0.01\n1.41\n0.01\n-1.08\n\n\nALA4\nfemale\n1.13\n3.84\n1.53\n0.57\n\n\nALA5\nmale\n1.01\n4.03\n1.37\n0.70\n\n\nALA5\nfemale\n0.50\n2.07\n0.68\n-0.63\n\n\nAAA1\nmale\n0.00\n1.29\n0.00\n-1.16\n\n\nAAA1\nfemale\n1.18\n3.26\n1.60\n0.18\n\n\nAAA2\nmale\n-0.92\n3.61\n-1.25\n0.41\n\n\nAAA2\nfemale\n0.82\n2.92\n1.11\n-0.05\n\n\nAAA3\nmale\n-0.10\n1.64\n-0.14\n-0.92\n\n\nAAA3\nfemale\n0.59\n1.96\n0.80\n-0.70\n\n\nAAA4\nmale\n-0.15\n1.88\n-0.20\n-0.76\n\n\nAAA4\nfemale\n0.72\n3.59\n0.97\n0.40\n\n\nAAA5\nmale\n-0.29\n1.62\n-0.39\n-0.93\n\n\nAAA5\nfemale\n-0.50\n2.01\n-0.68\n-0.67\n\n\n\n\n\n\n\n*With 13 out of 40 Z-values outside of the [-2:2] interval, this does not suggest true normal distribution",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst.html#histogram",
    "href": "analysis_of_lifst.html#histogram",
    "title": "Analysis of lifts",
    "section": "Histogram",
    "text": "Histogram\nOne more check for normality: lets plot histograms of all variables, grouped by sex.\n\n\nCode\nlibrary(ggplot2)\n\n# Identify numeric columns excluding 'Sex'\nnum_cols &lt;- names(data)[sapply(data, is.numeric) & names(data) != \"group\"]\n\n# Plot histograms for each numeric column, grouped by sex\nfor (col in num_cols) {\n  # Convert to numeric in case some columns are not numeric\n  data[[col]] &lt;- as.numeric(data[[col]])\n  \n  plot &lt;- ggplot(data, aes_string(x = col, fill = \"group\")) +\n    geom_histogram(alpha = 0.6, position = \"identity\") +\n    labs(title = paste(\"Histogram of\", col, \"Grouped by Sex\"),\n         x = col,\n         y = \"Frequency\") +\n    theme_minimal() +\n    facet_wrap(~group, scales = \"free_y\") +\n    theme(legend.position = \"top\") +\n    guides(fill = guide_legend(title = \"Sex\"))\n  \n  print(plot)\n}",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysis of lifts</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst_mean_values.html#statistical-analyses",
    "href": "analysis_of_lifst_mean_values.html#statistical-analyses",
    "title": "Analysis of lifts mean values",
    "section": "Statistical analyses",
    "text": "Statistical analyses\nLets run an MANOVA\n\n\nCode\n# Assuming 'data' is your dataframe and 'group' column indicates 'male' or 'female'\n# Separate numerical columns and the 'group' column\n# data2 &lt;- data[1:24, c(1:10, 21)]  # Selecting desired rows and columns\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\n\n# Perform MANOVA\nmanova_result &lt;- manova(as.matrix(numerical_data) ~ group, data = data)\n\n# Summary of MANOVA\nmanova_summary &lt;- summary(manova_result)\n# print(manova_summary)\n\nmanova_table &lt;- data.frame(\n  Test = \"Pillai\",\n  Df = manova_summary$stats[1, \"Df\"],\n  Pillai = manova_summary$stats[1, \"Pillai\"],\n  Approx_F = manova_summary$stats[1, \"approx F\"],\n  Num_Df = manova_summary$stats[1, \"num Df\"],\n  Den_Df = manova_summary$stats[1, \"den Df\"],\n  Pr_F = manova_summary$stats[1, \"Pr(&gt;F)\"]\n)\n\n# Print MANOVA results\nkable(manova_table, caption = \"Table X: Results of MANOVA\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Results of MANOVA\n\n\nTest\nDf\nPillai\nApprox_F\nNum_Df\nDen_Df\nPr_F\n\n\n\n\nPillai\n1\n0.9090909\n0.5\n20\n1\n0.8273217\n\n\n\n\n\n\n\nLooking at Pr(&gt;F), the result of the MANOVA is insignificant (hardcoded text)\n\n\nCode\n# Initialize an empty list to store results\nkruskal_results &lt;- list()\n\n# Loop through each numerical column\nfor (col in colnames(numerical_data)) {\n  # Extract the numeric vector for the column\n  x &lt;- split(numerical_data[[col]], data$group)\n  \n  # Perform Kruskal-Wallis test\n  kruskal_results[[col]] &lt;- kruskal.test(x)\n}\n\n# Extracting p-values from each test\np_values &lt;- sapply(kruskal_results, function(x) x$p.value)\n\n# Print results\nresults_df &lt;- data.frame(\n  Variable = names(kruskal_results),\n  P_Value = p_values\n)\n\nprint(results_df)\n\n\n     Variable     P_Value\nPLA1     PLA1 0.148217773\nPLA2     PLA2 0.035460320\nPLA3     PLA3 0.048652893\nPLA4     PLA4 0.056663148\nPLA5     PLA5 0.114727257\nPAA1     PAA1 0.048652893\nPAA2     PAA2 0.130641745\nPAA3     PAA3 0.167545185\nPAA4     PAA4 0.469724315\nPAA5     PAA5 0.392900523\nALA1     ALA1 0.237085084\nALA2     ALA2 0.012561139\nALA3     ALA3 0.005803011\nALA4     ALA4 0.056803061\nALA5     ALA5 0.167785416\nAAA1     AAA1 0.188956893\nAAA2     AAA2 0.030192378\nAAA3     AAA3 0.035564328\nAAA4     AAA4 0.076153168\nAAA5     AAA5 0.895484901\n\n\n\n\nCode\ndata2 &lt;- data\n\n# Check if the MANOVA is significant\nif (manova_summary$stats[1, \"Pr(&gt;F)\"] &lt; 0.05) {\n  # cat(\"MANOVA is significant. Performing univariate ANOVAs:\\n\")\n  \n  # Perform univariate ANOVAs for each dependent variable\n  univariate_results &lt;- map(names(numerical_data), ~aov(as.formula(paste(.x, \"~ group\")), data = data2))\n  \n  # Tidy the univariate ANOVA results\n  tidy_results &lt;- map2(names(numerical_data), univariate_results, ~tidy(.y) %&gt;% mutate(variable = .x))\n  \n  # Combine the tidy results into a single table\n  univariate_table &lt;- bind_rows(tidy_results) %&gt;% \n    select(variable, everything())\n  \n  # Print the univariate ANOVA table\n  knitr::kable(univariate_table, caption = \"Table X: Results of univariate ANOVAs\") %&gt;%\n    kableExtra::kable_styling(bootstrap_options = \"striped\")\n  \n} else {\n  cat(\"MANOVA is not significant. No further tests are performed.\\n\")\n}\n\n\nMANOVA is not significant. No further tests are performed.",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of lifts mean values</span>"
    ]
  },
  {
    "objectID": "analysis_of_lifst_mean_values.html#t-test-between-the-groups",
    "href": "analysis_of_lifst_mean_values.html#t-test-between-the-groups",
    "title": "Analysis of lifts mean values",
    "section": "T-test between the groups",
    "text": "T-test between the groups\n\n\nCode\n# Assuming 'data' is your dataframe and 'group' column indicates 'male' or 'female'\n# Separate numerical columns and the 'group' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\n\n# Perform t-tests for each numerical column\nt_test_results &lt;- map(names(numerical_data), ~ t.test(as.formula(paste(.x, \"~ group\")), data = data) %&gt;% tidy())\n\n# Combine the t-test results into a single table\nt_test_table &lt;- bind_rows(t_test_results) %&gt;% \n  mutate(variable = names(numerical_data)) %&gt;% \n  select(variable, everything())\n\n# Print the t-test results table\nknitr::kable(t_test_table, caption = \"Table X: Results of t-tests between groups\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Results of t-tests between groups\n\n\nvariable\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\nPLA1\n1.1869957\n19.946359\n18.759363\n1.7633050\n0.0967708\n16.13798\n-0.2390599\n2.6130513\nWelch Two Sample t-test\ntwo.sided\n\n\nPLA2\n-37.7814636\n39.779194\n77.560658\n-2.6602240\n0.0211690\n11.70317\n-68.8130221\n-6.7499051\nWelch Two Sample t-test\ntwo.sided\n\n\nPLA3\n-21.8039246\n23.555177\n45.359102\n-2.7273067\n0.0205253\n10.44638\n-39.5145259\n-4.0933232\nWelch Two Sample t-test\ntwo.sided\n\n\nPLA4\n-8.7380025\n22.411573\n31.149576\n-2.3973038\n0.0334956\n12.12160\n-16.6707940\n-0.8052110\nWelch Two Sample t-test\ntwo.sided\n\n\nPLA5\n-2.7274499\n21.960781\n24.688231\n-1.6056578\n0.1257626\n17.98653\n-6.2963718\n0.8414719\nWelch Two Sample t-test\ntwo.sided\n\n\nPAA1\n151.9387625\n307.911111\n155.972349\n2.2385900\n0.0367594\n19.92548\n10.3251992\n293.5523257\nWelch Two Sample t-test\ntwo.sided\n\n\nPAA2\n-61.7309954\n352.185135\n413.916130\n-1.1061728\n0.2829783\n18.30473\n-178.8351376\n55.3731467\nWelch Two Sample t-test\ntwo.sided\n\n\nPAA3\n-74.1451671\n260.088832\n334.233999\n-1.8279474\n0.0852683\n16.89815\n-159.7627745\n11.4724403\nWelch Two Sample t-test\ntwo.sided\n\n\nPAA4\n-6.0961907\n149.766968\n155.863159\n-0.6256482\n0.5386858\n19.81514\n-26.4335632\n14.2411818\nWelch Two Sample t-test\ntwo.sided\n\n\nPAA5\n10.3723500\n437.488341\n427.115991\n0.6575701\n0.5185074\n19.48587\n-22.5868692\n43.3315693\nWelch Two Sample t-test\ntwo.sided\n\n\nALA1\n-0.0947853\n9.852553\n9.947338\n-1.3000172\n0.2086430\n19.64396\n-0.2470514\n0.0574809\nWelch Two Sample t-test\ntwo.sided\n\n\nALA2\n-1.2430046\n12.118500\n13.361505\n-3.0647910\n0.0077791\n15.17151\n-2.1066182\n-0.3793909\nWelch Two Sample t-test\ntwo.sided\n\n\nALA3\n-1.3799302\n11.154198\n12.534129\n-3.3889323\n0.0065323\n10.39861\n-2.2825080\n-0.4773525\nWelch Two Sample t-test\ntwo.sided\n\n\nALA4\n-0.3768603\n10.673868\n11.050728\n-2.3737407\n0.0309858\n15.42390\n-0.7144457\n-0.0392749\nWelch Two Sample t-test\ntwo.sided\n\n\nALA5\n-0.2135029\n11.009821\n11.223323\n-1.5364561\n0.1417516\n18.07013\n-0.5053617\n0.0783559\nWelch Two Sample t-test\ntwo.sided\n\n\nAAA1\n18.5912374\n46.703677\n28.112440\n2.4701591\n0.0248162\n16.41365\n2.6687436\n34.5137313\nWelch Two Sample t-test\ntwo.sided\n\n\nAAA2\n-14.3822558\n75.044250\n89.426506\n-2.4974579\n0.0213852\n19.92516\n-26.3977080\n-2.3668036\nWelch Two Sample t-test\ntwo.sided\n\n\nAAA3\n-19.2928204\n66.314962\n85.607783\n-2.3546358\n0.0313222\n16.37611\n-36.6300000\n-1.9556407\nWelch Two Sample t-test\ntwo.sided\n\n\nAAA4\n-4.8513969\n34.663516\n39.514913\n-1.9077457\n0.0710036\n19.84086\n-10.1587304\n0.4559366\nWelch Two Sample t-test\ntwo.sided\n\n\nAAA5\n0.7877408\n92.939303\n92.151563\n0.1465594\n0.8849608\n19.81188\n-10.4309107\n12.0063923\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nBonferroni correction\nLets repeat it with Bonferroni correction. The bonferroni correction is implemented in such a way that the bonferroni_p represents the adjusted P-values. A quick calculation with alpha = 0.05 and 20 comparisons led to a adjusted alpha of 0.00250\n\n\nCode\n# Assuming 'data' is your dataframe and 'group' column indicates 'male' or 'female'\n# Separate numerical columns and the 'group' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\n\n# Perform t-tests for each numerical column\nt_test_results &lt;- map(names(numerical_data), ~ t.test(as.formula(paste(.x, \"~ group\")), data = data) %&gt;% tidy())\n\n# Combine the t-test results into a single table\nt_test_table &lt;- bind_rows(t_test_results) %&gt;% \n  mutate(variable = names(numerical_data)) %&gt;% \n  select(variable, everything())\n\n# Apply Bonferroni correction\nalpha &lt;- 0.05\nbonferroni_corrected_p &lt;- p.adjust(t_test_table$p.value, method = \"bonferroni\")\nt_test_table &lt;- t_test_table %&gt;% mutate(bonferroni_p = bonferroni_corrected_p)\n\n# Assuming t_test_table is your dataframe\nt_test_table &lt;- subset(t_test_table, select = -c(estimate, parameter, method, alternative))\n\n# Print the t-test results table with Bonferroni correction\nknitr::kable(t_test_table, caption = \"Table X: Results of t-tests between groups with Bonferroni correction\") %&gt;%\n  kableExtra::kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Results of t-tests between groups with Bonferroni correction\n\n\nvariable\nestimate1\nestimate2\nstatistic\np.value\nconf.low\nconf.high\nbonferroni_p\n\n\n\n\nPLA1\n19.946359\n18.759363\n1.7633050\n0.0967708\n-0.2390599\n2.6130513\n1.0000000\n\n\nPLA2\n39.779194\n77.560658\n-2.6602240\n0.0211690\n-68.8130221\n-6.7499051\n0.4233801\n\n\nPLA3\n23.555177\n45.359102\n-2.7273067\n0.0205253\n-39.5145259\n-4.0933232\n0.4105064\n\n\nPLA4\n22.411573\n31.149576\n-2.3973038\n0.0334956\n-16.6707940\n-0.8052110\n0.6699125\n\n\nPLA5\n21.960781\n24.688231\n-1.6056578\n0.1257626\n-6.2963718\n0.8414719\n1.0000000\n\n\nPAA1\n307.911111\n155.972349\n2.2385900\n0.0367594\n10.3251992\n293.5523257\n0.7351889\n\n\nPAA2\n352.185135\n413.916130\n-1.1061728\n0.2829783\n-178.8351376\n55.3731467\n1.0000000\n\n\nPAA3\n260.088832\n334.233999\n-1.8279474\n0.0852683\n-159.7627745\n11.4724403\n1.0000000\n\n\nPAA4\n149.766968\n155.863159\n-0.6256482\n0.5386858\n-26.4335632\n14.2411818\n1.0000000\n\n\nPAA5\n437.488341\n427.115991\n0.6575701\n0.5185074\n-22.5868692\n43.3315693\n1.0000000\n\n\nALA1\n9.852553\n9.947338\n-1.3000172\n0.2086430\n-0.2470514\n0.0574809\n1.0000000\n\n\nALA2\n12.118500\n13.361505\n-3.0647910\n0.0077791\n-2.1066182\n-0.3793909\n0.1555819\n\n\nALA3\n11.154198\n12.534129\n-3.3889323\n0.0065323\n-2.2825080\n-0.4773525\n0.1306463\n\n\nALA4\n10.673868\n11.050728\n-2.3737407\n0.0309858\n-0.7144457\n-0.0392749\n0.6197152\n\n\nALA5\n11.009821\n11.223323\n-1.5364561\n0.1417516\n-0.5053617\n0.0783559\n1.0000000\n\n\nAAA1\n46.703677\n28.112440\n2.4701591\n0.0248162\n2.6687436\n34.5137313\n0.4963244\n\n\nAAA2\n75.044250\n89.426506\n-2.4974579\n0.0213852\n-26.3977080\n-2.3668036\n0.4277047\n\n\nAAA3\n66.314962\n85.607783\n-2.3546358\n0.0313222\n-36.6300000\n-1.9556407\n0.6264440\n\n\nAAA4\n34.663516\n39.514913\n-1.9077457\n0.0710036\n-10.1587304\n0.4559366\n1.0000000\n\n\nAAA5\n92.939303\n92.151563\n0.1465594\n0.8849608\n-10.4309107\n12.0063923\n1.0000000\n\n\n\n\n\n\n\n\n\nEffect sizes\n\n\nCode\n# Load required libraries\nlibrary(dplyr)\nlibrary(broom)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Assuming 'data' is your dataframe and 'group' column indicates 'male' or 'female'\n# Separate numerical columns and the 'group' column\nnumerical_columns &lt;- sapply(data, is.numeric)\nnumerical_data &lt;- data[, numerical_columns]\n\n# Perform t-tests for each numerical column\nt_test_results &lt;- purrr::map(names(numerical_data), ~ t.test(as.formula(paste(.x, \"~ group\")), data = data) %&gt;% tidy())\n\n# Combine the t-test results into a single table\nt_test_table &lt;- bind_rows(t_test_results) %&gt;%\n  mutate(variable = names(numerical_data)) %&gt;%\n  select(variable, everything())\n\n# Apply Bonferroni correction\nalpha &lt;- 0.05\nbonferroni_corrected_p &lt;- p.adjust(t_test_table$p.value, method = \"bonferroni\")\nt_test_table &lt;- t_test_table %&gt;%\n  mutate(bonferroni_p = bonferroni_corrected_p)\n\n# Calculate Partial Eta Squared\n# Function to calculate partial eta squared\ncalc_partial_eta_squared &lt;- function(t_value, df) {\n  eta_squared &lt;- (t_value^2) / (t_value^2 + df)\n  return(eta_squared)\n}\n\n# Add partial eta squared to the t_test_table\nt_test_table &lt;- t_test_table %&gt;%\n  mutate(partial_eta_squared = calc_partial_eta_squared(statistic, parameter))\n\n# Remove unnecessary columns\nt_test_table &lt;- t_test_table %&gt;%\n  select(-estimate, -parameter, -method, -alternative)\n\n# Print the t-test results table with Bonferroni correction and partial eta squared\nkable(t_test_table, caption = \"Table X: Results of t-tests between groups with Bonferroni correction and Partial Eta Squared\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\")\n\n\n\nTable X: Results of t-tests between groups with Bonferroni correction and Partial Eta Squared\n\n\nvariable\nestimate1\nestimate2\nstatistic\np.value\nconf.low\nconf.high\nbonferroni_p\npartial_eta_squared\n\n\n\n\nPLA1\n19.946359\n18.759363\n1.7633050\n0.0967708\n-0.2390599\n2.6130513\n1.0000000\n0.1615424\n\n\nPLA2\n39.779194\n77.560658\n-2.6602240\n0.0211690\n-68.8130221\n-6.7499051\n0.4233801\n0.3768266\n\n\nPLA3\n23.555177\n45.359102\n-2.7273067\n0.0205253\n-39.5145259\n-4.0933232\n0.4105064\n0.4159002\n\n\nPLA4\n22.411573\n31.149576\n-2.3973038\n0.0334956\n-16.6707940\n-0.8052110\n0.6699125\n0.3216281\n\n\nPLA5\n21.960781\n24.688231\n-1.6056578\n0.1257626\n-6.2963718\n0.8414719\n1.0000000\n0.1253673\n\n\nPAA1\n307.911111\n155.972349\n2.2385900\n0.0367594\n10.3251992\n293.5523257\n0.7351889\n0.2009597\n\n\nPAA2\n352.185135\n413.916130\n-1.1061728\n0.2829783\n-178.8351376\n55.3731467\n1.0000000\n0.0626586\n\n\nPAA3\n260.088832\n334.233999\n-1.8279474\n0.0852683\n-159.7627745\n11.4724403\n1.0000000\n0.1650922\n\n\nPAA4\n149.766968\n155.863159\n-0.6256482\n0.5386858\n-26.4335632\n14.2411818\n1.0000000\n0.0193717\n\n\nPAA5\n437.488341\n427.115991\n0.6575701\n0.5185074\n-22.5868692\n43.3315693\n1.0000000\n0.0217086\n\n\nALA1\n9.852553\n9.947338\n-1.3000172\n0.2086430\n-0.2470514\n0.0574809\n1.0000000\n0.0792184\n\n\nALA2\n12.118500\n13.361505\n-3.0647910\n0.0077791\n-2.1066182\n-0.3793909\n0.1555819\n0.3823796\n\n\nALA3\n11.154198\n12.534129\n-3.3889323\n0.0065323\n-2.2825080\n-0.4773525\n0.1306463\n0.5248191\n\n\nALA4\n10.673868\n11.050728\n-2.3737407\n0.0309858\n-0.7144457\n-0.0392749\n0.6197152\n0.2675705\n\n\nALA5\n11.009821\n11.223323\n-1.5364561\n0.1417516\n-0.5053617\n0.0783559\n1.0000000\n0.1155458\n\n\nAAA1\n46.703677\n28.112440\n2.4701591\n0.0248162\n2.6687436\n34.5137313\n0.4963244\n0.2710013\n\n\nAAA2\n75.044250\n89.426506\n-2.4974579\n0.0213852\n-26.3977080\n-2.3668036\n0.4277047\n0.2384064\n\n\nAAA3\n66.314962\n85.607783\n-2.3546358\n0.0313222\n-36.6300000\n-1.9556407\n0.6264440\n0.2529290\n\n\nAAA4\n34.663516\n39.514913\n-1.9077457\n0.0710036\n-10.1587304\n0.4559366\n1.0000000\n0.1550017\n\n\nAAA5\n92.939303\n92.151563\n0.1465594\n0.8849608\n-10.4309107\n12.0063923\n1.0000000\n0.0010830",
    "crumbs": [
      "Statistical analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Analysis of lifts mean values</span>"
    ]
  }
]